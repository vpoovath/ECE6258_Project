Imports successful
Model Init Done.
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
Using adadelta Optimizer
{'rho': 0.9, 'epsilon': 1e-05}
Training loop started for 120 epochs:
[Epoch 0] train=0.121815 val_top1=0.131900 val_top5=0.455800 loss=145759.692627 time: 72.673556
[Epoch 1] train=0.218790 val_top1=0.201800 val_top5=0.617800 loss=125410.788330 time: 73.573058
[Epoch 2] train=0.302464 val_top1=0.338100 val_top5=0.751900 loss=111054.028015 time: 74.031062
[Epoch 3] train=0.369511 val_top1=0.370000 val_top5=0.770700 loss=99365.067291 time: 70.889898
[Epoch 4] train=0.422175 val_top1=0.373200 val_top5=0.757700 loss=90832.832275 time: 71.552960
[Epoch 5] train=0.467127 val_top1=0.468600 val_top5=0.841700 loss=83630.639008 time: 71.579629
[Epoch 6] train=0.500441 val_top1=0.458400 val_top5=0.822800 loss=78244.220428 time: 71.940719
[Epoch 7] train=0.525901 val_top1=0.467900 val_top5=0.831200 loss=73932.576965 time: 71.239545
[Epoch 8] train=0.552444 val_top1=0.507200 val_top5=0.865800 loss=69942.855133 time: 72.507641
[Epoch 9] train=0.572656 val_top1=0.558100 val_top5=0.890700 loss=66867.584320 time: 72.996931
[Epoch 10] train=0.590284 val_top1=0.587700 val_top5=0.894800 loss=63671.441711 time: 71.546531
[Epoch 11] train=0.606891 val_top1=0.585800 val_top5=0.901200 loss=61346.702606 time: 70.872909
[Epoch 12] train=0.622937 val_top1=0.589400 val_top5=0.909400 loss=58613.096817 time: 70.956332
[Epoch 13] train=0.637380 val_top1=0.578800 val_top5=0.904600 loss=56538.117035 time: 70.631979
[Epoch 14] train=0.647015 val_top1=0.603500 val_top5=0.912400 loss=54767.624420 time: 70.589949
[Epoch 15] train=0.654307 val_top1=0.607800 val_top5=0.919500 loss=53047.589615 time: 70.624684
[Epoch 16] train=0.668149 val_top1=0.630300 val_top5=0.921500 loss=51284.551392 time: 72.943899
[Epoch 17] train=0.679187 val_top1=0.578100 val_top5=0.877500 loss=49975.419785 time: 73.326290
[Epoch 18] train=0.684355 val_top1=0.631900 val_top5=0.919600 loss=48753.674316 time: 71.441758
[Epoch 19] train=0.693890 val_top1=0.640600 val_top5=0.927500 loss=47273.529953 time: 70.257838
[Epoch 20] train=0.701643 val_top1=0.645300 val_top5=0.926400 loss=45741.536423 time: 72.523520
[Epoch 21] train=0.708634 val_top1=0.574400 val_top5=0.900600 loss=44532.249359 time: 72.356774
[Epoch 22] train=0.717528 val_top1=0.658600 val_top5=0.930400 loss=43500.423599 time: 71.418152
[Epoch 23] train=0.725100 val_top1=0.622800 val_top5=0.918300 loss=42174.288437 time: 71.113786
[Epoch 24] train=0.731350 val_top1=0.655400 val_top5=0.921900 loss=41567.823349 time: 70.303044
[Epoch 25] train=0.731370 val_top1=0.672400 val_top5=0.938300 loss=41061.346832 time: 70.745088
[Epoch 26] train=0.744151 val_top1=0.651600 val_top5=0.925100 loss=39508.108032 time: 73.404617
[Epoch 27] train=0.746354 val_top1=0.580300 val_top5=0.877300 loss=38610.434067 time: 71.328624
[Epoch 28] train=0.751583 val_top1=0.673400 val_top5=0.935200 loss=37989.418228 time: 71.048926
[Epoch 29] train=0.757352 val_top1=0.653300 val_top5=0.922900 loss=36992.756744 time: 71.741871
[Epoch 30] train=0.764143 val_top1=0.675200 val_top5=0.940600 loss=36010.308212 time: 72.754276
[Epoch 31] train=0.769832 val_top1=0.676900 val_top5=0.935500 loss=35302.410355 time: 71.159616
[Epoch 32] train=0.775701 val_top1=0.630600 val_top5=0.922000 loss=34417.887787 time: 70.674141
[Epoch 33] train=0.778045 val_top1=0.672400 val_top5=0.934900 loss=33896.444382 time: 71.167391
[Epoch 34] train=0.783313 val_top1=0.653100 val_top5=0.931800 loss=32815.042938 time: 70.898859
[Epoch 35] train=0.785096 val_top1=0.662700 val_top5=0.925800 loss=32465.926308 time: 71.401337
[Epoch 36] train=0.790244 val_top1=0.690800 val_top5=0.941600 loss=31974.622925 time: 72.629756
[Epoch 37] train=0.795573 val_top1=0.687500 val_top5=0.941000 loss=31055.602722 time: 70.177675
[Epoch 38] train=0.800441 val_top1=0.680800 val_top5=0.942400 loss=30269.455574 time: 72.677518
[Epoch 39] train=0.800741 val_top1=0.674900 val_top5=0.932100 loss=29864.915047 time: 74.762473
[Epoch 40] train=0.806150 val_top1=0.670100 val_top5=0.939700 loss=29112.337990 time: 70.595418
[Epoch 41] train=0.811098 val_top1=0.691000 val_top5=0.937100 loss=28297.417831 time: 71.445493
[Epoch 42] train=0.815845 val_top1=0.687200 val_top5=0.942100 loss=27932.940269 time: 71.622477
[Epoch 43] train=0.817588 val_top1=0.677300 val_top5=0.930300 loss=27596.315399 time: 71.668182
[Epoch 44] train=0.823277 val_top1=0.701400 val_top5=0.945000 loss=26879.749039 time: 71.336242
[Epoch 45] train=0.824259 val_top1=0.688600 val_top5=0.937500 loss=26490.600327 time: 72.092425
[Epoch 46] train=0.828566 val_top1=0.678100 val_top5=0.933000 loss=25925.958328 time: 73.483510
[Epoch 47] train=0.830609 val_top1=0.675500 val_top5=0.930600 loss=25393.396278 time: 71.232375
[Epoch 48] train=0.833433 val_top1=0.691700 val_top5=0.932500 loss=24917.509850 time: 71.139397
[Epoch 49] train=0.835877 val_top1=0.694500 val_top5=0.936400 loss=24609.659531 time: 72.201986
[Epoch 50] train=0.840425 val_top1=0.692300 val_top5=0.936400 loss=24215.975792 time: 72.876258
[Epoch 51] train=0.844491 val_top1=0.628200 val_top5=0.908600 loss=23467.753502 time: 71.382995
[Epoch 52] train=0.845433 val_top1=0.663200 val_top5=0.930000 loss=23192.879417 time: 71.543341
[Epoch 53] train=0.848938 val_top1=0.690300 val_top5=0.940600 loss=22878.075905 time: 73.209444
[Epoch 54] train=0.850581 val_top1=0.696900 val_top5=0.935500 loss=22310.792198 time: 74.204670
[Epoch 55] train=0.855889 val_top1=0.696500 val_top5=0.942500 loss=21685.430450 time: 71.217460
[Epoch 56] train=0.855028 val_top1=0.700200 val_top5=0.941200 loss=21649.282463 time: 70.764346
[Epoch 57] train=0.861238 val_top1=0.705500 val_top5=0.939400 loss=20861.839859 time: 71.085603
[Epoch 58] train=0.862640 val_top1=0.709600 val_top5=0.936400 loss=20721.681358 time: 70.995584
[Epoch 59] train=0.862821 val_top1=0.703600 val_top5=0.939700 loss=20616.544930 time: 72.109085
[Epoch 60] train=0.864343 val_top1=0.679000 val_top5=0.933100 loss=20175.702301 time: 70.539964
[Epoch 61] train=0.870974 val_top1=0.670400 val_top5=0.927700 loss=19342.253967 time: 71.840318
[Epoch 62] train=0.871454 val_top1=0.710500 val_top5=0.943200 loss=19311.992729 time: 73.648629
[Epoch 63] train=0.868009 val_top1=0.711700 val_top5=0.942900 loss=19486.814373 time: 72.985836
[Epoch 64] train=0.876563 val_top1=0.689000 val_top5=0.933600 loss=18439.851009 time: 72.595908
[Epoch 65] train=0.875641 val_top1=0.708000 val_top5=0.943400 loss=18527.122189 time: 71.889299
[Epoch 66] train=0.877484 val_top1=0.678000 val_top5=0.934800 loss=18291.652508 time: 69.266600
[Epoch 67] train=0.878786 val_top1=0.686400 val_top5=0.933000 loss=18169.393776 time: 70.974869
[Epoch 68] train=0.882572 val_top1=0.707100 val_top5=0.939500 loss=17495.985344 time: 71.571162
[Epoch 69] train=0.883914 val_top1=0.703800 val_top5=0.941900 loss=17261.690601 time: 72.616428
[Epoch 70] train=0.886258 val_top1=0.701000 val_top5=0.943100 loss=16899.199879 time: 73.616763
[Epoch 71] train=0.888502 val_top1=0.709800 val_top5=0.940100 loss=16616.026329 time: 72.493706
[Epoch 72] train=0.885897 val_top1=0.709600 val_top5=0.941700 loss=16717.376102 time: 71.205129
[Epoch 73] train=0.889363 val_top1=0.675700 val_top5=0.924600 loss=16090.068356 time: 71.333441
[Epoch 74] train=0.893870 val_top1=0.706800 val_top5=0.938800 loss=15760.485912 time: 70.389807
[Epoch 75] train=0.889884 val_top1=0.700000 val_top5=0.940300 loss=16167.841942 time: 70.381895
[Epoch 76] train=0.894231 val_top1=0.705200 val_top5=0.940100 loss=15495.984306 time: 71.644790
[Epoch 77] train=0.891526 val_top1=0.713600 val_top5=0.942100 loss=15997.871304 time: 71.296571
[Epoch 78] train=0.898097 val_top1=0.707700 val_top5=0.943300 loss=15097.314693 time: 72.350201
[Epoch 79] train=0.900421 val_top1=0.716300 val_top5=0.942400 loss=14706.094776 time: 70.304123
[Epoch 80] train=0.899679 val_top1=0.719800 val_top5=0.943200 loss=14832.557076 time: 71.383581
[Epoch 81] train=0.904067 val_top1=0.719400 val_top5=0.944300 loss=14255.374386 time: 71.487006
[Epoch 82] train=0.902744 val_top1=0.695800 val_top5=0.936800 loss=14441.649597 time: 72.732234
[Epoch 83] train=0.903325 val_top1=0.721100 val_top5=0.945300 loss=14183.266006 time: 72.067945
[Epoch 84] train=0.907873 val_top1=0.702800 val_top5=0.938500 loss=13571.588234 time: 71.887524
[Epoch 85] train=0.908113 val_top1=0.725900 val_top5=0.946600 loss=13526.981739 time: 70.350214
[Epoch 86] train=0.907712 val_top1=0.711700 val_top5=0.945300 loss=13572.059799 time: 69.364176
[Epoch 87] train=0.910757 val_top1=0.703000 val_top5=0.945500 loss=13087.607899 time: 70.431175
[Epoch 88] train=0.913942 val_top1=0.714400 val_top5=0.945100 loss=12729.592781 time: 70.094693
[Epoch 89] train=0.913241 val_top1=0.704700 val_top5=0.938700 loss=12959.835228 time: 69.607590
[Epoch 90] train=0.912861 val_top1=0.708400 val_top5=0.939700 loss=12919.623795 time: 72.340833
[Epoch 91] train=0.914884 val_top1=0.701300 val_top5=0.939200 loss=12534.823097 time: 71.874287
[Epoch 92] train=0.914824 val_top1=0.711300 val_top5=0.939800 loss=12656.200674 time: 70.177696
[Epoch 93] train=0.916346 val_top1=0.718200 val_top5=0.944000 loss=12372.421566 time: 68.889229
[Epoch 94] train=0.921575 val_top1=0.710100 val_top5=0.946200 loss=11521.236237 time: 69.352159
[Epoch 95] train=0.919832 val_top1=0.715500 val_top5=0.941100 loss=11947.359734 time: 69.082400
[Epoch 96] train=0.920312 val_top1=0.706200 val_top5=0.938700 loss=11699.905081 time: 73.220310
[Epoch 97] train=0.922556 val_top1=0.717700 val_top5=0.942900 loss=11569.519535 time: 69.457092
[Epoch 98] train=0.923798 val_top1=0.727800 val_top5=0.947500 loss=11409.006695 time: 70.447140
[Epoch 99] train=0.923998 val_top1=0.710800 val_top5=0.943200 loss=11287.728792 time: 69.718086
[Epoch 100] train=0.924219 val_top1=0.723300 val_top5=0.947200 loss=11138.745743 time: 69.838148
[Epoch 101] train=0.926863 val_top1=0.709400 val_top5=0.942600 loss=10686.802670 time: 69.164298
[Epoch 102] train=0.926462 val_top1=0.711500 val_top5=0.942800 loss=11052.932026 time: 71.962016
[Epoch 103] train=0.927163 val_top1=0.708500 val_top5=0.941600 loss=10758.559679 time: 70.763709
[Epoch 104] train=0.928145 val_top1=0.709500 val_top5=0.941000 loss=10781.978434 time: 70.332086
[Epoch 105] train=0.927183 val_top1=0.713900 val_top5=0.945700 loss=10610.241896 time: 70.177464
[Epoch 106] train=0.930849 val_top1=0.699400 val_top5=0.941100 loss=10205.388474 time: 70.616125
[Epoch 107] train=0.929748 val_top1=0.721700 val_top5=0.945000 loss=10349.733265 time: 69.795712
[Epoch 108] train=0.928105 val_top1=0.719000 val_top5=0.943800 loss=10517.798510 time: 70.470713
[Epoch 109] train=0.932292 val_top1=0.702400 val_top5=0.936000 loss=9914.897560 time: 69.380504
[Epoch 110] train=0.935397 val_top1=0.706400 val_top5=0.944800 loss=9609.932560 time: 70.963988
[Epoch 111] train=0.932512 val_top1=0.712100 val_top5=0.940000 loss=9891.995256 time: 71.903248
[Epoch 112] train=0.933574 val_top1=0.719600 val_top5=0.942800 loss=9709.933640 time: 68.427573
[Epoch 113] train=0.934455 val_top1=0.702500 val_top5=0.939500 loss=9776.236265 time: 71.404814
[Epoch 114] train=0.935296 val_top1=0.720000 val_top5=0.943100 loss=9412.484676 time: 69.702698
[Epoch 115] train=0.936899 val_top1=0.706100 val_top5=0.939200 loss=9312.843191 time: 69.240177
[Epoch 116] train=0.936999 val_top1=0.717700 val_top5=0.942600 loss=9161.166649 time: 70.927235
[Epoch 117] train=0.936118 val_top1=0.712400 val_top5=0.944200 loss=9410.086742 time: 70.917304
[Epoch 118] train=0.938562 val_top1=0.725500 val_top5=0.947800 loss=9064.661901 time: 69.286284
[Epoch 119] train=0.937340 val_top1=0.715400 val_top5=0.944400 loss=9213.644979 time: 69.936524
Done.
