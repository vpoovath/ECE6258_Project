Imports successful
Model Init Done.
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
Using adam Optimizer
{'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-07}
Training loop started for 120 epochs:
[Epoch 0] train=0.160236 val_top1=0.218900 val_top5=0.585100 loss=140155.675903 time: 30.012397
[Epoch 1] train=0.271534 val_top1=0.294200 val_top5=0.701100 loss=117317.385803 time: 29.955236
[Epoch 2] train=0.346595 val_top1=0.365600 val_top5=0.773100 loss=104408.301819 time: 29.875078
[Epoch 3] train=0.403646 val_top1=0.392800 val_top5=0.782300 loss=94465.513550 time: 29.702908
[Epoch 4] train=0.446534 val_top1=0.440900 val_top5=0.829100 loss=87477.768860 time: 29.938751
[Epoch 5] train=0.479227 val_top1=0.469600 val_top5=0.846300 loss=81844.811584 time: 30.024769
[Epoch 6] train=0.508634 val_top1=0.500000 val_top5=0.850700 loss=77500.872284 time: 29.877383
[Epoch 7] train=0.534455 val_top1=0.542600 val_top5=0.881500 loss=73345.180420 time: 29.855788
[Epoch 8] train=0.553045 val_top1=0.533200 val_top5=0.876200 loss=69745.609375 time: 29.846285
[Epoch 9] train=0.572596 val_top1=0.547200 val_top5=0.882700 loss=66761.325714 time: 29.871082
[Epoch 10] train=0.589163 val_top1=0.543300 val_top5=0.885000 loss=64172.533295 time: 29.885894
[Epoch 11] train=0.604407 val_top1=0.561500 val_top5=0.888300 loss=61725.380646 time: 29.835218
[Epoch 12] train=0.617027 val_top1=0.580200 val_top5=0.898000 loss=59726.095947 time: 29.857635
[Epoch 13] train=0.629347 val_top1=0.594600 val_top5=0.908100 loss=57732.052277 time: 29.997164
[Epoch 14] train=0.639784 val_top1=0.596700 val_top5=0.906600 loss=55863.195343 time: 29.878965
[Epoch 15] train=0.652083 val_top1=0.560000 val_top5=0.893100 loss=53978.690262 time: 29.810983
[Epoch 16] train=0.660196 val_top1=0.601400 val_top5=0.901200 loss=52754.178818 time: 30.034416
[Epoch 17] train=0.669091 val_top1=0.621000 val_top5=0.920200 loss=51214.760147 time: 31.455052
[Epoch 18] train=0.678245 val_top1=0.636800 val_top5=0.919400 loss=49773.153076 time: 29.966156
[Epoch 19] train=0.683954 val_top1=0.631800 val_top5=0.919900 loss=48549.228302 time: 29.836674
[Epoch 20] train=0.691026 val_top1=0.624100 val_top5=0.914200 loss=47435.863724 time: 29.919647
[Epoch 21] train=0.702224 val_top1=0.622300 val_top5=0.908000 loss=45834.015076 time: 29.861847
[Epoch 22] train=0.705449 val_top1=0.640900 val_top5=0.929800 loss=45098.390121 time: 29.884553
[Epoch 23] train=0.713722 val_top1=0.662000 val_top5=0.935800 loss=44045.327087 time: 29.848559
[Epoch 24] train=0.723077 val_top1=0.638900 val_top5=0.918800 loss=42698.372116 time: 29.916427
[Epoch 25] train=0.725140 val_top1=0.654800 val_top5=0.929700 loss=42212.370621 time: 29.716217
[Epoch 26] train=0.731150 val_top1=0.672800 val_top5=0.935900 loss=41147.534546 time: 29.810257
[Epoch 27] train=0.736098 val_top1=0.652500 val_top5=0.936200 loss=40246.571838 time: 29.901788
[Epoch 28] train=0.740665 val_top1=0.656100 val_top5=0.935800 loss=39522.240677 time: 29.889688
[Epoch 29] train=0.748097 val_top1=0.655700 val_top5=0.926700 loss=38504.321259 time: 29.791637
[Epoch 30] train=0.754928 val_top1=0.682000 val_top5=0.939100 loss=37633.792313 time: 30.077936
[Epoch 31] train=0.756450 val_top1=0.680000 val_top5=0.943000 loss=37039.108383 time: 30.069241
[Epoch 32] train=0.763702 val_top1=0.671800 val_top5=0.930500 loss=36179.389526 time: 29.997980
[Epoch 33] train=0.767748 val_top1=0.658100 val_top5=0.927300 loss=35532.770004 time: 29.917760
[Epoch 34] train=0.771434 val_top1=0.645700 val_top5=0.922600 loss=34811.782013 time: 29.931247
[Epoch 35] train=0.775361 val_top1=0.687500 val_top5=0.933000 loss=34271.559746 time: 30.089238
[Epoch 36] train=0.780308 val_top1=0.672300 val_top5=0.930700 loss=33507.980583 time: 31.196665
[Epoch 37] train=0.784575 val_top1=0.692300 val_top5=0.936800 loss=32655.251060 time: 30.017407
[Epoch 38] train=0.790064 val_top1=0.680000 val_top5=0.939500 loss=32147.660614 time: 29.929747
[Epoch 39] train=0.791366 val_top1=0.682200 val_top5=0.939000 loss=31823.113060 time: 29.842197
[Epoch 40] train=0.797596 val_top1=0.700100 val_top5=0.945100 loss=30705.193962 time: 29.806051
[Epoch 41] train=0.799199 val_top1=0.705000 val_top5=0.946000 loss=30385.571846 time: 29.770889
[Epoch 42] train=0.803786 val_top1=0.696400 val_top5=0.943300 loss=29851.057487 time: 29.780092
[Epoch 43] train=0.804127 val_top1=0.700500 val_top5=0.941000 loss=29670.741112 time: 29.919794
[Epoch 44] train=0.810116 val_top1=0.691200 val_top5=0.936500 loss=28851.849159 time: 29.864311
[Epoch 45] train=0.815605 val_top1=0.703800 val_top5=0.942900 loss=27978.647507 time: 29.704096
[Epoch 46] train=0.814964 val_top1=0.687000 val_top5=0.936000 loss=27794.616676 time: 29.934941
[Epoch 47] train=0.817348 val_top1=0.701600 val_top5=0.943500 loss=27604.658356 time: 29.786903
[Epoch 48] train=0.826162 val_top1=0.705600 val_top5=0.943400 loss=26313.469223 time: 29.780463
[Epoch 49] train=0.824359 val_top1=0.704700 val_top5=0.943900 loss=26440.300682 time: 29.830414
[Epoch 50] train=0.828846 val_top1=0.691200 val_top5=0.942000 loss=25896.715897 time: 30.002607
[Epoch 51] train=0.828005 val_top1=0.701800 val_top5=0.935800 loss=25801.300674 time: 29.810754
[Epoch 52] train=0.831651 val_top1=0.699400 val_top5=0.940600 loss=25152.763199 time: 29.929698
[Epoch 53] train=0.834475 val_top1=0.705300 val_top5=0.942500 loss=24661.805717 time: 29.831703
[Epoch 54] train=0.838061 val_top1=0.703400 val_top5=0.942400 loss=24415.197563 time: 29.954315
[Epoch 55] train=0.842788 val_top1=0.715500 val_top5=0.948300 loss=23713.102722 time: 29.816186
[Epoch 56] train=0.841246 val_top1=0.697200 val_top5=0.942600 loss=23490.740883 time: 29.751934
[Epoch 57] train=0.848458 val_top1=0.706500 val_top5=0.943800 loss=22760.905716 time: 29.793113
[Epoch 58] train=0.848598 val_top1=0.697600 val_top5=0.940700 loss=22861.536682 time: 29.943754
[Epoch 59] train=0.846655 val_top1=0.708100 val_top5=0.942300 loss=22705.877647 time: 29.791937
[Epoch 60] train=0.849700 val_top1=0.707500 val_top5=0.942600 loss=22381.527679 time: 29.880002
[Epoch 61] train=0.856831 val_top1=0.704700 val_top5=0.944800 loss=21259.551620 time: 29.893008
[Epoch 62] train=0.858534 val_top1=0.700200 val_top5=0.942400 loss=21256.633461 time: 29.895980
[Epoch 63] train=0.858233 val_top1=0.718900 val_top5=0.945700 loss=21151.645508 time: 30.006606
[Epoch 64] train=0.860417 val_top1=0.697400 val_top5=0.942600 loss=20716.753960 time: 29.752271
[Epoch 65] train=0.864383 val_top1=0.699400 val_top5=0.941400 loss=20156.844093 time: 29.915331
[Epoch 66] train=0.867228 val_top1=0.720100 val_top5=0.946400 loss=19719.744606 time: 29.920296
[Epoch 67] train=0.865365 val_top1=0.698500 val_top5=0.940800 loss=19945.970528 time: 29.732726
[Epoch 68] train=0.868770 val_top1=0.696200 val_top5=0.937600 loss=19361.112553 time: 29.924160
[Epoch 69] train=0.868470 val_top1=0.700600 val_top5=0.941300 loss=19317.457413 time: 29.747859
[Epoch 70] train=0.872135 val_top1=0.712400 val_top5=0.943700 loss=18831.683117 time: 29.888751
[Epoch 71] train=0.874679 val_top1=0.714400 val_top5=0.942400 loss=18542.180393 time: 29.676339
[Epoch 72] train=0.875381 val_top1=0.724700 val_top5=0.948400 loss=18558.037655 time: 29.864738
[Epoch 73] train=0.877023 val_top1=0.701200 val_top5=0.939100 loss=18183.647938 time: 29.844259
[Epoch 74] train=0.883634 val_top1=0.699300 val_top5=0.942700 loss=17344.957100 time: 30.070628
[Epoch 75] train=0.881851 val_top1=0.700100 val_top5=0.939500 loss=17615.258118 time: 29.953978
[Epoch 76] train=0.880929 val_top1=0.715000 val_top5=0.943300 loss=17513.364117 time: 29.762919
[Epoch 77] train=0.884455 val_top1=0.711800 val_top5=0.945600 loss=17137.653790 time: 29.821757
[Epoch 78] train=0.885296 val_top1=0.715000 val_top5=0.945100 loss=16918.689121 time: 29.898633
[Epoch 79] train=0.890805 val_top1=0.721600 val_top5=0.945800 loss=16208.687420 time: 29.874600
[Epoch 80] train=0.888822 val_top1=0.716400 val_top5=0.943700 loss=16163.256432 time: 29.678569
[Epoch 81] train=0.888421 val_top1=0.710000 val_top5=0.943800 loss=16351.419292 time: 29.776304
[Epoch 82] train=0.894411 val_top1=0.702500 val_top5=0.943900 loss=15669.853752 time: 30.064274
[Epoch 83] train=0.893329 val_top1=0.708400 val_top5=0.940300 loss=15688.079044 time: 29.924765
[Epoch 84] train=0.893690 val_top1=0.720700 val_top5=0.945500 loss=15665.511223 time: 29.835944
[Epoch 85] train=0.894992 val_top1=0.713100 val_top5=0.942400 loss=15515.885838 time: 29.725136
[Epoch 86] train=0.897696 val_top1=0.723700 val_top5=0.944100 loss=14939.497498 time: 30.000147
[Epoch 87] train=0.897216 val_top1=0.719100 val_top5=0.944600 loss=15395.668358 time: 29.808283
[Epoch 88] train=0.902564 val_top1=0.719000 val_top5=0.946400 loss=14540.864258 time: 29.836043
[Epoch 89] train=0.902404 val_top1=0.716700 val_top5=0.945400 loss=14392.207825 time: 29.916717
[Epoch 90] train=0.903005 val_top1=0.710200 val_top5=0.938600 loss=14182.107574 time: 29.808645
[Epoch 91] train=0.902644 val_top1=0.713400 val_top5=0.944100 loss=14359.310810 time: 29.883722
[Epoch 92] train=0.904828 val_top1=0.720900 val_top5=0.945300 loss=13889.465862 time: 29.771207
[Epoch 93] train=0.905950 val_top1=0.717700 val_top5=0.944900 loss=13872.021057 time: 29.973807
[Epoch 94] train=0.904067 val_top1=0.722200 val_top5=0.944200 loss=13751.404881 time: 29.821714
[Epoch 95] train=0.907652 val_top1=0.705600 val_top5=0.941600 loss=13579.671379 time: 29.902494
[Epoch 96] train=0.907051 val_top1=0.717000 val_top5=0.942800 loss=13622.539566 time: 29.778072
[Epoch 97] train=0.911879 val_top1=0.716100 val_top5=0.944400 loss=13007.287033 time: 30.028830
[Epoch 98] train=0.910337 val_top1=0.722200 val_top5=0.946300 loss=13344.700512 time: 29.893033
[Epoch 99] train=0.912280 val_top1=0.725600 val_top5=0.944000 loss=13020.574257 time: 29.895382
[Epoch 100] train=0.913341 val_top1=0.723300 val_top5=0.942400 loss=12575.177525 time: 29.886098
[Epoch 101] train=0.913522 val_top1=0.721700 val_top5=0.946600 loss=12699.871502 time: 29.822605
[Epoch 102] train=0.913802 val_top1=0.710700 val_top5=0.943100 loss=12642.288689 time: 29.845882
[Epoch 103] train=0.914924 val_top1=0.711200 val_top5=0.938200 loss=12591.985744 time: 29.941808
[Epoch 104] train=0.914944 val_top1=0.716700 val_top5=0.943000 loss=12565.391926 time: 29.869099
[Epoch 105] train=0.917328 val_top1=0.723000 val_top5=0.946400 loss=12080.545422 time: 29.874972
[Epoch 106] train=0.919111 val_top1=0.726600 val_top5=0.944300 loss=11792.213387 time: 29.861578
[Epoch 107] train=0.916987 val_top1=0.724000 val_top5=0.944300 loss=12347.179955 time: 30.011919
[Epoch 108] train=0.919631 val_top1=0.714600 val_top5=0.944100 loss=11733.084747 time: 29.843550
[Epoch 109] train=0.921274 val_top1=0.705500 val_top5=0.937200 loss=11654.235775 time: 29.749669
[Epoch 110] train=0.919171 val_top1=0.718400 val_top5=0.941800 loss=11766.226425 time: 29.914325
[Epoch 111] train=0.921995 val_top1=0.700500 val_top5=0.938800 loss=11432.062073 time: 29.871444
[Epoch 112] train=0.920833 val_top1=0.714500 val_top5=0.943500 loss=11574.056988 time: 30.007128
[Epoch 113] train=0.921074 val_top1=0.708900 val_top5=0.939100 loss=11538.645378 time: 29.850366
[Epoch 114] train=0.925801 val_top1=0.712900 val_top5=0.943900 loss=10998.866991 time: 29.824712
[Epoch 115] train=0.923678 val_top1=0.714900 val_top5=0.946200 loss=11252.919092 time: 29.841944
[Epoch 116] train=0.925681 val_top1=0.717400 val_top5=0.945200 loss=10898.168783 time: 29.874855
[Epoch 117] train=0.928906 val_top1=0.719800 val_top5=0.947600 loss=10569.339626 time: 30.097215
[Epoch 118] train=0.927945 val_top1=0.722800 val_top5=0.947400 loss=10620.338728 time: 29.789991
[Epoch 119] train=0.929347 val_top1=0.727400 val_top5=0.944400 loss=10584.311726 time: 29.837002
Done.
