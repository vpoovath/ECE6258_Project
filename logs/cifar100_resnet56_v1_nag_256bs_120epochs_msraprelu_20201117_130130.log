Imports successful
Using MSRA Prelu Init.
Model Init Done.
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
Using nag Optimizer
{'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}
Training loop started for 120 epochs:
[Epoch 0] train=0.114603 val_top1=0.170300 val_top5=0.517200 loss=147519.632202 time: 30.234947
[Epoch 1] train=0.204407 val_top1=0.238000 val_top5=0.613100 loss=129334.548035 time: 30.042388
[Epoch 2] train=0.264383 val_top1=0.322800 val_top5=0.716300 loss=118544.882568 time: 30.223659
[Epoch 3] train=0.331691 val_top1=0.346800 val_top5=0.732900 loss=107273.977966 time: 29.922391
[Epoch 4] train=0.389623 val_top1=0.421400 val_top5=0.804300 loss=97226.320557 time: 30.056932
[Epoch 5] train=0.439463 val_top1=0.451500 val_top5=0.814800 loss=88936.343628 time: 30.036917
[Epoch 6] train=0.476643 val_top1=0.490100 val_top5=0.854300 loss=82543.019775 time: 30.132311
[Epoch 7] train=0.508934 val_top1=0.499300 val_top5=0.852800 loss=77084.375214 time: 30.169916
[Epoch 8] train=0.537680 val_top1=0.523000 val_top5=0.858700 loss=72462.625671 time: 30.034814
[Epoch 9] train=0.561919 val_top1=0.551100 val_top5=0.886900 loss=68389.420532 time: 30.202723
[Epoch 10] train=0.584455 val_top1=0.557900 val_top5=0.891500 loss=64957.185089 time: 30.079159
[Epoch 11] train=0.604667 val_top1=0.579400 val_top5=0.896100 loss=62197.241272 time: 30.102845
[Epoch 12] train=0.617628 val_top1=0.608100 val_top5=0.908800 loss=59451.619141 time: 29.885518
[Epoch 13] train=0.632312 val_top1=0.587500 val_top5=0.902900 loss=57407.481873 time: 30.096958
[Epoch 14] train=0.641727 val_top1=0.583400 val_top5=0.898600 loss=55825.001297 time: 30.093505
[Epoch 15] train=0.653606 val_top1=0.618600 val_top5=0.914700 loss=53837.102966 time: 30.146493
[Epoch 16] train=0.666687 val_top1=0.597900 val_top5=0.909400 loss=51924.681519 time: 30.058024
[Epoch 17] train=0.673137 val_top1=0.615300 val_top5=0.913900 loss=50497.823837 time: 30.293657
[Epoch 18] train=0.683454 val_top1=0.608300 val_top5=0.907500 loss=49213.007492 time: 30.140832
[Epoch 19] train=0.690805 val_top1=0.623500 val_top5=0.916700 loss=47793.184052 time: 30.143551
[Epoch 20] train=0.699439 val_top1=0.638800 val_top5=0.919900 loss=46367.913834 time: 29.890469
[Epoch 21] train=0.707812 val_top1=0.655400 val_top5=0.926300 loss=45276.207657 time: 30.065334
[Epoch 22] train=0.712760 val_top1=0.630600 val_top5=0.909300 loss=44318.370087 time: 30.075236
[Epoch 23] train=0.719872 val_top1=0.661300 val_top5=0.935400 loss=43400.581360 time: 29.942246
[Epoch 24] train=0.725761 val_top1=0.658200 val_top5=0.935000 loss=42232.662933 time: 30.051802
[Epoch 25] train=0.731851 val_top1=0.658300 val_top5=0.929800 loss=41314.179642 time: 30.190489
[Epoch 26] train=0.737039 val_top1=0.675000 val_top5=0.931800 loss=40554.849686 time: 30.065453
[Epoch 27] train=0.740905 val_top1=0.655400 val_top5=0.928500 loss=40046.794632 time: 30.032037
[Epoch 28] train=0.746254 val_top1=0.664800 val_top5=0.930200 loss=39132.195618 time: 30.001331
[Epoch 29] train=0.749479 val_top1=0.665600 val_top5=0.931900 loss=38426.352051 time: 29.949337
[Epoch 30] train=0.790505 val_top1=0.731700 val_top5=0.952700 loss=32224.548218 time: 30.032608
[Epoch 31] train=0.819211 val_top1=0.731400 val_top5=0.954700 loss=27750.746735 time: 30.142474
[Epoch 32] train=0.828325 val_top1=0.734100 val_top5=0.955600 loss=26395.926605 time: 29.981027
[Epoch 33] train=0.833854 val_top1=0.736100 val_top5=0.956000 loss=25552.708878 time: 30.132206
[Epoch 34] train=0.838361 val_top1=0.737000 val_top5=0.955900 loss=24690.737396 time: 29.979077
[Epoch 35] train=0.841807 val_top1=0.739100 val_top5=0.957800 loss=24361.202583 time: 30.091828
[Epoch 36] train=0.845813 val_top1=0.741200 val_top5=0.955000 loss=23703.260689 time: 29.921624
[Epoch 37] train=0.845593 val_top1=0.742900 val_top5=0.954200 loss=23585.190521 time: 30.139369
[Epoch 38] train=0.850300 val_top1=0.739600 val_top5=0.953500 loss=22723.530952 time: 30.184049
[Epoch 39] train=0.853666 val_top1=0.735100 val_top5=0.954300 loss=22298.294731 time: 30.172783
[Epoch 40] train=0.857472 val_top1=0.738000 val_top5=0.953300 loss=21835.294624 time: 30.076411
[Epoch 41] train=0.856991 val_top1=0.738700 val_top5=0.954600 loss=21778.689110 time: 29.951442
[Epoch 42] train=0.861659 val_top1=0.743400 val_top5=0.955200 loss=21093.872002 time: 30.014789
[Epoch 43] train=0.861719 val_top1=0.737900 val_top5=0.952400 loss=21010.753044 time: 30.009168
[Epoch 44] train=0.863381 val_top1=0.742500 val_top5=0.953800 loss=20631.899223 time: 29.872261
[Epoch 45] train=0.863962 val_top1=0.737000 val_top5=0.952500 loss=20434.247726 time: 30.214679
[Epoch 46] train=0.865665 val_top1=0.743700 val_top5=0.952800 loss=20086.785225 time: 29.788658
[Epoch 47] train=0.867027 val_top1=0.741800 val_top5=0.953200 loss=19725.252419 time: 30.028345
[Epoch 48] train=0.872516 val_top1=0.737900 val_top5=0.953500 loss=19229.349236 time: 30.034323
[Epoch 49] train=0.872957 val_top1=0.735200 val_top5=0.951500 loss=19184.346489 time: 30.115681
[Epoch 50] train=0.876162 val_top1=0.734600 val_top5=0.952900 loss=18804.218960 time: 29.993339
[Epoch 51] train=0.879247 val_top1=0.736500 val_top5=0.952500 loss=18343.509575 time: 29.864307
[Epoch 52] train=0.879507 val_top1=0.741700 val_top5=0.951600 loss=18310.994507 time: 30.021702
[Epoch 53] train=0.881931 val_top1=0.739900 val_top5=0.954000 loss=17773.445927 time: 29.992563
[Epoch 54] train=0.881130 val_top1=0.733400 val_top5=0.950700 loss=17835.939293 time: 30.051835
[Epoch 55] train=0.883133 val_top1=0.738000 val_top5=0.951200 loss=17665.953789 time: 30.254743
[Epoch 56] train=0.886458 val_top1=0.736200 val_top5=0.952700 loss=17072.636425 time: 29.962361
[Epoch 57] train=0.886999 val_top1=0.735900 val_top5=0.950500 loss=17059.193001 time: 30.129831
[Epoch 58] train=0.888602 val_top1=0.732700 val_top5=0.950300 loss=16817.926575 time: 30.321478
[Epoch 59] train=0.889523 val_top1=0.730200 val_top5=0.949300 loss=16547.834953 time: 29.970789
[Epoch 60] train=0.897877 val_top1=0.740500 val_top5=0.950700 loss=15480.846146 time: 29.973773
[Epoch 61] train=0.904087 val_top1=0.742300 val_top5=0.951400 loss=14685.599537 time: 30.185653
[Epoch 62] train=0.906530 val_top1=0.744300 val_top5=0.950900 loss=14441.301517 time: 30.152122
[Epoch 63] train=0.906370 val_top1=0.742100 val_top5=0.951400 loss=14424.109367 time: 29.886932
[Epoch 64] train=0.909014 val_top1=0.742000 val_top5=0.951300 loss=14083.253643 time: 30.090917
[Epoch 65] train=0.908514 val_top1=0.743000 val_top5=0.951000 loss=14058.410900 time: 30.180781
[Epoch 66] train=0.909054 val_top1=0.742200 val_top5=0.951100 loss=13994.019299 time: 30.256757
[Epoch 67] train=0.908714 val_top1=0.743600 val_top5=0.952200 loss=14070.063599 time: 30.090935
[Epoch 68] train=0.908273 val_top1=0.742800 val_top5=0.951900 loss=13904.180599 time: 29.981152
[Epoch 69] train=0.911999 val_top1=0.744100 val_top5=0.952200 loss=13675.795307 time: 29.984087
[Epoch 70] train=0.912119 val_top1=0.741500 val_top5=0.951900 loss=13569.444229 time: 29.912468
[Epoch 71] train=0.911138 val_top1=0.742000 val_top5=0.952000 loss=13555.488335 time: 30.037748
[Epoch 72] train=0.912260 val_top1=0.743000 val_top5=0.951700 loss=13437.099487 time: 29.933412
[Epoch 73] train=0.911759 val_top1=0.744200 val_top5=0.951000 loss=13522.976051 time: 30.086838
[Epoch 74] train=0.911659 val_top1=0.743600 val_top5=0.951800 loss=13553.857037 time: 29.927924
[Epoch 75] train=0.911939 val_top1=0.742600 val_top5=0.952600 loss=13530.507847 time: 30.075579
[Epoch 76] train=0.913482 val_top1=0.742100 val_top5=0.953400 loss=13205.951050 time: 30.223444
[Epoch 77] train=0.912620 val_top1=0.741900 val_top5=0.951500 loss=13191.744820 time: 30.095479
[Epoch 78] train=0.911338 val_top1=0.742100 val_top5=0.952100 loss=13403.841370 time: 30.031956
[Epoch 79] train=0.913482 val_top1=0.741000 val_top5=0.951200 loss=13130.142365 time: 29.972400
[Epoch 80] train=0.915425 val_top1=0.740800 val_top5=0.952000 loss=13012.030777 time: 29.869075
[Epoch 81] train=0.913141 val_top1=0.741700 val_top5=0.952200 loss=13133.753994 time: 30.031233
[Epoch 82] train=0.914984 val_top1=0.744000 val_top5=0.950800 loss=13042.448078 time: 30.036562
[Epoch 83] train=0.913862 val_top1=0.742300 val_top5=0.951700 loss=13125.774815 time: 30.245018
[Epoch 84] train=0.914844 val_top1=0.741800 val_top5=0.952200 loss=12889.644962 time: 30.209903
[Epoch 85] train=0.915725 val_top1=0.740100 val_top5=0.951600 loss=12856.377293 time: 30.158617
[Epoch 86] train=0.915925 val_top1=0.742200 val_top5=0.950900 loss=12829.117199 time: 30.039412
[Epoch 87] train=0.915885 val_top1=0.742600 val_top5=0.950600 loss=12795.440670 time: 29.996319
[Epoch 88] train=0.915325 val_top1=0.741500 val_top5=0.952100 loss=12894.114872 time: 29.933834
[Epoch 89] train=0.915385 val_top1=0.741300 val_top5=0.950600 loss=12922.589558 time: 30.095128
[Epoch 90] train=0.917548 val_top1=0.742700 val_top5=0.951900 loss=12746.691143 time: 30.094625
[Epoch 91] train=0.917127 val_top1=0.741100 val_top5=0.952100 loss=12756.998005 time: 30.099687
[Epoch 92] train=0.917989 val_top1=0.740500 val_top5=0.951200 loss=12504.399418 time: 29.846216
[Epoch 93] train=0.919591 val_top1=0.741300 val_top5=0.952100 loss=12473.229534 time: 29.995311
[Epoch 94] train=0.918269 val_top1=0.741700 val_top5=0.951100 loss=12451.167973 time: 29.951977
[Epoch 95] train=0.916787 val_top1=0.743600 val_top5=0.951200 loss=12643.855885 time: 30.298470
[Epoch 96] train=0.917568 val_top1=0.740900 val_top5=0.950700 loss=12634.309586 time: 30.042029
[Epoch 97] train=0.919050 val_top1=0.742000 val_top5=0.950900 loss=12470.063507 time: 30.098469
[Epoch 98] train=0.919932 val_top1=0.742100 val_top5=0.951400 loss=12402.213352 time: 30.127299
[Epoch 99] train=0.918850 val_top1=0.742700 val_top5=0.951700 loss=12467.584896 time: 29.801538
[Epoch 100] train=0.918530 val_top1=0.743200 val_top5=0.951800 loss=12560.132088 time: 29.885375
[Epoch 101] train=0.917808 val_top1=0.741800 val_top5=0.951800 loss=12640.625298 time: 29.975037
[Epoch 102] train=0.917768 val_top1=0.741600 val_top5=0.951900 loss=12567.722313 time: 30.042006
[Epoch 103] train=0.920312 val_top1=0.742100 val_top5=0.952000 loss=12232.517677 time: 30.024848
[Epoch 104] train=0.918810 val_top1=0.741900 val_top5=0.952500 loss=12391.600609 time: 30.120486
[Epoch 105] train=0.918389 val_top1=0.742400 val_top5=0.951200 loss=12611.324921 time: 30.003832
[Epoch 106] train=0.917568 val_top1=0.742300 val_top5=0.951800 loss=12515.587032 time: 29.908129
[Epoch 107] train=0.917067 val_top1=0.742100 val_top5=0.951000 loss=12557.628464 time: 29.978723
[Epoch 108] train=0.917368 val_top1=0.741600 val_top5=0.951900 loss=12705.376976 time: 30.160086
[Epoch 109] train=0.917288 val_top1=0.743400 val_top5=0.951600 loss=12500.575207 time: 30.126647
[Epoch 110] train=0.920413 val_top1=0.741500 val_top5=0.950900 loss=12394.755501 time: 30.159507
[Epoch 111] train=0.918750 val_top1=0.742200 val_top5=0.951700 loss=12537.619686 time: 30.005887
[Epoch 112] train=0.917588 val_top1=0.741600 val_top5=0.951900 loss=12704.238537 time: 30.031560
[Epoch 113] train=0.919631 val_top1=0.741400 val_top5=0.951800 loss=12428.243221 time: 29.932395
[Epoch 114] train=0.919030 val_top1=0.743100 val_top5=0.951800 loss=12571.889881 time: 30.034681
[Epoch 115] train=0.918149 val_top1=0.741600 val_top5=0.951800 loss=12369.957222 time: 30.068681
[Epoch 116] train=0.919832 val_top1=0.740600 val_top5=0.951800 loss=12291.412682 time: 29.839038
[Epoch 117] train=0.919752 val_top1=0.742500 val_top5=0.950900 loss=12243.491222 time: 29.933478
[Epoch 118] train=0.917788 val_top1=0.742300 val_top5=0.952000 loss=12507.650249 time: 30.091425
[Epoch 119] train=0.918710 val_top1=0.742900 val_top5=0.951000 loss=12399.733730 time: 30.142879
Done.
