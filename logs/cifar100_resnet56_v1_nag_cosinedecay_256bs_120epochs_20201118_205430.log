Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fcb0a8a8890>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 5
Warmup Learning Rate Mode: linear
Learing Rate Mode: cosine
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [30, 60, 90, inf]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.096174 val_top1=0.161600 val_top5=0.517200 loss=157345.916809 time: 30.481065
[Epoch 1] train=0.200641 val_top1=0.211700 val_top5=0.610900 loss=129413.528748 time: 30.253779
[Epoch 2] train=0.287119 val_top1=0.240100 val_top5=0.621300 loss=114737.832581 time: 30.476258
[Epoch 3] train=0.362800 val_top1=0.381600 val_top5=0.774600 loss=101787.789490 time: 30.468811
[Epoch 4] train=0.420833 val_top1=0.413700 val_top5=0.793900 loss=91886.163727 time: 30.476184
[Epoch 5] train=0.475841 val_top1=0.461100 val_top5=0.831000 loss=83002.287628 time: 30.396996
[Epoch 6] train=0.518349 val_top1=0.529000 val_top5=0.872900 loss=75851.544220 time: 30.341481
[Epoch 7] train=0.550721 val_top1=0.501100 val_top5=0.849200 loss=70241.849731 time: 30.309352
[Epoch 8] train=0.578245 val_top1=0.541000 val_top5=0.887700 loss=66045.552521 time: 30.300730
[Epoch 9] train=0.597336 val_top1=0.557400 val_top5=0.885500 loss=62584.198395 time: 30.245099
[Epoch 10] train=0.618470 val_top1=0.605300 val_top5=0.905700 loss=59452.499573 time: 30.077764
[Epoch 11] train=0.631130 val_top1=0.597600 val_top5=0.910500 loss=57290.465744 time: 30.448014
[Epoch 12] train=0.645533 val_top1=0.620500 val_top5=0.916100 loss=55050.619827 time: 30.199467
[Epoch 13] train=0.657492 val_top1=0.628300 val_top5=0.915000 loss=53060.796906 time: 30.351991
[Epoch 14] train=0.667648 val_top1=0.620000 val_top5=0.917500 loss=51427.470490 time: 30.413650
[Epoch 15] train=0.678546 val_top1=0.623300 val_top5=0.918100 loss=49704.559952 time: 30.214121
[Epoch 16] train=0.691446 val_top1=0.652300 val_top5=0.925200 loss=47835.714066 time: 30.283746
[Epoch 17] train=0.697396 val_top1=0.636300 val_top5=0.923200 loss=46401.458313 time: 30.499240
[Epoch 18] train=0.704367 val_top1=0.648500 val_top5=0.921400 loss=45625.076584 time: 30.306312
[Epoch 19] train=0.712720 val_top1=0.644500 val_top5=0.921900 loss=44216.586716 time: 30.484893
[Epoch 20] train=0.722236 val_top1=0.648500 val_top5=0.919400 loss=42919.189438 time: 30.296854
[Epoch 21] train=0.727264 val_top1=0.657000 val_top5=0.931300 loss=41943.348297 time: 30.517889
[Epoch 22] train=0.730589 val_top1=0.663400 val_top5=0.933300 loss=41322.406235 time: 30.243719
[Epoch 23] train=0.737360 val_top1=0.643500 val_top5=0.912800 loss=40293.942810 time: 30.371541
[Epoch 24] train=0.744531 val_top1=0.684100 val_top5=0.940500 loss=39197.881287 time: 30.389695
[Epoch 25] train=0.749920 val_top1=0.663600 val_top5=0.933500 loss=38450.017029 time: 30.349117
[Epoch 26] train=0.751883 val_top1=0.669200 val_top5=0.931900 loss=37805.977402 time: 30.379617
[Epoch 27] train=0.758534 val_top1=0.686800 val_top5=0.936100 loss=36962.159836 time: 30.464540
[Epoch 28] train=0.766887 val_top1=0.658600 val_top5=0.925300 loss=36064.404663 time: 30.223078
[Epoch 29] train=0.769151 val_top1=0.653800 val_top5=0.919500 loss=35120.153656 time: 30.218600
[Epoch 30] train=0.769712 val_top1=0.683000 val_top5=0.939900 loss=34978.871521 time: 30.187128
[Epoch 31] train=0.777224 val_top1=0.693600 val_top5=0.940800 loss=33809.997452 time: 30.261084
[Epoch 32] train=0.783173 val_top1=0.685100 val_top5=0.939400 loss=33248.029373 time: 30.190933
[Epoch 33] train=0.785377 val_top1=0.676300 val_top5=0.930400 loss=32781.108955 time: 30.435738
[Epoch 34] train=0.788582 val_top1=0.675500 val_top5=0.930900 loss=32211.245369 time: 30.349571
[Epoch 35] train=0.792788 val_top1=0.660700 val_top5=0.927600 loss=31534.036583 time: 30.319070
[Epoch 36] train=0.795994 val_top1=0.681700 val_top5=0.935300 loss=30923.844284 time: 30.361851
[Epoch 37] train=0.797676 val_top1=0.690400 val_top5=0.938100 loss=30845.039795 time: 30.405480
[Epoch 38] train=0.803205 val_top1=0.680400 val_top5=0.938800 loss=29773.690804 time: 30.277308
[Epoch 39] train=0.808213 val_top1=0.703000 val_top5=0.947700 loss=29309.083282 time: 30.201308
[Epoch 40] train=0.810236 val_top1=0.709800 val_top5=0.942300 loss=28584.636879 time: 30.452178
[Epoch 41] train=0.813722 val_top1=0.685900 val_top5=0.938100 loss=28404.759499 time: 30.234912
[Epoch 42] train=0.817648 val_top1=0.695100 val_top5=0.940300 loss=27603.519791 time: 30.090118
[Epoch 43] train=0.822396 val_top1=0.698400 val_top5=0.940300 loss=26975.179657 time: 30.387696
[Epoch 44] train=0.822316 val_top1=0.685200 val_top5=0.940200 loss=26689.917358 time: 30.214065
[Epoch 45] train=0.827584 val_top1=0.703300 val_top5=0.939100 loss=25974.916557 time: 30.211058
[Epoch 46] train=0.830028 val_top1=0.687000 val_top5=0.939200 loss=25745.617981 time: 30.225416
[Epoch 47] train=0.833594 val_top1=0.689400 val_top5=0.932400 loss=24767.385895 time: 30.357523
[Epoch 48] train=0.835076 val_top1=0.688500 val_top5=0.933300 loss=24900.605927 time: 30.167645
[Epoch 49] train=0.840284 val_top1=0.702100 val_top5=0.941300 loss=24102.207787 time: 30.460200
[Epoch 50] train=0.843950 val_top1=0.707800 val_top5=0.946300 loss=23615.547531 time: 30.005770
[Epoch 51] train=0.847696 val_top1=0.702600 val_top5=0.939800 loss=22975.054970 time: 30.300922
[Epoch 52] train=0.846314 val_top1=0.705300 val_top5=0.944700 loss=23045.350998 time: 31.870094
[Epoch 53] train=0.851562 val_top1=0.695600 val_top5=0.939900 loss=22238.150902 time: 30.244043
[Epoch 54] train=0.852224 val_top1=0.718600 val_top5=0.943600 loss=22129.815773 time: 30.320254
[Epoch 55] train=0.858113 val_top1=0.705700 val_top5=0.940100 loss=21443.695351 time: 30.374432
[Epoch 56] train=0.859796 val_top1=0.698700 val_top5=0.935400 loss=21120.803040 time: 30.261107
[Epoch 57] train=0.864643 val_top1=0.714600 val_top5=0.941600 loss=20343.480698 time: 30.296033
[Epoch 58] train=0.869511 val_top1=0.701200 val_top5=0.940800 loss=19506.672478 time: 30.276459
[Epoch 59] train=0.872396 val_top1=0.722200 val_top5=0.942700 loss=19159.467583 time: 30.370602
[Epoch 60] train=0.872616 val_top1=0.718000 val_top5=0.944100 loss=18911.814651 time: 30.233128
[Epoch 61] train=0.876763 val_top1=0.696300 val_top5=0.941300 loss=18189.187893 time: 30.346642
[Epoch 62] train=0.879968 val_top1=0.719000 val_top5=0.949900 loss=17854.123749 time: 30.464994
[Epoch 63] train=0.881150 val_top1=0.711700 val_top5=0.942700 loss=17609.725311 time: 31.883116
[Epoch 64] train=0.885116 val_top1=0.715000 val_top5=0.950700 loss=16977.937859 time: 30.162091
[Epoch 65] train=0.889343 val_top1=0.719900 val_top5=0.947600 loss=16310.992935 time: 30.294950
[Epoch 66] train=0.890665 val_top1=0.722900 val_top5=0.948700 loss=16237.525520 time: 30.218749
[Epoch 67] train=0.894050 val_top1=0.725000 val_top5=0.946900 loss=15644.677082 time: 30.218871
[Epoch 68] train=0.898638 val_top1=0.718100 val_top5=0.948600 loss=15069.804554 time: 30.275842
[Epoch 69] train=0.900621 val_top1=0.715300 val_top5=0.942600 loss=14681.426571 time: 30.445862
[Epoch 70] train=0.905589 val_top1=0.725100 val_top5=0.946700 loss=14166.499561 time: 30.370223
[Epoch 71] train=0.906851 val_top1=0.731400 val_top5=0.948900 loss=13915.353577 time: 31.571366
[Epoch 72] train=0.911759 val_top1=0.732700 val_top5=0.949900 loss=13101.417984 time: 30.426375
[Epoch 73] train=0.914103 val_top1=0.730600 val_top5=0.945200 loss=12599.805145 time: 30.340897
[Epoch 74] train=0.915605 val_top1=0.734900 val_top5=0.948700 loss=12594.935299 time: 30.423985
[Epoch 75] train=0.919972 val_top1=0.737300 val_top5=0.946600 loss=11850.939684 time: 30.417748
[Epoch 76] train=0.923638 val_top1=0.725700 val_top5=0.949300 loss=11428.844507 time: 30.515087
[Epoch 77] train=0.926502 val_top1=0.731800 val_top5=0.946000 loss=10969.616217 time: 30.461934
[Epoch 78] train=0.930228 val_top1=0.732500 val_top5=0.948200 loss=10445.925440 time: 30.393106
[Epoch 79] train=0.931971 val_top1=0.727000 val_top5=0.948300 loss=10229.610409 time: 30.272214
[Epoch 80] train=0.935136 val_top1=0.733800 val_top5=0.949300 loss=9598.298918 time: 30.359939
[Epoch 81] train=0.938722 val_top1=0.731000 val_top5=0.948700 loss=9225.480318 time: 30.400041
[Epoch 82] train=0.939683 val_top1=0.743000 val_top5=0.950200 loss=8982.079821 time: 31.817324
[Epoch 83] train=0.943229 val_top1=0.737600 val_top5=0.951300 loss=8552.852510 time: 30.358311
[Epoch 84] train=0.946675 val_top1=0.742100 val_top5=0.950300 loss=7959.645552 time: 30.192462
[Epoch 85] train=0.951362 val_top1=0.738900 val_top5=0.950500 loss=7341.958120 time: 30.346145
[Epoch 86] train=0.951583 val_top1=0.738900 val_top5=0.950800 loss=7236.136402 time: 30.502559
[Epoch 87] train=0.953866 val_top1=0.744700 val_top5=0.949300 loss=6994.780624 time: 30.257945
[Epoch 88] train=0.955990 val_top1=0.745300 val_top5=0.951800 loss=6645.346230 time: 30.389368
[Epoch 89] train=0.961038 val_top1=0.742600 val_top5=0.951200 loss=6035.287539 time: 30.245919
[Epoch 90] train=0.963442 val_top1=0.752800 val_top5=0.951100 loss=5607.505778 time: 30.452366
[Epoch 91] train=0.964724 val_top1=0.745500 val_top5=0.950200 loss=5384.240031 time: 30.262591
[Epoch 92] train=0.965345 val_top1=0.748500 val_top5=0.953600 loss=5316.247237 time: 30.378018
[Epoch 93] train=0.968029 val_top1=0.751600 val_top5=0.953900 loss=4954.148099 time: 30.352198
[Epoch 94] train=0.971094 val_top1=0.755400 val_top5=0.953900 loss=4504.629283 time: 30.350568
[Epoch 95] train=0.973237 val_top1=0.756000 val_top5=0.954900 loss=4230.894585 time: 30.096925
[Epoch 96] train=0.973337 val_top1=0.753500 val_top5=0.955600 loss=4162.074667 time: 30.260020
[Epoch 97] train=0.974800 val_top1=0.755400 val_top5=0.956000 loss=4024.758259 time: 30.383565
[Epoch 98] train=0.977244 val_top1=0.755700 val_top5=0.955600 loss=3683.839498 time: 30.337500
[Epoch 99] train=0.978085 val_top1=0.756800 val_top5=0.955800 loss=3506.973972 time: 30.374072
[Epoch 100] train=0.979147 val_top1=0.758300 val_top5=0.955300 loss=3371.761092 time: 30.345568
[Epoch 101] train=0.981090 val_top1=0.758200 val_top5=0.955300 loss=3205.349669 time: 30.362693
[Epoch 102] train=0.981030 val_top1=0.757900 val_top5=0.954700 loss=3209.981118 time: 30.308921
[Epoch 103] train=0.982632 val_top1=0.757800 val_top5=0.957000 loss=2977.106864 time: 30.264189
[Epoch 104] train=0.982492 val_top1=0.755300 val_top5=0.956600 loss=2876.124565 time: 30.270987
[Epoch 105] train=0.984315 val_top1=0.756400 val_top5=0.955000 loss=2745.437671 time: 30.164354
[Epoch 106] train=0.984655 val_top1=0.756700 val_top5=0.956200 loss=2607.398460 time: 30.270736
[Epoch 107] train=0.984515 val_top1=0.755400 val_top5=0.955800 loss=2590.104629 time: 30.169104
[Epoch 108] train=0.985497 val_top1=0.757600 val_top5=0.955900 loss=2535.401863 time: 30.275697
[Epoch 109] train=0.985557 val_top1=0.758800 val_top5=0.954800 loss=2470.987046 time: 30.284724
[Epoch 110] train=0.986278 val_top1=0.760100 val_top5=0.955500 loss=2386.561652 time: 31.486970
[Epoch 111] train=0.986759 val_top1=0.761000 val_top5=0.955600 loss=2301.032508 time: 31.686086
[Epoch 112] train=0.987059 val_top1=0.761700 val_top5=0.956700 loss=2284.984138 time: 30.205426
[Epoch 113] train=0.986398 val_top1=0.761400 val_top5=0.955900 loss=2372.113565 time: 30.346365
[Epoch 114] train=0.987119 val_top1=0.761000 val_top5=0.956100 loss=2255.578990 time: 30.355315
[Epoch 115] train=0.986318 val_top1=0.761000 val_top5=0.955800 loss=2377.432566 time: 30.260685
[Epoch 116] train=0.987720 val_top1=0.759400 val_top5=0.955700 loss=2200.495604 time: 30.361787
[Epoch 117] train=0.987800 val_top1=0.760900 val_top5=0.955700 loss=2178.927874 time: 30.390832
[Epoch 118] train=0.987660 val_top1=0.760900 val_top5=0.956200 loss=2219.203570 time: 30.466611
[Epoch 119] train=0.987400 val_top1=0.759900 val_top5=0.955500 loss=2217.916521 time: 30.378474
Done.
