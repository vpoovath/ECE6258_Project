Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fd2d170a750>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 10
Warmup Learning Rate Mode: linear
Learing Rate Mode: cosine
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [30, 60, 90, inf]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.080349 val_top1=0.143100 val_top5=0.475200 loss=163716.550476 time: 30.387068
[Epoch 1] train=0.178446 val_top1=0.236100 val_top5=0.614200 loss=133069.494263 time: 30.248475
[Epoch 2] train=0.269792 val_top1=0.280300 val_top5=0.654700 loss=117869.814148 time: 30.286518
[Epoch 3] train=0.344411 val_top1=0.289300 val_top5=0.662300 loss=105102.801086 time: 30.246342
[Epoch 4] train=0.399479 val_top1=0.311400 val_top5=0.709000 loss=95422.339691 time: 30.234074
[Epoch 5] train=0.444451 val_top1=0.411300 val_top5=0.783300 loss=87805.678558 time: 30.215363
[Epoch 6] train=0.482632 val_top1=0.439100 val_top5=0.793500 loss=81951.292938 time: 30.280761
[Epoch 7] train=0.518309 val_top1=0.489100 val_top5=0.838500 loss=75778.325989 time: 30.418367
[Epoch 8] train=0.540625 val_top1=0.427300 val_top5=0.818800 loss=71769.965271 time: 30.361459
[Epoch 9] train=0.565044 val_top1=0.538700 val_top5=0.887500 loss=68178.640045 time: 30.309433
[Epoch 10] train=0.587260 val_top1=0.547300 val_top5=0.886300 loss=64473.972839 time: 30.369948
[Epoch 11] train=0.607292 val_top1=0.584100 val_top5=0.896100 loss=61397.990601 time: 30.396910
[Epoch 12] train=0.624399 val_top1=0.614000 val_top5=0.912500 loss=58412.314880 time: 30.329333
[Epoch 13] train=0.635757 val_top1=0.599200 val_top5=0.907100 loss=56140.279343 time: 30.112881
[Epoch 14] train=0.652364 val_top1=0.631400 val_top5=0.914800 loss=53658.278748 time: 30.390747
[Epoch 15] train=0.667889 val_top1=0.623400 val_top5=0.912300 loss=51569.978638 time: 30.338640
[Epoch 16] train=0.676643 val_top1=0.634300 val_top5=0.920900 loss=50178.431488 time: 30.430412
[Epoch 17] train=0.684075 val_top1=0.648400 val_top5=0.923900 loss=48811.410751 time: 30.356360
[Epoch 18] train=0.695913 val_top1=0.640200 val_top5=0.922300 loss=47400.986511 time: 30.392190
[Epoch 19] train=0.705689 val_top1=0.639300 val_top5=0.916800 loss=45818.638412 time: 30.210368
[Epoch 20] train=0.713001 val_top1=0.654000 val_top5=0.926600 loss=44479.283844 time: 30.310152
[Epoch 21] train=0.717067 val_top1=0.639500 val_top5=0.917300 loss=43667.825211 time: 30.370280
[Epoch 22] train=0.724840 val_top1=0.668600 val_top5=0.928500 loss=42282.729675 time: 30.153647
[Epoch 23] train=0.728005 val_top1=0.646600 val_top5=0.928600 loss=41824.893524 time: 30.162668
[Epoch 24] train=0.735677 val_top1=0.655000 val_top5=0.930200 loss=40286.012894 time: 30.188894
[Epoch 25] train=0.742909 val_top1=0.647600 val_top5=0.926100 loss=39682.728500 time: 30.361475
[Epoch 26] train=0.745893 val_top1=0.664000 val_top5=0.930500 loss=38730.764267 time: 30.367454
[Epoch 27] train=0.755469 val_top1=0.653000 val_top5=0.925700 loss=37733.027222 time: 30.360028
[Epoch 28] train=0.755809 val_top1=0.669300 val_top5=0.932900 loss=37307.517715 time: 30.514952
[Epoch 29] train=0.764663 val_top1=0.665700 val_top5=0.934800 loss=36230.024490 time: 30.416717
[Epoch 30] train=0.766406 val_top1=0.665300 val_top5=0.924600 loss=35627.835632 time: 30.245028
[Epoch 31] train=0.774459 val_top1=0.685000 val_top5=0.941400 loss=34657.526062 time: 30.331681
[Epoch 32] train=0.773277 val_top1=0.665900 val_top5=0.930000 loss=34370.695450 time: 30.295514
[Epoch 33] train=0.781170 val_top1=0.681800 val_top5=0.938700 loss=33495.282692 time: 30.451594
[Epoch 34] train=0.784836 val_top1=0.676900 val_top5=0.933900 loss=32837.648407 time: 30.353309
[Epoch 35] train=0.787560 val_top1=0.691800 val_top5=0.938100 loss=32395.897217 time: 30.414652
[Epoch 36] train=0.790184 val_top1=0.689200 val_top5=0.936200 loss=32013.152107 time: 30.288068
[Epoch 37] train=0.795613 val_top1=0.688100 val_top5=0.930100 loss=31034.776138 time: 30.142324
[Epoch 38] train=0.797496 val_top1=0.690400 val_top5=0.937500 loss=30805.806885 time: 30.358703
[Epoch 39] train=0.801923 val_top1=0.693000 val_top5=0.938600 loss=30150.493629 time: 30.430964
[Epoch 40] train=0.805108 val_top1=0.682400 val_top5=0.933300 loss=29740.308388 time: 30.470787
[Epoch 41] train=0.813121 val_top1=0.689500 val_top5=0.938000 loss=28754.794579 time: 30.448443
[Epoch 42] train=0.813221 val_top1=0.696000 val_top5=0.941300 loss=28164.885391 time: 30.277485
[Epoch 43] train=0.816486 val_top1=0.714300 val_top5=0.946900 loss=28077.457993 time: 30.350405
[Epoch 44] train=0.819351 val_top1=0.692600 val_top5=0.939400 loss=27331.614883 time: 30.330036
[Epoch 45] train=0.818790 val_top1=0.707500 val_top5=0.940100 loss=27225.124100 time: 30.301231
[Epoch 46] train=0.824459 val_top1=0.710900 val_top5=0.939800 loss=26589.709496 time: 30.406377
[Epoch 47] train=0.830869 val_top1=0.688600 val_top5=0.938900 loss=25613.798332 time: 30.584185
[Epoch 48] train=0.829768 val_top1=0.705000 val_top5=0.943500 loss=25693.599174 time: 30.196063
[Epoch 49] train=0.835737 val_top1=0.715600 val_top5=0.944900 loss=24784.528114 time: 30.346859
[Epoch 50] train=0.838261 val_top1=0.712600 val_top5=0.946800 loss=24460.861267 time: 30.234730
[Epoch 51] train=0.837901 val_top1=0.708200 val_top5=0.950000 loss=24314.209770 time: 30.272414
[Epoch 52] train=0.845413 val_top1=0.697500 val_top5=0.941800 loss=23490.289841 time: 30.331865
[Epoch 53] train=0.848698 val_top1=0.702600 val_top5=0.942800 loss=22885.101517 time: 30.332404
[Epoch 54] train=0.848638 val_top1=0.716700 val_top5=0.945000 loss=22676.151894 time: 30.522094
[Epoch 55] train=0.850901 val_top1=0.690000 val_top5=0.940500 loss=22169.262825 time: 30.347452
[Epoch 56] train=0.857252 val_top1=0.718500 val_top5=0.943800 loss=21432.221191 time: 30.205568
[Epoch 57] train=0.860577 val_top1=0.706400 val_top5=0.946000 loss=20904.494308 time: 30.406038
[Epoch 58] train=0.864002 val_top1=0.708700 val_top5=0.944700 loss=20220.123711 time: 30.512180
[Epoch 59] train=0.862720 val_top1=0.697000 val_top5=0.937600 loss=20361.620480 time: 30.402875
[Epoch 60] train=0.867768 val_top1=0.703600 val_top5=0.941800 loss=19802.314846 time: 30.560292
[Epoch 61] train=0.871394 val_top1=0.725600 val_top5=0.945600 loss=19164.732517 time: 30.283268
[Epoch 62] train=0.873898 val_top1=0.697400 val_top5=0.937200 loss=18695.647514 time: 30.282382
[Epoch 63] train=0.878626 val_top1=0.707500 val_top5=0.942200 loss=18196.900948 time: 32.000295
[Epoch 64] train=0.880188 val_top1=0.721300 val_top5=0.947500 loss=17697.866379 time: 30.334003
[Epoch 65] train=0.882592 val_top1=0.720100 val_top5=0.948100 loss=17710.871162 time: 30.252371
[Epoch 66] train=0.886158 val_top1=0.720000 val_top5=0.946400 loss=17059.253380 time: 30.486880
[Epoch 67] train=0.891907 val_top1=0.726900 val_top5=0.944000 loss=16138.374081 time: 30.190365
[Epoch 68] train=0.895994 val_top1=0.724600 val_top5=0.947900 loss=15667.661858 time: 30.399277
[Epoch 69] train=0.895272 val_top1=0.715500 val_top5=0.943100 loss=15308.759869 time: 30.274839
[Epoch 70] train=0.897937 val_top1=0.724900 val_top5=0.948500 loss=15098.996861 time: 30.496093
[Epoch 71] train=0.904387 val_top1=0.720200 val_top5=0.948600 loss=14342.795547 time: 30.284393
[Epoch 72] train=0.905228 val_top1=0.739000 val_top5=0.952400 loss=14004.028862 time: 30.193528
[Epoch 73] train=0.910857 val_top1=0.726600 val_top5=0.949800 loss=13343.073395 time: 30.280916
[Epoch 74] train=0.911558 val_top1=0.732400 val_top5=0.950200 loss=13030.685856 time: 30.520053
[Epoch 75] train=0.917208 val_top1=0.729700 val_top5=0.948700 loss=12254.197872 time: 30.357681
[Epoch 76] train=0.918650 val_top1=0.743600 val_top5=0.952500 loss=12024.211281 time: 30.180397
[Epoch 77] train=0.924018 val_top1=0.731700 val_top5=0.945600 loss=11495.585190 time: 30.214068
[Epoch 78] train=0.922656 val_top1=0.737500 val_top5=0.952000 loss=11355.535902 time: 30.298820
[Epoch 79] train=0.928405 val_top1=0.744500 val_top5=0.954400 loss=10724.390411 time: 30.262775
[Epoch 80] train=0.930869 val_top1=0.732100 val_top5=0.951500 loss=10248.389139 time: 30.161904
[Epoch 81] train=0.933634 val_top1=0.746000 val_top5=0.952700 loss=9794.560394 time: 30.300928
[Epoch 82] train=0.937159 val_top1=0.742800 val_top5=0.955800 loss=9251.429186 time: 30.217597
[Epoch 83] train=0.940865 val_top1=0.747400 val_top5=0.954200 loss=8878.693428 time: 30.168031
[Epoch 84] train=0.944050 val_top1=0.746400 val_top5=0.952100 loss=8380.770182 time: 30.353281
[Epoch 85] train=0.946334 val_top1=0.741900 val_top5=0.953000 loss=7944.171511 time: 30.313990
[Epoch 86] train=0.946254 val_top1=0.749300 val_top5=0.954000 loss=7836.544315 time: 31.598563
[Epoch 87] train=0.952404 val_top1=0.747000 val_top5=0.954200 loss=7273.484850 time: 30.151477
[Epoch 88] train=0.953546 val_top1=0.751400 val_top5=0.954700 loss=6938.474335 time: 30.334656
[Epoch 89] train=0.957933 val_top1=0.751800 val_top5=0.953300 loss=6290.574345 time: 30.426385
[Epoch 90] train=0.959856 val_top1=0.748900 val_top5=0.952400 loss=6099.714194 time: 30.315493
[Epoch 91] train=0.962620 val_top1=0.748000 val_top5=0.953200 loss=5708.683252 time: 30.498607
[Epoch 92] train=0.963462 val_top1=0.756700 val_top5=0.957700 loss=5616.614310 time: 30.393594
[Epoch 93] train=0.965705 val_top1=0.753700 val_top5=0.956500 loss=5237.922200 time: 30.325791
[Epoch 94] train=0.969331 val_top1=0.761600 val_top5=0.955400 loss=4658.519794 time: 30.458542
[Epoch 95] train=0.972015 val_top1=0.756800 val_top5=0.953800 loss=4341.781613 time: 31.886890
[Epoch 96] train=0.972636 val_top1=0.757900 val_top5=0.956800 loss=4354.349535 time: 30.265596
[Epoch 97] train=0.974379 val_top1=0.760000 val_top5=0.956900 loss=4136.840182 time: 30.220183
[Epoch 98] train=0.977163 val_top1=0.758300 val_top5=0.956200 loss=3639.817645 time: 30.516520
[Epoch 99] train=0.977564 val_top1=0.759400 val_top5=0.956300 loss=3581.750422 time: 30.291045
[Epoch 100] train=0.978726 val_top1=0.756500 val_top5=0.955100 loss=3458.570271 time: 30.286682
[Epoch 101] train=0.978846 val_top1=0.757900 val_top5=0.957600 loss=3400.003797 time: 30.209595
[Epoch 102] train=0.979547 val_top1=0.760900 val_top5=0.956000 loss=3290.533457 time: 30.275420
[Epoch 103] train=0.981751 val_top1=0.758300 val_top5=0.956600 loss=2966.982999 time: 30.358615
[Epoch 104] train=0.982452 val_top1=0.763600 val_top5=0.956300 loss=2898.385582 time: 30.241842
[Epoch 105] train=0.983994 val_top1=0.762700 val_top5=0.957400 loss=2766.060125 time: 30.329734
[Epoch 106] train=0.984696 val_top1=0.763600 val_top5=0.956500 loss=2623.834140 time: 30.395303
[Epoch 107] train=0.983794 val_top1=0.763000 val_top5=0.956300 loss=2664.664905 time: 30.544570
[Epoch 108] train=0.984716 val_top1=0.766700 val_top5=0.956600 loss=2466.224598 time: 30.478767
[Epoch 109] train=0.985276 val_top1=0.764000 val_top5=0.956600 loss=2552.223565 time: 30.563860
[Epoch 110] train=0.985317 val_top1=0.765000 val_top5=0.956300 loss=2463.922162 time: 30.323226
[Epoch 111] train=0.986258 val_top1=0.765600 val_top5=0.955900 loss=2373.902611 time: 30.251403
[Epoch 112] train=0.986799 val_top1=0.764500 val_top5=0.956200 loss=2223.050155 time: 30.153097
[Epoch 113] train=0.986438 val_top1=0.763000 val_top5=0.954800 loss=2362.828477 time: 30.426199
[Epoch 114] train=0.987901 val_top1=0.765700 val_top5=0.954800 loss=2175.501411 time: 30.247158
[Epoch 115] train=0.987139 val_top1=0.765900 val_top5=0.955600 loss=2184.463013 time: 30.300488
[Epoch 116] train=0.987119 val_top1=0.765600 val_top5=0.956100 loss=2220.909157 time: 31.177718
[Epoch 117] train=0.987500 val_top1=0.766100 val_top5=0.955300 loss=2246.314433 time: 30.693332
[Epoch 118] train=0.987360 val_top1=0.765100 val_top5=0.954900 loss=2263.882220 time: 30.315516
[Epoch 119] train=0.986859 val_top1=0.765800 val_top5=0.955900 loss=2319.822364 time: 30.129542
Done.
