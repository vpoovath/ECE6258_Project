Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fe1bfe03a90>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 20
Warmup Learning Rate Mode: linear
Learing Rate Mode: cosine
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [30, 60, 90, inf]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.071715 val_top1=0.119200 val_top5=0.420900 loss=170647.666931 time: 31.283647
[Epoch 1] train=0.150000 val_top1=0.179000 val_top5=0.527100 loss=137243.000732 time: 30.948457
[Epoch 2] train=0.220513 val_top1=0.218600 val_top5=0.597700 loss=126251.372803 time: 31.210198
[Epoch 3] train=0.279347 val_top1=0.264800 val_top5=0.666500 loss=115841.711304 time: 31.166111
[Epoch 4] train=0.338381 val_top1=0.317800 val_top5=0.707900 loss=106201.832397 time: 30.943604
[Epoch 5] train=0.392508 val_top1=0.398500 val_top5=0.784400 loss=97344.334991 time: 31.094831
[Epoch 6] train=0.430489 val_top1=0.426200 val_top5=0.804800 loss=90376.347015 time: 32.499150
[Epoch 7] train=0.467107 val_top1=0.474900 val_top5=0.825600 loss=84331.365997 time: 31.068809
[Epoch 8] train=0.500381 val_top1=0.458900 val_top5=0.815700 loss=79116.306274 time: 31.151086
[Epoch 9] train=0.524599 val_top1=0.471900 val_top5=0.816100 loss=74734.897095 time: 31.014126
[Epoch 10] train=0.545012 val_top1=0.522700 val_top5=0.867600 loss=71417.539673 time: 31.092536
[Epoch 11] train=0.568269 val_top1=0.517600 val_top5=0.861700 loss=68078.059753 time: 30.996101
[Epoch 12] train=0.583534 val_top1=0.552900 val_top5=0.878300 loss=65086.284760 time: 31.291100
[Epoch 13] train=0.598998 val_top1=0.500300 val_top5=0.849200 loss=62792.941101 time: 31.308192
[Epoch 14] train=0.613261 val_top1=0.580900 val_top5=0.890000 loss=60597.241425 time: 30.964994
[Epoch 15] train=0.623518 val_top1=0.611000 val_top5=0.908600 loss=58790.124756 time: 32.446367
[Epoch 16] train=0.633093 val_top1=0.575000 val_top5=0.886500 loss=57080.253372 time: 30.958144
[Epoch 17] train=0.646875 val_top1=0.522100 val_top5=0.870200 loss=54995.268555 time: 31.177535
[Epoch 18] train=0.653946 val_top1=0.614100 val_top5=0.914300 loss=53933.765274 time: 31.226587
[Epoch 19] train=0.663562 val_top1=0.531100 val_top5=0.870500 loss=52352.393585 time: 31.235860
[Epoch 20] train=0.672035 val_top1=0.625200 val_top5=0.908600 loss=50874.455505 time: 31.096724
[Epoch 21] train=0.683634 val_top1=0.610100 val_top5=0.904000 loss=48810.452057 time: 31.069716
[Epoch 22] train=0.692208 val_top1=0.619700 val_top5=0.908400 loss=47643.284256 time: 31.227704
[Epoch 23] train=0.700942 val_top1=0.649600 val_top5=0.921700 loss=46035.471985 time: 31.164581
[Epoch 24] train=0.708894 val_top1=0.633100 val_top5=0.915300 loss=44962.904266 time: 31.337757
[Epoch 25] train=0.719491 val_top1=0.638300 val_top5=0.927700 loss=43462.680618 time: 31.403763
[Epoch 26] train=0.723478 val_top1=0.664100 val_top5=0.931700 loss=42575.620941 time: 31.160588
[Epoch 27] train=0.730008 val_top1=0.667400 val_top5=0.929000 loss=41425.967560 time: 31.051618
[Epoch 28] train=0.733594 val_top1=0.661300 val_top5=0.931800 loss=40856.166275 time: 30.960991
[Epoch 29] train=0.743850 val_top1=0.629600 val_top5=0.912700 loss=39740.989670 time: 31.173322
[Epoch 30] train=0.745333 val_top1=0.668000 val_top5=0.934200 loss=39218.906540 time: 31.257045
[Epoch 31] train=0.753225 val_top1=0.664800 val_top5=0.933700 loss=38005.988403 time: 31.167958
[Epoch 32] train=0.757432 val_top1=0.679000 val_top5=0.937900 loss=37261.806885 time: 31.181163
[Epoch 33] train=0.763922 val_top1=0.687900 val_top5=0.933700 loss=36261.666672 time: 31.251524
[Epoch 34] train=0.766987 val_top1=0.669800 val_top5=0.927700 loss=35683.513367 time: 31.106330
[Epoch 35] train=0.770413 val_top1=0.680400 val_top5=0.934600 loss=34993.060165 time: 31.156526
[Epoch 36] train=0.775921 val_top1=0.686400 val_top5=0.930300 loss=34052.844528 time: 30.845688
[Epoch 37] train=0.778065 val_top1=0.675800 val_top5=0.930600 loss=33771.162094 time: 31.056981
[Epoch 38] train=0.783814 val_top1=0.685900 val_top5=0.935500 loss=32899.579185 time: 31.023205
[Epoch 39] train=0.788301 val_top1=0.682800 val_top5=0.939600 loss=32238.184036 time: 30.996730
[Epoch 40] train=0.789483 val_top1=0.671800 val_top5=0.930500 loss=32053.210396 time: 31.003103
[Epoch 41] train=0.794692 val_top1=0.698400 val_top5=0.943000 loss=31207.409737 time: 31.079530
[Epoch 42] train=0.798197 val_top1=0.686600 val_top5=0.936100 loss=30727.483505 time: 31.133168
[Epoch 43] train=0.800521 val_top1=0.693200 val_top5=0.944800 loss=30159.642342 time: 31.121124
[Epoch 44] train=0.804688 val_top1=0.702600 val_top5=0.944900 loss=29633.408134 time: 31.045738
[Epoch 45] train=0.809896 val_top1=0.691100 val_top5=0.934700 loss=28949.360611 time: 31.054909
[Epoch 46] train=0.810958 val_top1=0.691600 val_top5=0.938800 loss=28432.835358 time: 31.343655
[Epoch 47] train=0.812901 val_top1=0.695400 val_top5=0.936600 loss=28176.987816 time: 31.309930
[Epoch 48] train=0.818630 val_top1=0.707000 val_top5=0.944000 loss=27380.728966 time: 31.350810
[Epoch 49] train=0.823137 val_top1=0.704400 val_top5=0.944000 loss=26895.144310 time: 31.414283
[Epoch 50] train=0.824820 val_top1=0.700100 val_top5=0.946100 loss=26465.636246 time: 31.183846
[Epoch 51] train=0.829788 val_top1=0.688100 val_top5=0.934400 loss=25835.934624 time: 31.278117
[Epoch 52] train=0.830629 val_top1=0.714500 val_top5=0.945400 loss=25699.360077 time: 31.128968
[Epoch 53] train=0.834535 val_top1=0.688100 val_top5=0.935000 loss=24786.127312 time: 31.263289
[Epoch 54] train=0.837079 val_top1=0.701600 val_top5=0.943200 loss=24454.390228 time: 31.240222
[Epoch 55] train=0.838902 val_top1=0.698200 val_top5=0.935400 loss=24189.391083 time: 31.333555
[Epoch 56] train=0.843329 val_top1=0.713300 val_top5=0.946400 loss=23393.957611 time: 31.232670
[Epoch 57] train=0.844471 val_top1=0.695900 val_top5=0.934600 loss=23279.051979 time: 31.326269
[Epoch 58] train=0.849599 val_top1=0.705900 val_top5=0.942400 loss=22472.627182 time: 31.056584
[Epoch 59] train=0.852324 val_top1=0.712300 val_top5=0.941600 loss=22259.432854 time: 31.198466
[Epoch 60] train=0.855268 val_top1=0.709700 val_top5=0.943400 loss=21589.946632 time: 31.211545
[Epoch 61] train=0.856891 val_top1=0.724000 val_top5=0.948800 loss=21322.177925 time: 31.229437
[Epoch 62] train=0.863802 val_top1=0.714500 val_top5=0.944100 loss=20451.062771 time: 31.356722
[Epoch 63] train=0.867648 val_top1=0.710500 val_top5=0.940000 loss=19865.114765 time: 31.528949
[Epoch 64] train=0.868149 val_top1=0.719700 val_top5=0.943700 loss=19773.261665 time: 31.343492
[Epoch 65] train=0.871234 val_top1=0.718800 val_top5=0.943700 loss=19226.476780 time: 31.148866
[Epoch 66] train=0.875100 val_top1=0.719300 val_top5=0.944400 loss=18610.777107 time: 31.125980
[Epoch 67] train=0.881210 val_top1=0.723000 val_top5=0.947000 loss=17949.052265 time: 31.076509
[Epoch 68] train=0.881070 val_top1=0.711600 val_top5=0.941200 loss=17726.636829 time: 31.131916
[Epoch 69] train=0.884054 val_top1=0.725800 val_top5=0.948600 loss=17225.355549 time: 31.481729
[Epoch 70] train=0.886378 val_top1=0.721500 val_top5=0.951400 loss=16845.414177 time: 31.274953
[Epoch 71] train=0.889323 val_top1=0.729300 val_top5=0.945800 loss=16346.296177 time: 31.173515
[Epoch 72] train=0.894091 val_top1=0.719200 val_top5=0.945600 loss=15686.859341 time: 31.112593
[Epoch 73] train=0.895232 val_top1=0.728800 val_top5=0.948400 loss=15433.576191 time: 31.033166
[Epoch 74] train=0.901723 val_top1=0.732100 val_top5=0.946500 loss=14513.671196 time: 31.364559
[Epoch 75] train=0.902544 val_top1=0.731400 val_top5=0.951300 loss=14301.012505 time: 31.273675
[Epoch 76] train=0.907352 val_top1=0.725400 val_top5=0.950400 loss=13719.524731 time: 31.288659
[Epoch 77] train=0.909776 val_top1=0.730300 val_top5=0.945800 loss=13466.586731 time: 31.175122
[Epoch 78] train=0.915825 val_top1=0.737800 val_top5=0.949300 loss=12572.044159 time: 31.031780
[Epoch 79] train=0.919251 val_top1=0.737100 val_top5=0.950400 loss=11986.141678 time: 31.230807
[Epoch 80] train=0.919371 val_top1=0.739100 val_top5=0.945200 loss=11928.782917 time: 31.065416
[Epoch 81] train=0.924399 val_top1=0.736800 val_top5=0.948700 loss=11211.013237 time: 31.205574
[Epoch 82] train=0.928105 val_top1=0.747800 val_top5=0.948900 loss=10520.902506 time: 30.983596
[Epoch 83] train=0.932853 val_top1=0.744000 val_top5=0.948400 loss=10091.484951 time: 31.001467
[Epoch 84] train=0.937240 val_top1=0.738200 val_top5=0.952600 loss=9298.792208 time: 31.204502
[Epoch 85] train=0.939002 val_top1=0.742000 val_top5=0.949800 loss=9168.952688 time: 30.999181
[Epoch 86] train=0.942127 val_top1=0.739500 val_top5=0.950300 loss=8804.847088 time: 31.121199
[Epoch 87] train=0.943830 val_top1=0.743000 val_top5=0.946400 loss=8368.088781 time: 31.172993
[Epoch 88] train=0.945893 val_top1=0.747200 val_top5=0.953400 loss=8093.180244 time: 31.185004
[Epoch 89] train=0.950020 val_top1=0.747700 val_top5=0.951600 loss=7518.756281 time: 31.214131
[Epoch 90] train=0.952003 val_top1=0.751500 val_top5=0.950100 loss=7199.167822 time: 31.214455
[Epoch 91] train=0.953486 val_top1=0.751400 val_top5=0.949500 loss=6810.363495 time: 31.196481
[Epoch 92] train=0.959575 val_top1=0.759600 val_top5=0.952300 loss=6251.708065 time: 31.358386
[Epoch 93] train=0.960517 val_top1=0.755600 val_top5=0.953800 loss=5955.865404 time: 31.232717
[Epoch 94] train=0.964223 val_top1=0.754100 val_top5=0.951200 loss=5474.255220 time: 31.211442
[Epoch 95] train=0.967107 val_top1=0.764000 val_top5=0.954100 loss=5107.542656 time: 31.155300
[Epoch 96] train=0.968570 val_top1=0.759600 val_top5=0.953800 loss=4855.012586 time: 31.217400
[Epoch 97] train=0.971655 val_top1=0.760900 val_top5=0.953600 loss=4476.140876 time: 31.123511
[Epoch 98] train=0.972596 val_top1=0.760500 val_top5=0.953600 loss=4282.909410 time: 31.284694
[Epoch 99] train=0.974539 val_top1=0.762700 val_top5=0.953500 loss=4038.311502 time: 31.430774
[Epoch 100] train=0.974920 val_top1=0.764600 val_top5=0.952800 loss=3957.914376 time: 31.161701
[Epoch 101] train=0.977344 val_top1=0.763600 val_top5=0.953300 loss=3639.993292 time: 31.245642
[Epoch 102] train=0.978205 val_top1=0.768000 val_top5=0.953700 loss=3498.008712 time: 31.205117
[Epoch 103] train=0.978846 val_top1=0.769200 val_top5=0.954800 loss=3341.445553 time: 31.295204
[Epoch 104] train=0.980288 val_top1=0.767500 val_top5=0.955700 loss=3175.897974 time: 31.185661
[Epoch 105] train=0.981711 val_top1=0.767300 val_top5=0.955300 loss=2993.173844 time: 31.194952
[Epoch 106] train=0.982752 val_top1=0.768400 val_top5=0.955100 loss=2840.626826 time: 31.119830
[Epoch 107] train=0.982933 val_top1=0.768600 val_top5=0.954200 loss=2740.277308 time: 31.140204
[Epoch 108] train=0.984415 val_top1=0.768400 val_top5=0.954800 loss=2638.766977 time: 31.235627
[Epoch 109] train=0.984095 val_top1=0.767500 val_top5=0.953600 loss=2596.352165 time: 31.134320
[Epoch 110] train=0.984876 val_top1=0.769300 val_top5=0.954000 loss=2521.796288 time: 31.219557
[Epoch 111] train=0.985737 val_top1=0.772600 val_top5=0.954300 loss=2484.790009 time: 31.121937
[Epoch 112] train=0.985938 val_top1=0.770000 val_top5=0.954100 loss=2413.356452 time: 32.754968
[Epoch 113] train=0.986018 val_top1=0.770700 val_top5=0.954300 loss=2437.715497 time: 31.148286
[Epoch 114] train=0.984876 val_top1=0.769300 val_top5=0.953700 loss=2521.610780 time: 31.192869
[Epoch 115] train=0.985657 val_top1=0.769200 val_top5=0.954800 loss=2402.191716 time: 31.408105
[Epoch 116] train=0.986659 val_top1=0.771300 val_top5=0.953600 loss=2239.376871 time: 31.217495
[Epoch 117] train=0.986659 val_top1=0.769000 val_top5=0.954200 loss=2277.294595 time: 31.290752
[Epoch 118] train=0.986699 val_top1=0.770600 val_top5=0.953400 loss=2292.176410 time: 31.135031
[Epoch 119] train=0.987560 val_top1=0.769900 val_top5=0.954000 loss=2218.335163 time: 31.043960
Done.
