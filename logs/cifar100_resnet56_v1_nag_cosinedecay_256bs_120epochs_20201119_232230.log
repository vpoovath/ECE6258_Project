Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fd0a0262790>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 10
Warmup Learning Rate Mode: linear
Learing Rate Mode: cosine
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [40, 80]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.085296 val_top1=0.158300 val_top5=0.493600 loss=163062.357056 time: 31.298808
[Epoch 1] train=0.191827 val_top1=0.193900 val_top5=0.540100 loss=130340.617065 time: 32.331047
[Epoch 2] train=0.282532 val_top1=0.302200 val_top5=0.711900 loss=114911.125916 time: 31.141171
[Epoch 3] train=0.353005 val_top1=0.303200 val_top5=0.718300 loss=103233.672089 time: 30.906033
[Epoch 4] train=0.409575 val_top1=0.376400 val_top5=0.790900 loss=93905.162201 time: 31.113616
[Epoch 5] train=0.450962 val_top1=0.449300 val_top5=0.825500 loss=86699.301178 time: 30.941621
[Epoch 6] train=0.488702 val_top1=0.412700 val_top5=0.791500 loss=80793.517853 time: 31.345391
[Epoch 7] train=0.516887 val_top1=0.411800 val_top5=0.768300 loss=75952.619873 time: 31.009427
[Epoch 8] train=0.540986 val_top1=0.471300 val_top5=0.847100 loss=71971.284698 time: 30.920631
[Epoch 9] train=0.563582 val_top1=0.544300 val_top5=0.879700 loss=68349.385101 time: 31.057057
[Epoch 10] train=0.585397 val_top1=0.568500 val_top5=0.894600 loss=64903.059937 time: 30.978471
[Epoch 11] train=0.603726 val_top1=0.535700 val_top5=0.870900 loss=61959.448334 time: 31.114807
[Epoch 12] train=0.619271 val_top1=0.587100 val_top5=0.890500 loss=59008.657974 time: 32.808132
[Epoch 13] train=0.638922 val_top1=0.593300 val_top5=0.902200 loss=56357.360458 time: 31.006337
[Epoch 14] train=0.647276 val_top1=0.576500 val_top5=0.892100 loss=54604.072601 time: 30.946826
[Epoch 15] train=0.662901 val_top1=0.634300 val_top5=0.919300 loss=52515.225616 time: 30.890705
[Epoch 16] train=0.672196 val_top1=0.604200 val_top5=0.900800 loss=50767.732697 time: 30.993619
[Epoch 17] train=0.682752 val_top1=0.617300 val_top5=0.916500 loss=49192.515289 time: 30.897089
[Epoch 18] train=0.688782 val_top1=0.598200 val_top5=0.900200 loss=47897.310440 time: 31.218889
[Epoch 19] train=0.697736 val_top1=0.586400 val_top5=0.890500 loss=46647.580872 time: 31.175972
[Epoch 20] train=0.706030 val_top1=0.636700 val_top5=0.921300 loss=45299.236145 time: 31.053121
[Epoch 21] train=0.712019 val_top1=0.631800 val_top5=0.919100 loss=44197.689301 time: 31.065682
[Epoch 22] train=0.719191 val_top1=0.673400 val_top5=0.932100 loss=43140.906555 time: 31.219967
[Epoch 23] train=0.729447 val_top1=0.658700 val_top5=0.928300 loss=41870.858292 time: 30.970977
[Epoch 24] train=0.733794 val_top1=0.655100 val_top5=0.921200 loss=41150.575485 time: 31.033301
[Epoch 25] train=0.739523 val_top1=0.662300 val_top5=0.936600 loss=40235.706161 time: 31.404032
[Epoch 26] train=0.744351 val_top1=0.653700 val_top5=0.932600 loss=39392.782257 time: 32.313863
[Epoch 27] train=0.748898 val_top1=0.662300 val_top5=0.930300 loss=38404.212479 time: 30.878549
[Epoch 28] train=0.754687 val_top1=0.670200 val_top5=0.931900 loss=37742.957748 time: 31.018513
[Epoch 29] train=0.757812 val_top1=0.696600 val_top5=0.941600 loss=36959.867798 time: 30.889840
[Epoch 30] train=0.759275 val_top1=0.658000 val_top5=0.930400 loss=36673.257843 time: 31.137907
[Epoch 31] train=0.769531 val_top1=0.676600 val_top5=0.934800 loss=35333.114136 time: 30.987683
[Epoch 32] train=0.771174 val_top1=0.657100 val_top5=0.922300 loss=34903.044647 time: 31.109769
[Epoch 33] train=0.776743 val_top1=0.680500 val_top5=0.938000 loss=34307.807358 time: 30.906827
[Epoch 34] train=0.781450 val_top1=0.676800 val_top5=0.933400 loss=33334.681252 time: 30.975448
[Epoch 35] train=0.783834 val_top1=0.642700 val_top5=0.915800 loss=33193.791870 time: 31.084942
[Epoch 36] train=0.786278 val_top1=0.693500 val_top5=0.941000 loss=32426.653488 time: 32.632780
[Epoch 37] train=0.792007 val_top1=0.686900 val_top5=0.941800 loss=31847.165512 time: 30.935569
[Epoch 38] train=0.796194 val_top1=0.700700 val_top5=0.941800 loss=31177.017677 time: 30.856581
[Epoch 39] train=0.800220 val_top1=0.698600 val_top5=0.937900 loss=30479.563538 time: 31.032809
[Epoch 40] train=0.803405 val_top1=0.665200 val_top5=0.923800 loss=29942.126640 time: 30.988674
[Epoch 41] train=0.806310 val_top1=0.651800 val_top5=0.916200 loss=29536.493248 time: 30.991043
[Epoch 42] train=0.810617 val_top1=0.667700 val_top5=0.925500 loss=28964.147949 time: 30.849509
[Epoch 43] train=0.816086 val_top1=0.689500 val_top5=0.938400 loss=28153.493843 time: 30.994239
[Epoch 44] train=0.816106 val_top1=0.705600 val_top5=0.941100 loss=27804.220222 time: 31.015554
[Epoch 45] train=0.819351 val_top1=0.687300 val_top5=0.935100 loss=27362.332306 time: 31.060293
[Epoch 46] train=0.822075 val_top1=0.684300 val_top5=0.933500 loss=26874.025604 time: 31.006635
[Epoch 47] train=0.824219 val_top1=0.700200 val_top5=0.941100 loss=26506.519821 time: 30.988530
[Epoch 48] train=0.827504 val_top1=0.700200 val_top5=0.934700 loss=26065.790306 time: 30.936066
[Epoch 49] train=0.831751 val_top1=0.692800 val_top5=0.934900 loss=25381.842545 time: 31.110270
[Epoch 50] train=0.834956 val_top1=0.713200 val_top5=0.941400 loss=25013.296989 time: 31.060746
[Epoch 51] train=0.837480 val_top1=0.703700 val_top5=0.941000 loss=24365.205414 time: 30.958032
[Epoch 52] train=0.840325 val_top1=0.684000 val_top5=0.932400 loss=24021.879303 time: 30.894206
[Epoch 53] train=0.844010 val_top1=0.710900 val_top5=0.945900 loss=23518.975632 time: 30.956138
[Epoch 54] train=0.846334 val_top1=0.699100 val_top5=0.941600 loss=22992.559624 time: 31.126293
[Epoch 55] train=0.851102 val_top1=0.697200 val_top5=0.936600 loss=22584.349541 time: 32.547004
[Epoch 56] train=0.853566 val_top1=0.719400 val_top5=0.943600 loss=22117.480431 time: 31.230618
[Epoch 57] train=0.857792 val_top1=0.699700 val_top5=0.934700 loss=21283.450340 time: 32.294348
[Epoch 58] train=0.858634 val_top1=0.702600 val_top5=0.934600 loss=21360.089760 time: 31.286830
[Epoch 59] train=0.863662 val_top1=0.710700 val_top5=0.939400 loss=20360.529411 time: 30.840986
[Epoch 60] train=0.866687 val_top1=0.710700 val_top5=0.944000 loss=20070.321007 time: 31.056820
[Epoch 61] train=0.867748 val_top1=0.709900 val_top5=0.944200 loss=19846.682983 time: 31.224207
[Epoch 62] train=0.870773 val_top1=0.706300 val_top5=0.945300 loss=19172.564690 time: 32.627146
[Epoch 63] train=0.872576 val_top1=0.727400 val_top5=0.949200 loss=18854.439125 time: 30.968975
[Epoch 64] train=0.879267 val_top1=0.724300 val_top5=0.949800 loss=18074.554283 time: 30.961127
[Epoch 65] train=0.881370 val_top1=0.722700 val_top5=0.950400 loss=17766.034027 time: 32.841662
[Epoch 66] train=0.884415 val_top1=0.714500 val_top5=0.945800 loss=17292.934002 time: 31.003349
[Epoch 67] train=0.887540 val_top1=0.719900 val_top5=0.946000 loss=16730.445580 time: 31.049039
[Epoch 68] train=0.890565 val_top1=0.719300 val_top5=0.944900 loss=16412.107952 time: 31.023603
[Epoch 69] train=0.894531 val_top1=0.706100 val_top5=0.943500 loss=15619.992100 time: 31.090239
[Epoch 70] train=0.897376 val_top1=0.726100 val_top5=0.947300 loss=15138.479919 time: 31.075095
[Epoch 71] train=0.898377 val_top1=0.719600 val_top5=0.946700 loss=15011.375954 time: 31.159038
[Epoch 72] train=0.902123 val_top1=0.719600 val_top5=0.946000 loss=14456.772812 time: 31.065060
[Epoch 73] train=0.905308 val_top1=0.720900 val_top5=0.946400 loss=13971.544216 time: 30.957578
[Epoch 74] train=0.909515 val_top1=0.724600 val_top5=0.946900 loss=13403.205055 time: 30.968213
[Epoch 75] train=0.912440 val_top1=0.732600 val_top5=0.951700 loss=12877.881470 time: 31.098526
[Epoch 76] train=0.916406 val_top1=0.730700 val_top5=0.945500 loss=12474.546272 time: 31.306580
[Epoch 77] train=0.920212 val_top1=0.731100 val_top5=0.947200 loss=12084.885555 time: 31.294020
[Epoch 78] train=0.921014 val_top1=0.730300 val_top5=0.948200 loss=11656.895647 time: 31.229673
[Epoch 79] train=0.926783 val_top1=0.740000 val_top5=0.951600 loss=10997.412540 time: 32.505933
[Epoch 80] train=0.928866 val_top1=0.739200 val_top5=0.952400 loss=10567.237604 time: 31.058535
[Epoch 81] train=0.931130 val_top1=0.733900 val_top5=0.952100 loss=10161.949482 time: 30.930334
[Epoch 82] train=0.932472 val_top1=0.742100 val_top5=0.953200 loss=9973.405951 time: 31.176826
[Epoch 83] train=0.939583 val_top1=0.744800 val_top5=0.954100 loss=9257.052044 time: 31.175604
[Epoch 84] train=0.941426 val_top1=0.743000 val_top5=0.952400 loss=8804.975014 time: 31.045431
[Epoch 85] train=0.942808 val_top1=0.736700 val_top5=0.951500 loss=8506.509987 time: 31.022110
[Epoch 86] train=0.945954 val_top1=0.748000 val_top5=0.949700 loss=8089.544044 time: 30.967377
[Epoch 87] train=0.948257 val_top1=0.747100 val_top5=0.952000 loss=7762.940287 time: 30.904092
[Epoch 88] train=0.952183 val_top1=0.748600 val_top5=0.951600 loss=7247.922331 time: 30.995410
[Epoch 89] train=0.954848 val_top1=0.747100 val_top5=0.952100 loss=6817.752419 time: 32.592649
[Epoch 90] train=0.956751 val_top1=0.753400 val_top5=0.951200 loss=6430.506761 time: 30.926549
[Epoch 91] train=0.960797 val_top1=0.754000 val_top5=0.953300 loss=5858.157485 time: 32.317812
[Epoch 92] train=0.961879 val_top1=0.755400 val_top5=0.953100 loss=5881.207367 time: 31.027983
[Epoch 93] train=0.965465 val_top1=0.757400 val_top5=0.952500 loss=5399.085262 time: 31.169313
[Epoch 94] train=0.965284 val_top1=0.757200 val_top5=0.952900 loss=5190.098413 time: 31.103948
[Epoch 95] train=0.967468 val_top1=0.759100 val_top5=0.955000 loss=4981.103827 time: 31.223824
[Epoch 96] train=0.970052 val_top1=0.760200 val_top5=0.951500 loss=4681.846620 time: 31.421219
[Epoch 97] train=0.972416 val_top1=0.761600 val_top5=0.952200 loss=4328.068341 time: 31.295206
[Epoch 98] train=0.972756 val_top1=0.757300 val_top5=0.953100 loss=4171.472933 time: 30.979851
[Epoch 99] train=0.974700 val_top1=0.755000 val_top5=0.952800 loss=3959.968624 time: 31.425533
[Epoch 100] train=0.976562 val_top1=0.760800 val_top5=0.955000 loss=3759.998743 time: 31.336282
[Epoch 101] train=0.978586 val_top1=0.758400 val_top5=0.954300 loss=3490.143970 time: 31.420244
[Epoch 102] train=0.978566 val_top1=0.758200 val_top5=0.953800 loss=3444.355293 time: 31.330248
[Epoch 103] train=0.979407 val_top1=0.762200 val_top5=0.955600 loss=3308.969514 time: 31.335627
[Epoch 104] train=0.981150 val_top1=0.763800 val_top5=0.955000 loss=3039.164385 time: 31.186730
[Epoch 105] train=0.982171 val_top1=0.763500 val_top5=0.955000 loss=2987.706627 time: 31.195201
[Epoch 106] train=0.981591 val_top1=0.764000 val_top5=0.955200 loss=3053.698055 time: 33.268742
[Epoch 107] train=0.982853 val_top1=0.761700 val_top5=0.954200 loss=2862.967538 time: 31.592233
[Epoch 108] train=0.983594 val_top1=0.762100 val_top5=0.954500 loss=2733.094629 time: 30.935565
[Epoch 109] train=0.985076 val_top1=0.763500 val_top5=0.953700 loss=2608.857906 time: 31.223164
[Epoch 110] train=0.985677 val_top1=0.763200 val_top5=0.953800 loss=2475.398344 time: 30.976464
[Epoch 111] train=0.984455 val_top1=0.763100 val_top5=0.953900 loss=2617.063068 time: 31.062861
[Epoch 112] train=0.985737 val_top1=0.762500 val_top5=0.954800 loss=2461.167604 time: 31.172109
[Epoch 113] train=0.985677 val_top1=0.762800 val_top5=0.954500 loss=2464.571312 time: 30.973923
[Epoch 114] train=0.985517 val_top1=0.764000 val_top5=0.952700 loss=2455.562488 time: 31.137152
[Epoch 115] train=0.987480 val_top1=0.764200 val_top5=0.953700 loss=2312.033113 time: 31.003879
[Epoch 116] train=0.986078 val_top1=0.766100 val_top5=0.953600 loss=2344.812396 time: 31.264156
[Epoch 117] train=0.987139 val_top1=0.763900 val_top5=0.953200 loss=2302.050267 time: 32.879754
[Epoch 118] train=0.986659 val_top1=0.764800 val_top5=0.954300 loss=2307.815585 time: 31.840068
[Epoch 119] train=0.985777 val_top1=0.764900 val_top5=0.953600 loss=2396.975629 time: 32.271778
Done.
