Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7f3cd779e810>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 5
Warmup Learning Rate Mode: linear
Learing Rate Mode: step
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [40, 80]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.100841 val_top1=0.145400 val_top5=0.487500 loss=156804.979614 time: 31.126825
[Epoch 1] train=0.207712 val_top1=0.249000 val_top5=0.631400 loss=128431.470093 time: 31.014914
[Epoch 2] train=0.298317 val_top1=0.293800 val_top5=0.685600 loss=112412.561951 time: 31.086707
[Epoch 3] train=0.377784 val_top1=0.419100 val_top5=0.799100 loss=99107.689575 time: 30.977985
[Epoch 4] train=0.440645 val_top1=0.337900 val_top5=0.675900 loss=88821.679596 time: 31.031067
[Epoch 5] train=0.489563 val_top1=0.456600 val_top5=0.831800 loss=80431.315979 time: 30.855363
[Epoch 6] train=0.531270 val_top1=0.509400 val_top5=0.853600 loss=73721.149933 time: 30.970116
[Epoch 7] train=0.561178 val_top1=0.532700 val_top5=0.873500 loss=68775.475769 time: 30.910133
[Epoch 8] train=0.587159 val_top1=0.526100 val_top5=0.873500 loss=64623.067200 time: 31.130502
[Epoch 9] train=0.606550 val_top1=0.597100 val_top5=0.903500 loss=61408.220642 time: 32.595409
[Epoch 10] train=0.621995 val_top1=0.578600 val_top5=0.897200 loss=58676.051544 time: 30.905187
[Epoch 11] train=0.637119 val_top1=0.592900 val_top5=0.881700 loss=56549.238342 time: 31.092514
[Epoch 12] train=0.648277 val_top1=0.613900 val_top5=0.916900 loss=54446.375214 time: 30.960910
[Epoch 13] train=0.662600 val_top1=0.620600 val_top5=0.911700 loss=52267.889191 time: 30.787422
[Epoch 14] train=0.673438 val_top1=0.625400 val_top5=0.914300 loss=50585.976715 time: 31.009465
[Epoch 15] train=0.682472 val_top1=0.640900 val_top5=0.926000 loss=49080.345642 time: 31.072664
[Epoch 16] train=0.690986 val_top1=0.650200 val_top5=0.923900 loss=47945.183289 time: 31.055879
[Epoch 17] train=0.697997 val_top1=0.634700 val_top5=0.918500 loss=46608.600067 time: 31.097247
[Epoch 18] train=0.708814 val_top1=0.648100 val_top5=0.927300 loss=45102.366760 time: 31.075758
[Epoch 19] train=0.715345 val_top1=0.643700 val_top5=0.929600 loss=44038.201447 time: 30.969807
[Epoch 20] train=0.721214 val_top1=0.649500 val_top5=0.922700 loss=42904.684952 time: 30.987115
[Epoch 21] train=0.728686 val_top1=0.679500 val_top5=0.938300 loss=41894.205231 time: 31.039104
[Epoch 22] train=0.735156 val_top1=0.665300 val_top5=0.934300 loss=40929.798248 time: 31.007617
[Epoch 23] train=0.738502 val_top1=0.622400 val_top5=0.913800 loss=40276.509003 time: 31.025462
[Epoch 24] train=0.743590 val_top1=0.655500 val_top5=0.932400 loss=39475.784546 time: 31.050640
[Epoch 25] train=0.748017 val_top1=0.680200 val_top5=0.939900 loss=38533.401535 time: 30.963390
[Epoch 26] train=0.753706 val_top1=0.669000 val_top5=0.937000 loss=37862.166092 time: 30.863602
[Epoch 27] train=0.757993 val_top1=0.663300 val_top5=0.937900 loss=37195.219986 time: 31.256486
[Epoch 28] train=0.760317 val_top1=0.641000 val_top5=0.921500 loss=36672.261292 time: 30.883352
[Epoch 29] train=0.768450 val_top1=0.655200 val_top5=0.924800 loss=35661.408958 time: 32.262672
[Epoch 30] train=0.766987 val_top1=0.675400 val_top5=0.937900 loss=35611.186554 time: 31.001049
[Epoch 31] train=0.772536 val_top1=0.672400 val_top5=0.938500 loss=34754.248917 time: 31.081687
[Epoch 32] train=0.777284 val_top1=0.663600 val_top5=0.923000 loss=34091.217873 time: 30.903859
[Epoch 33] train=0.780068 val_top1=0.669100 val_top5=0.926900 loss=33756.613640 time: 31.070930
[Epoch 34] train=0.783694 val_top1=0.702500 val_top5=0.945900 loss=33212.793625 time: 30.851561
[Epoch 35] train=0.784155 val_top1=0.686600 val_top5=0.942000 loss=32986.405731 time: 31.038432
[Epoch 36] train=0.789964 val_top1=0.702800 val_top5=0.943000 loss=32149.358887 time: 31.152928
[Epoch 37] train=0.796234 val_top1=0.683800 val_top5=0.935100 loss=31524.488190 time: 31.080162
[Epoch 38] train=0.794331 val_top1=0.681900 val_top5=0.935800 loss=31446.098824 time: 30.944849
[Epoch 39] train=0.795172 val_top1=0.689700 val_top5=0.942800 loss=31151.588379 time: 32.953618
[Epoch 40] train=0.797656 val_top1=0.703100 val_top5=0.940700 loss=30842.005615 time: 30.949841
[Epoch 41] train=0.803325 val_top1=0.667900 val_top5=0.931400 loss=30199.743668 time: 30.987484
[Epoch 42] train=0.803045 val_top1=0.684600 val_top5=0.933900 loss=29751.399521 time: 31.050687
[Epoch 43] train=0.809375 val_top1=0.686100 val_top5=0.934300 loss=29008.360550 time: 31.092981
[Epoch 44] train=0.807051 val_top1=0.692800 val_top5=0.939600 loss=29330.399864 time: 30.882501
[Epoch 45] train=0.849900 val_top1=0.760300 val_top5=0.960200 loss=23248.896210 time: 30.951877
[Epoch 46] train=0.882031 val_top1=0.762900 val_top5=0.959900 loss=18264.802700 time: 31.223675
[Epoch 47] train=0.888562 val_top1=0.760600 val_top5=0.960600 loss=17129.585480 time: 30.971368
[Epoch 48] train=0.896615 val_top1=0.762600 val_top5=0.958300 loss=16049.937218 time: 31.068575
[Epoch 49] train=0.898538 val_top1=0.760400 val_top5=0.960700 loss=15576.391823 time: 31.167958
[Epoch 50] train=0.902444 val_top1=0.762800 val_top5=0.961600 loss=14955.452839 time: 31.135054
[Epoch 51] train=0.908053 val_top1=0.760600 val_top5=0.959700 loss=14332.968384 time: 31.153752
[Epoch 52] train=0.906811 val_top1=0.756900 val_top5=0.959700 loss=14309.644154 time: 30.820673
[Epoch 53] train=0.910857 val_top1=0.764100 val_top5=0.960200 loss=13630.915363 time: 31.000598
[Epoch 54] train=0.911739 val_top1=0.760500 val_top5=0.959600 loss=13514.903862 time: 31.088184
[Epoch 55] train=0.916607 val_top1=0.758800 val_top5=0.960200 loss=12884.993031 time: 31.132270
[Epoch 56] train=0.916867 val_top1=0.760400 val_top5=0.959100 loss=12641.813904 time: 30.944248
[Epoch 57] train=0.918830 val_top1=0.759100 val_top5=0.959200 loss=12430.064350 time: 31.135647
[Epoch 58] train=0.921795 val_top1=0.759000 val_top5=0.959300 loss=12009.384922 time: 32.463268
[Epoch 59] train=0.923177 val_top1=0.759900 val_top5=0.959000 loss=11831.313572 time: 30.872671
[Epoch 60] train=0.923117 val_top1=0.758200 val_top5=0.959100 loss=11574.988514 time: 31.068363
[Epoch 61] train=0.924339 val_top1=0.760300 val_top5=0.958700 loss=11431.003124 time: 30.918656
[Epoch 62] train=0.927063 val_top1=0.759300 val_top5=0.957800 loss=11123.204296 time: 31.017990
[Epoch 63] train=0.928245 val_top1=0.757400 val_top5=0.957200 loss=10787.174589 time: 30.927068
[Epoch 64] train=0.929748 val_top1=0.758700 val_top5=0.958800 loss=10652.621731 time: 31.059360
[Epoch 65] train=0.930449 val_top1=0.758100 val_top5=0.957200 loss=10508.105911 time: 31.166811
[Epoch 66] train=0.931410 val_top1=0.756800 val_top5=0.956400 loss=10347.871572 time: 30.964897
[Epoch 67] train=0.933774 val_top1=0.755600 val_top5=0.957300 loss=10043.665455 time: 31.040000
[Epoch 68] train=0.934235 val_top1=0.759100 val_top5=0.955900 loss=9996.323494 time: 32.353214
[Epoch 69] train=0.933313 val_top1=0.753800 val_top5=0.958100 loss=9989.823412 time: 31.144134
[Epoch 70] train=0.936278 val_top1=0.758400 val_top5=0.956200 loss=9618.430841 time: 32.470177
[Epoch 71] train=0.937500 val_top1=0.757000 val_top5=0.955500 loss=9478.933725 time: 30.928378
[Epoch 72] train=0.936719 val_top1=0.760400 val_top5=0.956000 loss=9443.785995 time: 30.978634
[Epoch 73] train=0.939143 val_top1=0.756300 val_top5=0.956700 loss=9168.713261 time: 30.853422
[Epoch 74] train=0.940565 val_top1=0.756800 val_top5=0.957600 loss=9007.612345 time: 31.003137
[Epoch 75] train=0.941607 val_top1=0.755800 val_top5=0.955300 loss=8772.261265 time: 30.798228
[Epoch 76] train=0.940805 val_top1=0.760400 val_top5=0.956200 loss=8887.097895 time: 30.864513
[Epoch 77] train=0.941366 val_top1=0.754100 val_top5=0.956700 loss=8823.672655 time: 30.987583
[Epoch 78] train=0.942728 val_top1=0.755600 val_top5=0.956100 loss=8455.032555 time: 30.716640
[Epoch 79] train=0.943209 val_top1=0.758400 val_top5=0.957300 loss=8535.878233 time: 31.008805
[Epoch 80] train=0.943950 val_top1=0.760100 val_top5=0.955700 loss=8288.943041 time: 31.193353
[Epoch 81] train=0.945272 val_top1=0.755300 val_top5=0.953700 loss=8176.575346 time: 31.094988
[Epoch 82] train=0.945112 val_top1=0.752500 val_top5=0.957000 loss=8080.548653 time: 30.909102
[Epoch 83] train=0.946494 val_top1=0.756400 val_top5=0.953200 loss=7990.268305 time: 31.072914
[Epoch 84] train=0.946274 val_top1=0.754200 val_top5=0.954400 loss=7937.653839 time: 30.899969
[Epoch 85] train=0.953045 val_top1=0.759100 val_top5=0.956900 loss=7128.712114 time: 30.973500
[Epoch 86] train=0.957492 val_top1=0.761300 val_top5=0.957700 loss=6580.720961 time: 30.823411
[Epoch 87] train=0.959215 val_top1=0.762000 val_top5=0.958900 loss=6378.968549 time: 30.970569
[Epoch 88] train=0.959275 val_top1=0.759600 val_top5=0.958300 loss=6356.650003 time: 32.446461
[Epoch 89] train=0.960938 val_top1=0.762100 val_top5=0.957600 loss=6142.284866 time: 31.037956
[Epoch 90] train=0.961118 val_top1=0.761500 val_top5=0.958500 loss=6033.054585 time: 30.933409
[Epoch 91] train=0.961518 val_top1=0.760700 val_top5=0.957500 loss=5965.450830 time: 31.107172
[Epoch 92] train=0.962059 val_top1=0.762300 val_top5=0.957800 loss=6012.831094 time: 30.989747
[Epoch 93] train=0.961959 val_top1=0.761500 val_top5=0.957900 loss=5921.648033 time: 30.831934
[Epoch 94] train=0.963381 val_top1=0.760500 val_top5=0.957600 loss=5823.249150 time: 30.784927
[Epoch 95] train=0.961919 val_top1=0.762800 val_top5=0.957800 loss=5901.989917 time: 30.770754
[Epoch 96] train=0.963782 val_top1=0.761900 val_top5=0.959000 loss=5784.770940 time: 32.234995
[Epoch 97] train=0.962640 val_top1=0.762100 val_top5=0.957500 loss=5698.437451 time: 31.105100
[Epoch 98] train=0.964904 val_top1=0.763100 val_top5=0.957300 loss=5623.556112 time: 32.654935
[Epoch 99] train=0.963662 val_top1=0.761600 val_top5=0.958400 loss=5570.402205 time: 31.002771
[Epoch 100] train=0.963702 val_top1=0.761400 val_top5=0.957800 loss=5595.575064 time: 30.799060
[Epoch 101] train=0.964744 val_top1=0.761500 val_top5=0.957600 loss=5606.315879 time: 30.944188
[Epoch 102] train=0.964022 val_top1=0.762900 val_top5=0.957700 loss=5676.351693 time: 30.867646
[Epoch 103] train=0.964223 val_top1=0.762200 val_top5=0.957900 loss=5486.013645 time: 30.938695
[Epoch 104] train=0.964463 val_top1=0.761800 val_top5=0.958200 loss=5531.849445 time: 31.025285
[Epoch 105] train=0.964623 val_top1=0.760600 val_top5=0.957400 loss=5410.980036 time: 32.617619
[Epoch 106] train=0.965004 val_top1=0.762300 val_top5=0.958300 loss=5369.716372 time: 30.812464
[Epoch 107] train=0.965365 val_top1=0.761000 val_top5=0.957200 loss=5454.487798 time: 30.859933
[Epoch 108] train=0.964623 val_top1=0.761600 val_top5=0.957200 loss=5437.732496 time: 30.925449
[Epoch 109] train=0.966146 val_top1=0.762800 val_top5=0.957200 loss=5318.824574 time: 31.154347
[Epoch 110] train=0.965645 val_top1=0.760500 val_top5=0.957900 loss=5368.785476 time: 31.179377
[Epoch 111] train=0.964964 val_top1=0.760100 val_top5=0.957300 loss=5415.591088 time: 31.170616
[Epoch 112] train=0.967829 val_top1=0.761500 val_top5=0.958000 loss=5167.754167 time: 30.992841
[Epoch 113] train=0.965865 val_top1=0.762700 val_top5=0.958100 loss=5343.391934 time: 30.998652
[Epoch 114] train=0.965505 val_top1=0.761300 val_top5=0.957900 loss=5346.397653 time: 30.979396
[Epoch 115] train=0.965425 val_top1=0.762100 val_top5=0.958000 loss=5261.761446 time: 30.582844
[Epoch 116] train=0.966907 val_top1=0.762800 val_top5=0.957800 loss=5171.571932 time: 30.825812
[Epoch 117] train=0.967228 val_top1=0.761900 val_top5=0.957000 loss=5108.436965 time: 30.850469
[Epoch 118] train=0.966406 val_top1=0.761900 val_top5=0.958100 loss=5226.461843 time: 30.888771
[Epoch 119] train=0.966366 val_top1=0.761800 val_top5=0.958600 loss=5200.786086 time: 31.037845
Done.
