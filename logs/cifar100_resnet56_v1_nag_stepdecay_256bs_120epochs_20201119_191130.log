Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fb57be80890>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 10
Warmup Learning Rate Mode: linear
Learing Rate Mode: step
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [40, 80]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.085497 val_top1=0.134200 val_top5=0.449000 loss=163139.247681 time: 31.146793
[Epoch 1] train=0.183714 val_top1=0.242700 val_top5=0.615700 loss=132578.864258 time: 30.704227
[Epoch 2] train=0.268950 val_top1=0.241700 val_top5=0.601500 loss=118265.796570 time: 30.940249
[Epoch 3] train=0.340024 val_top1=0.343900 val_top5=0.750700 loss=105580.193878 time: 32.323779
[Epoch 4] train=0.398057 val_top1=0.421500 val_top5=0.808800 loss=95740.044525 time: 30.928858
[Epoch 5] train=0.449018 val_top1=0.420400 val_top5=0.808300 loss=87223.394318 time: 30.944157
[Epoch 6] train=0.486739 val_top1=0.451200 val_top5=0.831800 loss=80990.401245 time: 30.849223
[Epoch 7] train=0.520653 val_top1=0.477800 val_top5=0.841700 loss=75265.856018 time: 30.858917
[Epoch 8] train=0.547236 val_top1=0.509400 val_top5=0.869800 loss=70973.979004 time: 30.902359
[Epoch 9] train=0.567849 val_top1=0.508300 val_top5=0.883200 loss=67604.839386 time: 31.172062
[Epoch 10] train=0.590705 val_top1=0.566200 val_top5=0.895100 loss=64184.285980 time: 31.033739
[Epoch 11] train=0.609355 val_top1=0.574100 val_top5=0.897200 loss=61149.164581 time: 31.104034
[Epoch 12] train=0.624099 val_top1=0.560300 val_top5=0.884300 loss=58117.631714 time: 31.050026
[Epoch 13] train=0.641567 val_top1=0.568700 val_top5=0.893300 loss=55702.874695 time: 30.787075
[Epoch 14] train=0.653185 val_top1=0.614600 val_top5=0.922600 loss=53568.395615 time: 30.916706
[Epoch 15] train=0.665064 val_top1=0.603500 val_top5=0.912800 loss=51767.630524 time: 31.083253
[Epoch 16] train=0.676983 val_top1=0.640400 val_top5=0.921100 loss=49982.213821 time: 30.723451
[Epoch 17] train=0.684034 val_top1=0.606200 val_top5=0.900800 loss=48699.756943 time: 32.188537
[Epoch 18] train=0.697175 val_top1=0.642700 val_top5=0.928500 loss=46813.640640 time: 31.004896
[Epoch 19] train=0.703466 val_top1=0.646000 val_top5=0.918300 loss=45655.307007 time: 30.828721
[Epoch 20] train=0.710677 val_top1=0.659900 val_top5=0.934100 loss=44669.739899 time: 31.071464
[Epoch 21] train=0.718650 val_top1=0.654000 val_top5=0.927400 loss=43391.332794 time: 31.057382
[Epoch 22] train=0.724018 val_top1=0.659900 val_top5=0.933700 loss=42365.517456 time: 32.222751
[Epoch 23] train=0.730469 val_top1=0.607700 val_top5=0.901400 loss=41560.384766 time: 30.906583
[Epoch 24] train=0.737099 val_top1=0.658000 val_top5=0.933000 loss=40495.654755 time: 30.997242
[Epoch 25] train=0.739103 val_top1=0.664300 val_top5=0.932100 loss=39978.212524 time: 30.952262
[Epoch 26] train=0.747296 val_top1=0.679700 val_top5=0.939000 loss=38950.837891 time: 31.110450
[Epoch 27] train=0.750000 val_top1=0.677000 val_top5=0.936700 loss=38189.054855 time: 31.054767
[Epoch 28] train=0.754026 val_top1=0.636300 val_top5=0.920000 loss=37600.769470 time: 32.452976
[Epoch 29] train=0.760136 val_top1=0.659800 val_top5=0.933400 loss=36693.315170 time: 31.086794
[Epoch 30] train=0.762139 val_top1=0.680100 val_top5=0.941500 loss=36298.720993 time: 30.831972
[Epoch 31] train=0.766827 val_top1=0.676600 val_top5=0.932300 loss=35729.547493 time: 31.105721
[Epoch 32] train=0.769531 val_top1=0.646400 val_top5=0.924000 loss=35146.923157 time: 30.783245
[Epoch 33] train=0.773838 val_top1=0.651500 val_top5=0.927400 loss=34596.107544 time: 30.935423
[Epoch 34] train=0.776322 val_top1=0.694000 val_top5=0.941900 loss=34197.116150 time: 30.694546
[Epoch 35] train=0.784816 val_top1=0.670700 val_top5=0.934800 loss=33168.178864 time: 30.816600
[Epoch 36] train=0.784696 val_top1=0.690000 val_top5=0.935300 loss=32828.284157 time: 30.923350
[Epoch 37] train=0.788822 val_top1=0.679700 val_top5=0.938000 loss=32259.084839 time: 30.915349
[Epoch 38] train=0.791647 val_top1=0.653700 val_top5=0.922500 loss=31899.709450 time: 31.015331
[Epoch 39] train=0.791386 val_top1=0.692000 val_top5=0.942000 loss=31673.953934 time: 31.148036
[Epoch 40] train=0.794611 val_top1=0.685500 val_top5=0.936700 loss=31055.741150 time: 32.825952
[Epoch 41] train=0.798818 val_top1=0.698600 val_top5=0.939700 loss=30704.121353 time: 31.213579
[Epoch 42] train=0.799740 val_top1=0.710200 val_top5=0.947400 loss=30520.380768 time: 30.890164
[Epoch 43] train=0.800240 val_top1=0.686900 val_top5=0.941500 loss=30375.987045 time: 30.889350
[Epoch 44] train=0.804387 val_top1=0.683300 val_top5=0.937200 loss=29709.285278 time: 30.853532
[Epoch 45] train=0.805108 val_top1=0.679700 val_top5=0.934700 loss=29554.290375 time: 30.943714
[Epoch 46] train=0.809375 val_top1=0.646500 val_top5=0.920900 loss=28788.442062 time: 30.877791
[Epoch 47] train=0.809034 val_top1=0.695000 val_top5=0.941200 loss=28975.263748 time: 30.915213
[Epoch 48] train=0.811018 val_top1=0.672200 val_top5=0.929800 loss=28577.830223 time: 30.852960
[Epoch 49] train=0.812360 val_top1=0.684600 val_top5=0.938300 loss=28261.392708 time: 30.883197
[Epoch 50] train=0.854067 val_top1=0.751600 val_top5=0.957800 loss=22450.447121 time: 32.401086
[Epoch 51] train=0.887881 val_top1=0.758500 val_top5=0.958300 loss=17369.995625 time: 31.035895
[Epoch 52] train=0.893650 val_top1=0.759400 val_top5=0.959400 loss=16214.784370 time: 30.902995
[Epoch 53] train=0.900160 val_top1=0.760000 val_top5=0.958700 loss=15253.733017 time: 31.038605
[Epoch 54] train=0.904167 val_top1=0.760900 val_top5=0.959600 loss=14634.854782 time: 31.103434
[Epoch 55] train=0.908654 val_top1=0.760100 val_top5=0.959200 loss=14080.544041 time: 31.156584
[Epoch 56] train=0.909675 val_top1=0.760900 val_top5=0.956500 loss=13830.149906 time: 30.904868
[Epoch 57] train=0.913522 val_top1=0.759400 val_top5=0.957900 loss=13260.401264 time: 31.020545
[Epoch 58] train=0.915084 val_top1=0.760100 val_top5=0.958600 loss=12896.666668 time: 31.074453
[Epoch 59] train=0.918109 val_top1=0.760800 val_top5=0.956900 loss=12480.877953 time: 30.950552
[Epoch 60] train=0.920353 val_top1=0.758800 val_top5=0.954600 loss=12137.965141 time: 31.167948
[Epoch 61] train=0.925260 val_top1=0.758900 val_top5=0.957600 loss=11703.186298 time: 31.304197
[Epoch 62] train=0.923678 val_top1=0.764000 val_top5=0.955900 loss=11544.226208 time: 32.547889
[Epoch 63] train=0.923077 val_top1=0.765600 val_top5=0.957000 loss=11424.861713 time: 30.983722
[Epoch 64] train=0.926022 val_top1=0.765200 val_top5=0.957000 loss=11159.720325 time: 31.293406
[Epoch 65] train=0.930148 val_top1=0.763000 val_top5=0.956600 loss=10696.837650 time: 30.882725
[Epoch 66] train=0.929848 val_top1=0.760600 val_top5=0.955700 loss=10580.562824 time: 30.970017
[Epoch 67] train=0.930789 val_top1=0.762700 val_top5=0.955700 loss=10472.272598 time: 31.042306
[Epoch 68] train=0.931651 val_top1=0.755500 val_top5=0.956500 loss=10241.970451 time: 31.046671
[Epoch 69] train=0.933974 val_top1=0.759400 val_top5=0.955600 loss=9883.870485 time: 31.162095
[Epoch 70] train=0.935036 val_top1=0.756000 val_top5=0.955200 loss=9754.022066 time: 31.041133
[Epoch 71] train=0.936759 val_top1=0.760400 val_top5=0.955400 loss=9541.015074 time: 31.108849
[Epoch 72] train=0.939143 val_top1=0.761700 val_top5=0.955700 loss=9272.986700 time: 30.908366
[Epoch 73] train=0.939864 val_top1=0.759700 val_top5=0.956400 loss=9182.930813 time: 32.733544
[Epoch 74] train=0.940545 val_top1=0.759000 val_top5=0.957100 loss=9141.391491 time: 31.199966
[Epoch 75] train=0.942007 val_top1=0.758200 val_top5=0.955200 loss=8857.631531 time: 31.350404
[Epoch 76] train=0.942187 val_top1=0.752900 val_top5=0.955500 loss=8768.860443 time: 30.993405
[Epoch 77] train=0.942348 val_top1=0.755900 val_top5=0.953900 loss=8797.209816 time: 30.931170
[Epoch 78] train=0.943850 val_top1=0.758200 val_top5=0.954700 loss=8374.478731 time: 30.862971
[Epoch 79] train=0.943990 val_top1=0.756300 val_top5=0.953000 loss=8377.802681 time: 30.903828
[Epoch 80] train=0.944251 val_top1=0.759200 val_top5=0.955600 loss=8391.014097 time: 30.914194
[Epoch 81] train=0.946034 val_top1=0.756900 val_top5=0.955400 loss=8070.427042 time: 30.924990
[Epoch 82] train=0.948878 val_top1=0.755700 val_top5=0.954500 loss=7785.322618 time: 30.960860
[Epoch 83] train=0.947316 val_top1=0.756000 val_top5=0.956400 loss=7934.442528 time: 31.059016
[Epoch 84] train=0.946735 val_top1=0.758300 val_top5=0.954400 loss=7920.084103 time: 31.051182
[Epoch 85] train=0.949319 val_top1=0.755000 val_top5=0.954400 loss=7607.097343 time: 31.135574
[Epoch 86] train=0.948798 val_top1=0.756600 val_top5=0.955200 loss=7568.585300 time: 31.127694
[Epoch 87] train=0.948618 val_top1=0.757000 val_top5=0.954500 loss=7635.440264 time: 31.121517
[Epoch 88] train=0.950180 val_top1=0.762000 val_top5=0.954600 loss=7472.861063 time: 31.002581
[Epoch 89] train=0.952404 val_top1=0.754300 val_top5=0.955100 loss=7223.008036 time: 31.077626
[Epoch 90] train=0.956931 val_top1=0.762600 val_top5=0.957000 loss=6675.656366 time: 31.160826
[Epoch 91] train=0.960557 val_top1=0.762600 val_top5=0.956700 loss=6118.428095 time: 32.645898
[Epoch 92] train=0.962340 val_top1=0.763000 val_top5=0.956300 loss=5960.644087 time: 31.056726
[Epoch 93] train=0.963241 val_top1=0.764800 val_top5=0.956900 loss=5767.579064 time: 30.991763
[Epoch 94] train=0.965224 val_top1=0.764600 val_top5=0.956300 loss=5452.073889 time: 31.055984
[Epoch 95] train=0.963201 val_top1=0.763900 val_top5=0.956400 loss=5758.998512 time: 30.881844
[Epoch 96] train=0.964583 val_top1=0.764600 val_top5=0.956500 loss=5443.178815 time: 32.211630
[Epoch 97] train=0.965765 val_top1=0.762300 val_top5=0.955200 loss=5471.187088 time: 30.997506
[Epoch 98] train=0.965124 val_top1=0.764100 val_top5=0.955700 loss=5458.631891 time: 30.941041
[Epoch 99] train=0.966166 val_top1=0.764200 val_top5=0.954900 loss=5376.839562 time: 30.988578
[Epoch 100] train=0.966747 val_top1=0.764900 val_top5=0.956000 loss=5210.460317 time: 31.217347
[Epoch 101] train=0.965084 val_top1=0.763800 val_top5=0.955600 loss=5487.375000 time: 31.236848
[Epoch 102] train=0.965425 val_top1=0.764900 val_top5=0.955900 loss=5400.104175 time: 31.145804
[Epoch 103] train=0.965905 val_top1=0.765200 val_top5=0.956400 loss=5389.844082 time: 31.073085
[Epoch 104] train=0.967248 val_top1=0.764600 val_top5=0.956200 loss=5230.886714 time: 31.222204
[Epoch 105] train=0.967268 val_top1=0.765100 val_top5=0.955500 loss=5288.421810 time: 32.501432
[Epoch 106] train=0.966927 val_top1=0.762600 val_top5=0.955600 loss=5175.133648 time: 31.090501
[Epoch 107] train=0.967268 val_top1=0.762900 val_top5=0.955900 loss=5116.077814 time: 31.035927
[Epoch 108] train=0.967889 val_top1=0.764300 val_top5=0.955800 loss=5094.853031 time: 31.127920
[Epoch 109] train=0.968269 val_top1=0.762900 val_top5=0.954800 loss=5066.790400 time: 31.181741
[Epoch 110] train=0.967428 val_top1=0.763600 val_top5=0.955300 loss=5191.907635 time: 31.144798
[Epoch 111] train=0.967308 val_top1=0.765200 val_top5=0.955700 loss=5122.842383 time: 30.893797
[Epoch 112] train=0.969271 val_top1=0.764800 val_top5=0.954800 loss=5014.293629 time: 30.996706
[Epoch 113] train=0.968850 val_top1=0.764800 val_top5=0.954500 loss=4962.408850 time: 30.936202
[Epoch 114] train=0.967808 val_top1=0.764100 val_top5=0.955300 loss=5165.265426 time: 31.225115
[Epoch 115] train=0.969351 val_top1=0.763500 val_top5=0.955300 loss=4945.139531 time: 31.202420
[Epoch 116] train=0.968770 val_top1=0.764100 val_top5=0.955600 loss=4995.252597 time: 31.185351
[Epoch 117] train=0.969411 val_top1=0.764800 val_top5=0.953800 loss=4876.070538 time: 31.176903
[Epoch 118] train=0.969291 val_top1=0.764300 val_top5=0.954400 loss=4810.268602 time: 31.180138
[Epoch 119] train=0.969030 val_top1=0.763800 val_top5=0.954300 loss=4928.374675 time: 31.221322
Done.
