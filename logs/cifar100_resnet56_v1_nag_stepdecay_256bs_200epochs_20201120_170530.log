Imports successful

Using label smoothing: False
Using mixup: False

Model Init Done.

Not using distillation
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
sparse label loss: True

Using label smoothing: False
Using mixup: False

Using nag Optimizer
{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7f14d8a7ec90>, 'wd': 0.0001, 'momentum': 0.9}

Number of warmup epochs: 20
Warmup Learning Rate Mode: linear
Learing Rate Mode: step
Learing Rate Decay: 0.1
Learning Rate Decay Epochs: [40, 80]

Training Settings Set Successfully.
Training loop started for 120 epochs:
[Epoch 0] train=0.073838 val_top1=0.117600 val_top5=0.433000 loss=170219.195801 time: 30.777406
[Epoch 1] train=0.151042 val_top1=0.189600 val_top5=0.554800 loss=137870.294006 time: 30.721531
[Epoch 2] train=0.219251 val_top1=0.259700 val_top5=0.657000 loss=126717.948425 time: 30.498878
[Epoch 3] train=0.285817 val_top1=0.291600 val_top5=0.678600 loss=114741.545532 time: 30.608626
[Epoch 4] train=0.343389 val_top1=0.376500 val_top5=0.763200 loss=104672.535980 time: 30.531427
[Epoch 5] train=0.395152 val_top1=0.367300 val_top5=0.763400 loss=96091.971863 time: 30.645341
[Epoch 6] train=0.434235 val_top1=0.350500 val_top5=0.757000 loss=89815.805969 time: 30.655436
[Epoch 7] train=0.469071 val_top1=0.423300 val_top5=0.801100 loss=83619.173889 time: 30.487295
[Epoch 8] train=0.502063 val_top1=0.489400 val_top5=0.847100 loss=78415.407623 time: 30.575020
[Epoch 9] train=0.528786 val_top1=0.483300 val_top5=0.846300 loss=73767.242157 time: 30.510279
[Epoch 10] train=0.553626 val_top1=0.519100 val_top5=0.881700 loss=70109.783844 time: 30.583795
[Epoch 11] train=0.572857 val_top1=0.524300 val_top5=0.867800 loss=67017.592651 time: 30.761647
[Epoch 12] train=0.589724 val_top1=0.558500 val_top5=0.891100 loss=63875.044891 time: 30.605764
[Epoch 13] train=0.603546 val_top1=0.562800 val_top5=0.888200 loss=61789.264526 time: 30.511347
[Epoch 14] train=0.614363 val_top1=0.586300 val_top5=0.904200 loss=59601.992325 time: 30.688589
[Epoch 15] train=0.632312 val_top1=0.596900 val_top5=0.901000 loss=57276.705780 time: 30.682486
[Epoch 16] train=0.641206 val_top1=0.564800 val_top5=0.895000 loss=55601.670639 time: 30.820744
[Epoch 17] train=0.650361 val_top1=0.635300 val_top5=0.917200 loss=54257.971817 time: 30.588196
[Epoch 18] train=0.663021 val_top1=0.618500 val_top5=0.914500 loss=52258.734375 time: 30.720860
[Epoch 19] train=0.666987 val_top1=0.619100 val_top5=0.915100 loss=51282.868073 time: 30.680502
[Epoch 20] train=0.677724 val_top1=0.621500 val_top5=0.915100 loss=49792.560913 time: 30.662913
[Epoch 21] train=0.688702 val_top1=0.623500 val_top5=0.921700 loss=48056.158127 time: 30.532403
[Epoch 22] train=0.699339 val_top1=0.655700 val_top5=0.932300 loss=46688.209213 time: 30.525465
[Epoch 23] train=0.706090 val_top1=0.632300 val_top5=0.925200 loss=45278.375244 time: 30.689587
[Epoch 24] train=0.713862 val_top1=0.661300 val_top5=0.929600 loss=44048.368713 time: 30.552673
[Epoch 25] train=0.720313 val_top1=0.650800 val_top5=0.923600 loss=43017.356339 time: 30.635339
[Epoch 26] train=0.727284 val_top1=0.673900 val_top5=0.935500 loss=42018.904587 time: 30.516507
[Epoch 27] train=0.733614 val_top1=0.646400 val_top5=0.927800 loss=40885.238571 time: 30.596808
[Epoch 28] train=0.738602 val_top1=0.684900 val_top5=0.939000 loss=39998.704208 time: 30.719918
[Epoch 29] train=0.744211 val_top1=0.665600 val_top5=0.932800 loss=39224.923813 time: 30.808589
[Epoch 30] train=0.750741 val_top1=0.672000 val_top5=0.928700 loss=38203.301056 time: 30.645839
[Epoch 31] train=0.751122 val_top1=0.650000 val_top5=0.924900 loss=38049.047058 time: 30.553461
[Epoch 32] train=0.760657 val_top1=0.621900 val_top5=0.905700 loss=36985.889984 time: 30.609477
[Epoch 33] train=0.760417 val_top1=0.667100 val_top5=0.928800 loss=36444.987465 time: 30.691524
[Epoch 34] train=0.764403 val_top1=0.678500 val_top5=0.933400 loss=35901.079544 time: 30.743818
[Epoch 35] train=0.766226 val_top1=0.674000 val_top5=0.937000 loss=35554.065948 time: 30.817392
[Epoch 36] train=0.773478 val_top1=0.649600 val_top5=0.917300 loss=34610.300156 time: 30.632056
[Epoch 37] train=0.777484 val_top1=0.685300 val_top5=0.940900 loss=34327.101074 time: 30.618960
[Epoch 38] train=0.781330 val_top1=0.692000 val_top5=0.939700 loss=33587.461113 time: 30.650113
[Epoch 39] train=0.784175 val_top1=0.705600 val_top5=0.943600 loss=32960.480423 time: 30.522659
[Epoch 40] train=0.785357 val_top1=0.667500 val_top5=0.933400 loss=32571.041100 time: 30.559926
[Epoch 41] train=0.786979 val_top1=0.698400 val_top5=0.943400 loss=32392.377632 time: 30.641356
[Epoch 42] train=0.791286 val_top1=0.700100 val_top5=0.941900 loss=31870.789139 time: 30.659948
[Epoch 43] train=0.794431 val_top1=0.695700 val_top5=0.943700 loss=31431.664368 time: 30.569710
[Epoch 44] train=0.800461 val_top1=0.704400 val_top5=0.945200 loss=30570.846710 time: 30.775500
[Epoch 45] train=0.799519 val_top1=0.682200 val_top5=0.936000 loss=30768.274864 time: 30.675146
[Epoch 46] train=0.801883 val_top1=0.684800 val_top5=0.936700 loss=30042.165176 time: 30.672672
[Epoch 47] train=0.802905 val_top1=0.707300 val_top5=0.940500 loss=29826.738831 time: 30.674349
[Epoch 48] train=0.806711 val_top1=0.695700 val_top5=0.941200 loss=29346.741402 time: 30.564410
[Epoch 49] train=0.807893 val_top1=0.692400 val_top5=0.939600 loss=29152.890511 time: 30.623904
[Epoch 50] train=0.812360 val_top1=0.683000 val_top5=0.936100 loss=28654.251045 time: 30.536193
[Epoch 51] train=0.811498 val_top1=0.676100 val_top5=0.924400 loss=28687.806068 time: 30.622863
[Epoch 52] train=0.813602 val_top1=0.700000 val_top5=0.941800 loss=28188.352318 time: 30.583712
[Epoch 53] train=0.814083 val_top1=0.705300 val_top5=0.941000 loss=28079.032982 time: 30.652351
[Epoch 54] train=0.816366 val_top1=0.669300 val_top5=0.919700 loss=27653.211540 time: 30.588818
[Epoch 55] train=0.818930 val_top1=0.683500 val_top5=0.936400 loss=27314.061653 time: 30.594419
[Epoch 56] train=0.821254 val_top1=0.629900 val_top5=0.907200 loss=26976.262962 time: 30.780793
[Epoch 57] train=0.820373 val_top1=0.684500 val_top5=0.934000 loss=27133.293152 time: 30.789884
[Epoch 58] train=0.823177 val_top1=0.706600 val_top5=0.942900 loss=26723.351723 time: 30.577254
[Epoch 59] train=0.825521 val_top1=0.659200 val_top5=0.927700 loss=26240.676155 time: 30.526356
[Epoch 60] train=0.864704 val_top1=0.763800 val_top5=0.959400 loss=20608.908031 time: 30.607894
[Epoch 61] train=0.898217 val_top1=0.768400 val_top5=0.961600 loss=15692.953186 time: 30.827377
[Epoch 62] train=0.907893 val_top1=0.769200 val_top5=0.962400 loss=14328.524040 time: 30.788594
[Epoch 63] train=0.911959 val_top1=0.766700 val_top5=0.962000 loss=13530.408306 time: 30.664313
[Epoch 64] train=0.917829 val_top1=0.765900 val_top5=0.962900 loss=12837.824142 time: 30.882099
[Epoch 65] train=0.919932 val_top1=0.767300 val_top5=0.960800 loss=12192.841007 time: 30.683046
[Epoch 66] train=0.923758 val_top1=0.770600 val_top5=0.962200 loss=11843.190125 time: 30.571341
[Epoch 67] train=0.924519 val_top1=0.770400 val_top5=0.961200 loss=11639.938179 time: 30.793870
[Epoch 68] train=0.926222 val_top1=0.771600 val_top5=0.961500 loss=11160.216358 time: 30.567107
[Epoch 69] train=0.929968 val_top1=0.770300 val_top5=0.961500 loss=10738.157204 time: 30.594566
[Epoch 70] train=0.931991 val_top1=0.767800 val_top5=0.960300 loss=10495.827641 time: 30.636795
[Epoch 71] train=0.933133 val_top1=0.767400 val_top5=0.960700 loss=10163.949783 time: 30.633145
[Epoch 72] train=0.935537 val_top1=0.765900 val_top5=0.960900 loss=9944.574343 time: 30.614695
[Epoch 73] train=0.936879 val_top1=0.767900 val_top5=0.960200 loss=9766.096498 time: 30.419974
[Epoch 74] train=0.936078 val_top1=0.766600 val_top5=0.960500 loss=9624.551838 time: 30.775155
[Epoch 75] train=0.937300 val_top1=0.764100 val_top5=0.959800 loss=9425.394293 time: 30.530743
[Epoch 76] train=0.940004 val_top1=0.766400 val_top5=0.959900 loss=9186.644924 time: 30.649579
[Epoch 77] train=0.941627 val_top1=0.765200 val_top5=0.960100 loss=8914.132090 time: 30.444195
[Epoch 78] train=0.943830 val_top1=0.768300 val_top5=0.960500 loss=8719.256737 time: 30.663571
[Epoch 79] train=0.943790 val_top1=0.763100 val_top5=0.962500 loss=8630.893520 time: 30.629347
[Epoch 80] train=0.944451 val_top1=0.769200 val_top5=0.962600 loss=8394.220356 time: 30.733977
[Epoch 81] train=0.946234 val_top1=0.766600 val_top5=0.960900 loss=8246.052155 time: 30.464532
[Epoch 82] train=0.947035 val_top1=0.761500 val_top5=0.961800 loss=8134.292860 time: 30.746774
[Epoch 83] train=0.947496 val_top1=0.766000 val_top5=0.960100 loss=7900.807659 time: 30.674216
[Epoch 84] train=0.947075 val_top1=0.762300 val_top5=0.958600 loss=7891.706842 time: 30.604070
[Epoch 85] train=0.948898 val_top1=0.761000 val_top5=0.958800 loss=7754.170654 time: 30.629217
[Epoch 86] train=0.950841 val_top1=0.763600 val_top5=0.960500 loss=7632.799200 time: 30.605271
[Epoch 87] train=0.951082 val_top1=0.763400 val_top5=0.960900 loss=7276.971458 time: 30.653827
[Epoch 88] train=0.950781 val_top1=0.761900 val_top5=0.959000 loss=7369.479469 time: 30.636395
[Epoch 89] train=0.953646 val_top1=0.759700 val_top5=0.959700 loss=7079.133020 time: 30.604314
[Epoch 90] train=0.953446 val_top1=0.760700 val_top5=0.961000 loss=7050.933884 time: 30.599644
[Epoch 91] train=0.953085 val_top1=0.763200 val_top5=0.957600 loss=7015.492290 time: 30.665338
[Epoch 92] train=0.954407 val_top1=0.760100 val_top5=0.957700 loss=6837.363115 time: 30.659916
[Epoch 93] train=0.955609 val_top1=0.761000 val_top5=0.958300 loss=6673.694925 time: 30.545253
[Epoch 94] train=0.954667 val_top1=0.761800 val_top5=0.957100 loss=6699.380167 time: 30.627865
[Epoch 95] train=0.956350 val_top1=0.757700 val_top5=0.958500 loss=6617.170325 time: 30.678844
[Epoch 96] train=0.956210 val_top1=0.760700 val_top5=0.958200 loss=6526.730221 time: 30.511995
[Epoch 97] train=0.958153 val_top1=0.755600 val_top5=0.957700 loss=6308.815384 time: 30.732118
[Epoch 98] train=0.957752 val_top1=0.763600 val_top5=0.958800 loss=6329.551961 time: 30.658361
[Epoch 99] train=0.957712 val_top1=0.761000 val_top5=0.957800 loss=6310.430183 time: 30.680228
[Epoch 100] train=0.962780 val_top1=0.765200 val_top5=0.958800 loss=5707.607165 time: 30.718187
[Epoch 101] train=0.965946 val_top1=0.765000 val_top5=0.959500 loss=5328.081968 time: 30.714432
[Epoch 102] train=0.965525 val_top1=0.768000 val_top5=0.959800 loss=5286.289430 time: 30.621457
[Epoch 103] train=0.969111 val_top1=0.764600 val_top5=0.959800 loss=4978.258146 time: 30.604918
[Epoch 104] train=0.967288 val_top1=0.767000 val_top5=0.960500 loss=5054.876484 time: 30.748742
[Epoch 105] train=0.968910 val_top1=0.768600 val_top5=0.960500 loss=4880.577991 time: 30.559642
[Epoch 106] train=0.969651 val_top1=0.767500 val_top5=0.960800 loss=4729.247316 time: 30.618330
[Epoch 107] train=0.969471 val_top1=0.767600 val_top5=0.960400 loss=4757.515203 time: 30.787544
[Epoch 108] train=0.970933 val_top1=0.766500 val_top5=0.960600 loss=4702.759468 time: 30.631889
[Epoch 109] train=0.970473 val_top1=0.766700 val_top5=0.960200 loss=4782.046858 time: 30.712088
[Epoch 110] train=0.969671 val_top1=0.767900 val_top5=0.960200 loss=4784.720952 time: 30.814476
[Epoch 111] train=0.970954 val_top1=0.766400 val_top5=0.960200 loss=4608.993109 time: 30.837733
[Epoch 112] train=0.970873 val_top1=0.768100 val_top5=0.961300 loss=4593.714238 time: 30.845399
[Epoch 113] train=0.971474 val_top1=0.766700 val_top5=0.960200 loss=4505.305284 time: 30.747357
[Epoch 114] train=0.970353 val_top1=0.765200 val_top5=0.959400 loss=4697.969681 time: 30.549415
[Epoch 115] train=0.971214 val_top1=0.766900 val_top5=0.960600 loss=4576.131779 time: 30.529142
[Epoch 116] train=0.972115 val_top1=0.766200 val_top5=0.961000 loss=4520.183249 time: 30.606798
[Epoch 117] train=0.971454 val_top1=0.766300 val_top5=0.960100 loss=4577.096866 time: 30.541538
[Epoch 118] train=0.971434 val_top1=0.767700 val_top5=0.960300 loss=4490.037040 time: 30.594299
[Epoch 119] train=0.971855 val_top1=0.767700 val_top5=0.959900 loss=4443.167817 time: 30.604915
Done.
