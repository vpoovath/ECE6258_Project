Imports successful
Model Init Done.
Preprocessing Step Successful.
Initialization of train_data and val_data successful.
Per Device Batch Size: 256
Using rmsprop Optimizer
{'learning_rate': 0.001, 'gamma1': 0.9, 'gamma2': 0.9, 'epsilon': 1e-08}
Training loop started for 120 epochs:
[Epoch 0] train=0.142508 val_top1=0.189000 val_top5=0.576400 loss=141505.368835 time: 31.214816
[Epoch 1] train=0.268389 val_top1=0.331600 val_top5=0.726600 loss=117457.138855 time: 30.974991
[Epoch 2] train=0.348097 val_top1=0.389300 val_top5=0.787800 loss=103796.058777 time: 31.036045
[Epoch 3] train=0.404327 val_top1=0.374700 val_top5=0.750900 loss=94527.743988 time: 31.020794
[Epoch 4] train=0.446595 val_top1=0.444300 val_top5=0.806400 loss=87498.514740 time: 31.048399
[Epoch 5] train=0.481470 val_top1=0.457400 val_top5=0.837000 loss=81598.214203 time: 31.272044
[Epoch 6] train=0.509315 val_top1=0.501900 val_top5=0.849600 loss=77235.158173 time: 31.297251
[Epoch 7] train=0.530970 val_top1=0.427900 val_top5=0.813400 loss=73514.378845 time: 31.239605
[Epoch 8] train=0.553446 val_top1=0.460700 val_top5=0.840000 loss=69976.584686 time: 31.091743
[Epoch 9] train=0.566567 val_top1=0.458000 val_top5=0.839900 loss=67444.952057 time: 32.685214
[Epoch 10] train=0.582512 val_top1=0.549400 val_top5=0.886600 loss=64951.537659 time: 31.115143
[Epoch 11] train=0.598518 val_top1=0.495100 val_top5=0.856900 loss=62533.472092 time: 31.215717
[Epoch 12] train=0.609776 val_top1=0.570200 val_top5=0.895300 loss=60609.852173 time: 32.658192
[Epoch 13] train=0.621715 val_top1=0.513700 val_top5=0.858100 loss=58456.965195 time: 30.972749
[Epoch 14] train=0.634175 val_top1=0.577000 val_top5=0.896000 loss=56610.489090 time: 31.031673
[Epoch 15] train=0.644511 val_top1=0.592200 val_top5=0.903500 loss=55003.670319 time: 31.260502
[Epoch 16] train=0.652985 val_top1=0.596200 val_top5=0.917100 loss=53310.474487 time: 31.128752
[Epoch 17] train=0.664443 val_top1=0.619500 val_top5=0.915200 loss=51946.212036 time: 31.105630
[Epoch 18] train=0.670533 val_top1=0.573800 val_top5=0.895200 loss=50827.693756 time: 31.198894
[Epoch 19] train=0.678285 val_top1=0.641400 val_top5=0.924400 loss=49546.404800 time: 31.181296
[Epoch 20] train=0.687941 val_top1=0.631300 val_top5=0.927500 loss=48150.880096 time: 31.416986
[Epoch 21] train=0.695773 val_top1=0.606600 val_top5=0.911400 loss=47026.536667 time: 31.222007
[Epoch 22] train=0.701863 val_top1=0.630500 val_top5=0.918500 loss=45782.298309 time: 31.266992
[Epoch 23] train=0.710597 val_top1=0.655600 val_top5=0.930200 loss=44560.596222 time: 31.016474
[Epoch 24] train=0.713101 val_top1=0.656800 val_top5=0.930500 loss=43969.646484 time: 31.030168
[Epoch 25] train=0.722055 val_top1=0.643500 val_top5=0.926300 loss=42673.543045 time: 31.113288
[Epoch 26] train=0.726102 val_top1=0.638100 val_top5=0.920600 loss=41861.648834 time: 31.103266
[Epoch 27] train=0.728506 val_top1=0.640500 val_top5=0.919200 loss=41341.887253 time: 31.408165
[Epoch 28] train=0.736999 val_top1=0.645900 val_top5=0.928700 loss=40291.885193 time: 31.090979
[Epoch 29] train=0.740144 val_top1=0.654900 val_top5=0.929600 loss=39505.368027 time: 31.117989
[Epoch 30] train=0.745813 val_top1=0.669400 val_top5=0.936100 loss=38693.412476 time: 31.168884
[Epoch 31] train=0.750721 val_top1=0.652700 val_top5=0.925200 loss=37918.799408 time: 31.109346
[Epoch 32] train=0.755308 val_top1=0.662000 val_top5=0.929800 loss=37522.075378 time: 31.169707
[Epoch 33] train=0.760497 val_top1=0.678000 val_top5=0.935600 loss=36194.623215 time: 31.339154
[Epoch 34] train=0.765124 val_top1=0.668900 val_top5=0.934700 loss=35819.690491 time: 31.175102
[Epoch 35] train=0.767869 val_top1=0.661600 val_top5=0.926400 loss=35392.243332 time: 31.407027
[Epoch 36] train=0.774940 val_top1=0.658200 val_top5=0.930200 loss=34470.508537 time: 31.185376
[Epoch 37] train=0.777584 val_top1=0.643700 val_top5=0.930700 loss=33724.154831 time: 31.156802
[Epoch 38] train=0.782091 val_top1=0.665400 val_top5=0.930900 loss=33207.914276 time: 31.040283
[Epoch 39] train=0.784495 val_top1=0.666300 val_top5=0.930300 loss=32663.013962 time: 31.176303
[Epoch 40] train=0.788582 val_top1=0.676600 val_top5=0.932100 loss=32099.846573 time: 31.115147
[Epoch 41] train=0.793570 val_top1=0.679000 val_top5=0.935400 loss=31097.740517 time: 32.313640
[Epoch 42] train=0.797155 val_top1=0.686300 val_top5=0.935400 loss=30701.873260 time: 31.197124
[Epoch 43] train=0.799419 val_top1=0.681200 val_top5=0.936900 loss=30100.292122 time: 31.315795
[Epoch 44] train=0.802224 val_top1=0.677000 val_top5=0.936400 loss=30035.440857 time: 31.226865
[Epoch 45] train=0.804788 val_top1=0.677900 val_top5=0.933100 loss=29433.322563 time: 30.986930
[Epoch 46] train=0.810817 val_top1=0.667500 val_top5=0.929600 loss=28525.138779 time: 31.101439
[Epoch 47] train=0.815144 val_top1=0.638500 val_top5=0.933200 loss=28016.401772 time: 30.897694
[Epoch 48] train=0.817568 val_top1=0.676200 val_top5=0.937600 loss=27624.069496 time: 31.171777
[Epoch 49] train=0.818690 val_top1=0.669300 val_top5=0.929400 loss=27075.277664 time: 31.340677
[Epoch 50] train=0.821875 val_top1=0.684200 val_top5=0.932300 loss=26744.795258 time: 31.160975
[Epoch 51] train=0.824279 val_top1=0.674100 val_top5=0.932200 loss=26390.392883 time: 31.262471
[Epoch 52] train=0.825160 val_top1=0.687500 val_top5=0.933200 loss=25973.635841 time: 31.197534
[Epoch 53] train=0.829828 val_top1=0.685500 val_top5=0.939500 loss=25468.473152 time: 31.039813
[Epoch 54] train=0.832392 val_top1=0.685000 val_top5=0.940000 loss=25124.482323 time: 31.273429
[Epoch 55] train=0.836538 val_top1=0.680200 val_top5=0.932600 loss=24373.374809 time: 32.788545
[Epoch 56] train=0.838381 val_top1=0.674100 val_top5=0.935400 loss=24172.430298 time: 30.976439
[Epoch 57] train=0.840044 val_top1=0.677500 val_top5=0.934300 loss=23817.123520 time: 30.952227
[Epoch 58] train=0.843830 val_top1=0.676100 val_top5=0.932100 loss=23513.189590 time: 31.327884
[Epoch 59] train=0.845272 val_top1=0.693200 val_top5=0.939200 loss=23189.307236 time: 31.208575
[Epoch 60] train=0.845893 val_top1=0.669000 val_top5=0.938400 loss=22728.099838 time: 32.171773
[Epoch 61] train=0.849319 val_top1=0.689900 val_top5=0.935700 loss=22344.834129 time: 31.106423
[Epoch 62] train=0.850661 val_top1=0.678900 val_top5=0.935200 loss=22214.833160 time: 31.289317
[Epoch 63] train=0.855569 val_top1=0.701000 val_top5=0.944400 loss=21600.084549 time: 31.144397
[Epoch 64] train=0.855669 val_top1=0.708500 val_top5=0.945800 loss=21577.558235 time: 31.165514
[Epoch 65] train=0.859415 val_top1=0.697900 val_top5=0.937300 loss=20905.596680 time: 31.239087
[Epoch 66] train=0.858754 val_top1=0.646500 val_top5=0.919500 loss=20993.105072 time: 31.404004
[Epoch 67] train=0.864483 val_top1=0.701300 val_top5=0.935400 loss=20340.673279 time: 31.127027
[Epoch 68] train=0.866166 val_top1=0.686200 val_top5=0.940700 loss=19937.812881 time: 31.112034
[Epoch 69] train=0.867107 val_top1=0.699900 val_top5=0.941000 loss=19724.186600 time: 31.143841
[Epoch 70] train=0.869331 val_top1=0.718400 val_top5=0.947700 loss=19463.977463 time: 31.335224
[Epoch 71] train=0.869571 val_top1=0.695100 val_top5=0.940000 loss=19164.527805 time: 31.133439
[Epoch 72] train=0.873878 val_top1=0.709500 val_top5=0.941400 loss=18840.910561 time: 31.212328
[Epoch 73] train=0.871474 val_top1=0.693100 val_top5=0.941500 loss=18895.148857 time: 32.546458
[Epoch 74] train=0.876122 val_top1=0.704500 val_top5=0.941700 loss=18099.790813 time: 31.172397
[Epoch 75] train=0.876142 val_top1=0.700700 val_top5=0.937600 loss=18227.056580 time: 31.210587
[Epoch 76] train=0.877784 val_top1=0.696800 val_top5=0.935500 loss=18029.991783 time: 31.187959
[Epoch 77] train=0.879828 val_top1=0.687800 val_top5=0.938500 loss=17592.592823 time: 31.230527
[Epoch 78] train=0.882532 val_top1=0.712900 val_top5=0.940900 loss=17272.282757 time: 31.241227
[Epoch 79] train=0.883914 val_top1=0.689800 val_top5=0.935500 loss=17121.485020 time: 31.163438
[Epoch 80] train=0.883974 val_top1=0.715500 val_top5=0.943700 loss=17040.590809 time: 31.065401
[Epoch 81] train=0.887640 val_top1=0.694900 val_top5=0.939300 loss=16593.560169 time: 31.110066
[Epoch 82] train=0.888281 val_top1=0.707200 val_top5=0.941800 loss=16515.234261 time: 33.023410
[Epoch 83] train=0.888622 val_top1=0.714000 val_top5=0.945000 loss=16303.853485 time: 31.277984
[Epoch 84] train=0.892969 val_top1=0.714300 val_top5=0.944900 loss=15872.345364 time: 31.421311
[Epoch 85] train=0.891406 val_top1=0.709600 val_top5=0.940200 loss=16031.690186 time: 31.258200
[Epoch 86] train=0.893490 val_top1=0.675800 val_top5=0.932100 loss=15618.657890 time: 31.149807
[Epoch 87] train=0.894511 val_top1=0.708900 val_top5=0.944300 loss=15473.116825 time: 31.428604
[Epoch 88] train=0.897897 val_top1=0.704900 val_top5=0.939400 loss=15030.396114 time: 31.244102
[Epoch 89] train=0.896554 val_top1=0.694400 val_top5=0.938500 loss=14977.527447 time: 31.209826
[Epoch 90] train=0.898798 val_top1=0.709100 val_top5=0.940700 loss=14886.927258 time: 31.312498
[Epoch 91] train=0.901482 val_top1=0.703000 val_top5=0.938800 loss=14387.960693 time: 31.175362
[Epoch 92] train=0.899099 val_top1=0.702200 val_top5=0.941100 loss=14742.320637 time: 31.063196
[Epoch 93] train=0.902965 val_top1=0.693300 val_top5=0.941500 loss=14314.035316 time: 31.498998
[Epoch 94] train=0.903265 val_top1=0.685800 val_top5=0.936100 loss=14344.342934 time: 31.412869
[Epoch 95] train=0.903586 val_top1=0.711200 val_top5=0.938300 loss=14084.385620 time: 31.415164
[Epoch 96] train=0.906611 val_top1=0.709500 val_top5=0.942500 loss=13922.428001 time: 31.220298
[Epoch 97] train=0.905629 val_top1=0.698200 val_top5=0.939300 loss=13773.441891 time: 31.249365
[Epoch 98] train=0.905729 val_top1=0.707000 val_top5=0.940900 loss=13701.406387 time: 31.024321
[Epoch 99] train=0.906270 val_top1=0.702100 val_top5=0.937900 loss=13676.584114 time: 31.265652
[Epoch 100] train=0.911278 val_top1=0.703100 val_top5=0.941100 loss=13260.912037 time: 30.894295
[Epoch 101] train=0.911398 val_top1=0.708600 val_top5=0.941200 loss=13168.363853 time: 31.512941
[Epoch 102] train=0.910096 val_top1=0.718800 val_top5=0.946500 loss=13213.491364 time: 31.541078
[Epoch 103] train=0.912740 val_top1=0.707100 val_top5=0.942300 loss=12944.525501 time: 31.339520
[Epoch 104] train=0.912941 val_top1=0.682900 val_top5=0.928400 loss=12722.805656 time: 32.634945
[Epoch 105] train=0.914643 val_top1=0.700900 val_top5=0.939500 loss=12629.055374 time: 31.242676
[Epoch 106] train=0.913862 val_top1=0.707400 val_top5=0.940600 loss=12316.738804 time: 30.961925
[Epoch 107] train=0.915605 val_top1=0.714300 val_top5=0.941300 loss=12405.692482 time: 31.264986
[Epoch 108] train=0.915485 val_top1=0.712100 val_top5=0.942400 loss=12311.251305 time: 31.510616
[Epoch 109] train=0.918149 val_top1=0.704600 val_top5=0.941100 loss=12143.044254 time: 31.328259
[Epoch 110] train=0.919832 val_top1=0.716000 val_top5=0.942100 loss=11815.060532 time: 31.542186
[Epoch 111] train=0.920052 val_top1=0.705200 val_top5=0.939200 loss=11693.000870 time: 31.074718
[Epoch 112] train=0.918530 val_top1=0.709200 val_top5=0.942900 loss=11761.196037 time: 31.265673
[Epoch 113] train=0.919311 val_top1=0.694400 val_top5=0.936700 loss=11801.394054 time: 32.645158
[Epoch 114] train=0.924199 val_top1=0.705100 val_top5=0.937300 loss=11103.509068 time: 31.049449
[Epoch 115] train=0.923718 val_top1=0.705500 val_top5=0.934700 loss=11268.526823 time: 31.440773
[Epoch 116] train=0.923618 val_top1=0.710400 val_top5=0.943800 loss=11331.480770 time: 31.468890
[Epoch 117] train=0.923417 val_top1=0.710600 val_top5=0.943300 loss=11131.088293 time: 31.388011
[Epoch 118] train=0.924079 val_top1=0.710100 val_top5=0.940600 loss=11149.130890 time: 31.358276
[Epoch 119] train=0.926442 val_top1=0.714900 val_top5=0.940700 loss=10788.354933 time: 30.989897
Done.
