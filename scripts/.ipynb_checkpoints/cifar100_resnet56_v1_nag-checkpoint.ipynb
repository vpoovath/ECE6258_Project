{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "net = get_model('cifar_resnet56_v1', classes=100)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "    \n",
    "    # Randomly flip the image horizontally\n",
    "#     transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "#     transforms.RandomBrightness(brightness=jitter_param),\n",
    "#     transforms.RandomSaturation(saturation=jitter_param),\n",
    "#     transforms.RandomHue(hue = jitter_param),\n",
    "    \n",
    "#     transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "\n",
    "# Number of data loader workers\n",
    "num_workers = 2\n",
    "\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started:\n",
      "[Epoch 0] train=0.190224 val=0.263200 loss=132160.956085 time: 28.515546\n",
      "[Epoch 1] train=0.301803 val=0.331400 loss=111663.769730 time: 28.616709\n",
      "[Epoch 2] train=0.373618 val=0.413800 loss=99323.927429 time: 28.314152\n",
      "[Epoch 3] train=0.438742 val=0.469700 loss=88842.103317 time: 28.243113\n",
      "[Epoch 4] train=0.492668 val=0.495100 loss=79889.370026 time: 28.007910\n",
      "[Epoch 5] train=0.535377 val=0.518800 loss=72944.637466 time: 28.172078\n",
      "[Epoch 6] train=0.571294 val=0.561800 loss=67125.756058 time: 28.202486\n",
      "[Epoch 7] train=0.601482 val=0.576000 loss=61970.586090 time: 28.194473\n",
      "[Epoch 8] train=0.626422 val=0.600800 loss=57774.010941 time: 28.383024\n",
      "[Epoch 9] train=0.654647 val=0.620100 loss=53937.551208 time: 28.440617\n",
      "[Epoch 10] train=0.672897 val=0.630300 loss=50672.878807 time: 28.308287\n",
      "[Epoch 11] train=0.690385 val=0.636400 loss=47555.741020 time: 28.196708\n",
      "[Epoch 12] train=0.706370 val=0.642900 loss=45128.902542 time: 28.121052\n",
      "[Epoch 13] train=0.720893 val=0.635500 loss=43057.121437 time: 28.474366\n",
      "[Epoch 14] train=0.733153 val=0.641600 loss=40942.653557 time: 28.522737\n",
      "[Epoch 15] train=0.745112 val=0.656100 loss=39171.853767 time: 28.341946\n",
      "[Epoch 16] train=0.750341 val=0.667200 loss=37851.582363 time: 28.711476\n",
      "[Epoch 17] train=0.764042 val=0.650600 loss=36097.531910 time: 28.509971\n",
      "[Epoch 18] train=0.772656 val=0.661500 loss=34809.364700 time: 28.524903\n",
      "[Epoch 19] train=0.778225 val=0.650000 loss=33374.385597 time: 28.268373\n",
      "[Epoch 20] train=0.787821 val=0.672600 loss=32205.280933 time: 28.078431\n",
      "[Epoch 21] train=0.794151 val=0.672200 loss=31238.100281 time: 28.342477\n",
      "[Epoch 22] train=0.798117 val=0.663400 loss=30513.877605 time: 28.399914\n",
      "[Epoch 23] train=0.803025 val=0.689300 loss=29550.586391 time: 28.176813\n",
      "[Epoch 24] train=0.808634 val=0.686700 loss=28752.961384 time: 28.370482\n",
      "[Epoch 25] train=0.815164 val=0.656700 loss=27687.941299 time: 28.509585\n",
      "[Epoch 26] train=0.820994 val=0.683700 loss=27039.614531 time: 27.981086\n",
      "[Epoch 27] train=0.825701 val=0.671900 loss=26129.508179 time: 28.329543\n",
      "[Epoch 28] train=0.829387 val=0.678400 loss=25374.590088 time: 29.097292\n",
      "[Epoch 29] train=0.834355 val=0.663600 loss=24864.334560 time: 28.717233\n",
      "[Epoch 30] train=0.902043 val=0.751500 loss=15111.596610 time: 28.792576\n",
      "[Epoch 31] train=0.936558 val=0.748800 loss=10108.417998 time: 28.728766\n",
      "[Epoch 32] train=0.946835 val=0.750800 loss=8740.334434 time: 28.621016\n",
      "[Epoch 33] train=0.953566 val=0.750100 loss=7519.325364 time: 28.783389\n",
      "[Epoch 34] train=0.957913 val=0.752600 loss=6740.833673 time: 28.641689\n",
      "[Epoch 35] train=0.964022 val=0.747900 loss=6068.903350 time: 28.361514\n",
      "[Epoch 36] train=0.966807 val=0.745300 loss=5459.895826 time: 28.698767\n",
      "[Epoch 37] train=0.969832 val=0.745800 loss=5051.827125 time: 28.471459\n",
      "[Epoch 38] train=0.972396 val=0.747500 loss=4636.397048 time: 28.337176\n",
      "[Epoch 39] train=0.975461 val=0.746100 loss=4244.291570 time: 28.114690\n",
      "[Epoch 40] train=0.976583 val=0.747000 loss=3983.359542 time: 28.502001\n",
      "[Epoch 41] train=0.978285 val=0.748800 loss=3750.745176 time: 28.532212\n",
      "[Epoch 42] train=0.980168 val=0.747700 loss=3478.568591 time: 28.618080\n",
      "[Epoch 43] train=0.981410 val=0.749900 loss=3221.608679 time: 28.522439\n",
      "[Epoch 44] train=0.983393 val=0.746200 loss=2989.167839 time: 28.626659\n",
      "[Epoch 45] train=0.983994 val=0.748800 loss=2836.388774 time: 28.220038\n",
      "[Epoch 46] train=0.984776 val=0.747600 loss=2643.553664 time: 28.590219\n",
      "[Epoch 47] train=0.986098 val=0.747600 loss=2576.032326 time: 28.680043\n",
      "[Epoch 48] train=0.985857 val=0.744600 loss=2495.812177 time: 28.431305\n",
      "[Epoch 49] train=0.986999 val=0.744400 loss=2341.008319 time: 28.724456\n",
      "[Epoch 50] train=0.988241 val=0.745400 loss=2185.152652 time: 28.546681\n",
      "[Epoch 51] train=0.989183 val=0.747900 loss=2070.050100 time: 28.980222\n",
      "[Epoch 52] train=0.989083 val=0.746300 loss=2023.725934 time: 28.608915\n",
      "[Epoch 53] train=0.989964 val=0.747100 loss=1933.547776 time: 28.298010\n",
      "[Epoch 54] train=0.989884 val=0.745000 loss=1930.036310 time: 28.505244\n",
      "[Epoch 55] train=0.990184 val=0.742800 loss=1811.426878 time: 28.166236\n",
      "[Epoch 56] train=0.990505 val=0.747600 loss=1786.886205 time: 28.995403\n",
      "[Epoch 57] train=0.991787 val=0.746500 loss=1588.887594 time: 28.649674\n",
      "[Epoch 58] train=0.991987 val=0.749600 loss=1571.753896 time: 28.208398\n",
      "[Epoch 59] train=0.990345 val=0.746600 loss=1684.303622 time: 28.620866\n",
      "[Epoch 60] train=0.992768 val=0.748400 loss=1403.543571 time: 29.138328\n",
      "[Epoch 61] train=0.993730 val=0.748100 loss=1312.627830 time: 29.175515\n",
      "[Epoch 62] train=0.994611 val=0.748200 loss=1131.378423 time: 28.963256\n",
      "[Epoch 63] train=0.994551 val=0.749800 loss=1160.547162 time: 28.949787\n",
      "[Epoch 64] train=0.995292 val=0.749600 loss=1047.238490 time: 29.039320\n",
      "[Epoch 65] train=0.995353 val=0.749500 loss=1081.048746 time: 28.855516\n",
      "[Epoch 66] train=0.995853 val=0.750100 loss=1006.557827 time: 28.789578\n",
      "[Epoch 67] train=0.995333 val=0.750600 loss=1038.703036 time: 28.649616\n",
      "[Epoch 68] train=0.995713 val=0.750000 loss=1011.994150 time: 28.575987\n",
      "[Epoch 69] train=0.996014 val=0.748800 loss=968.869666 time: 28.758739\n",
      "[Epoch 70] train=0.996114 val=0.748900 loss=958.329600 time: 28.721720\n",
      "[Epoch 71] train=0.995653 val=0.749100 loss=1012.023409 time: 28.660935\n",
      "[Epoch 72] train=0.995673 val=0.750100 loss=1021.064905 time: 28.232642\n",
      "[Epoch 73] train=0.995813 val=0.748600 loss=967.640845 time: 28.365407\n",
      "[Epoch 74] train=0.996254 val=0.749300 loss=945.513106 time: 28.885067\n",
      "[Epoch 75] train=0.996074 val=0.750500 loss=957.154763 time: 28.681853\n",
      "[Epoch 76] train=0.995833 val=0.748600 loss=950.155868 time: 28.415439\n",
      "[Epoch 77] train=0.996635 val=0.749100 loss=900.363670 time: 28.460226\n",
      "[Epoch 78] train=0.996575 val=0.748600 loss=905.224140 time: 28.437952\n",
      "[Epoch 79] train=0.996595 val=0.747800 loss=888.625577 time: 28.400002\n",
      "[Epoch 80] train=0.996374 val=0.749400 loss=907.014806 time: 28.694526\n",
      "[Epoch 81] train=0.996735 val=0.746700 loss=870.242866 time: 28.774626\n",
      "[Epoch 82] train=0.996635 val=0.747800 loss=886.101768 time: 28.715924\n",
      "[Epoch 83] train=0.996595 val=0.748200 loss=877.102490 time: 28.762066\n",
      "[Epoch 84] train=0.996134 val=0.748900 loss=935.390406 time: 28.360038\n",
      "[Epoch 85] train=0.996995 val=0.748000 loss=847.036241 time: 28.124903\n",
      "[Epoch 86] train=0.996815 val=0.749300 loss=844.922011 time: 28.474448\n",
      "[Epoch 87] train=0.996394 val=0.749700 loss=878.059362 time: 28.454605\n",
      "[Epoch 88] train=0.996514 val=0.749600 loss=861.971328 time: 29.183619\n",
      "[Epoch 89] train=0.996855 val=0.747800 loss=826.532363 time: 28.778028\n",
      "[Epoch 90] train=0.996474 val=0.749400 loss=867.698765 time: 28.666973\n",
      "[Epoch 91] train=0.996715 val=0.747600 loss=829.774477 time: 28.406634\n",
      "[Epoch 92] train=0.996735 val=0.748600 loss=841.923082 time: 28.680328\n",
      "[Epoch 93] train=0.996815 val=0.747100 loss=863.483328 time: 28.556508\n",
      "[Epoch 94] train=0.997155 val=0.748800 loss=795.951613 time: 29.069265\n",
      "[Epoch 95] train=0.996695 val=0.748500 loss=852.508657 time: 29.152325\n",
      "[Epoch 96] train=0.997236 val=0.747400 loss=787.031242 time: 28.645524\n",
      "[Epoch 97] train=0.996855 val=0.749000 loss=853.508045 time: 29.056610\n",
      "[Epoch 98] train=0.996835 val=0.748100 loss=848.561339 time: 28.771860\n",
      "[Epoch 99] train=0.996995 val=0.749000 loss=831.725654 time: 28.870165\n",
      "[Epoch 100] train=0.997276 val=0.748100 loss=801.086663 time: 28.753225\n",
      "[Epoch 101] train=0.997175 val=0.747300 loss=792.380718 time: 29.278370\n",
      "[Epoch 102] train=0.997196 val=0.747100 loss=792.594874 time: 28.790204\n",
      "[Epoch 103] train=0.997055 val=0.749100 loss=807.619870 time: 28.688311\n",
      "[Epoch 104] train=0.997316 val=0.748200 loss=777.896795 time: 28.698670\n",
      "[Epoch 105] train=0.997376 val=0.748900 loss=793.199874 time: 28.443805\n",
      "[Epoch 106] train=0.996835 val=0.748100 loss=814.081820 time: 28.455558\n",
      "[Epoch 107] train=0.997135 val=0.748000 loss=807.762856 time: 28.826420\n",
      "[Epoch 108] train=0.996554 val=0.748600 loss=838.506986 time: 29.059412\n",
      "[Epoch 109] train=0.997276 val=0.746800 loss=813.167666 time: 28.408996\n",
      "[Epoch 110] train=0.996695 val=0.748600 loss=827.996212 time: 28.373728\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "lr_decay_count = 0\n",
    "\n",
    "print(\"Training loop started:\")\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot(['training_loss','validation_loss'], save_path=\"./cifar100_resnet56_v1_nag.png\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
