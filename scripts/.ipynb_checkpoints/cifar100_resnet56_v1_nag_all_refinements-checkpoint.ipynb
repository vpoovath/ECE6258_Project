{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = True\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "dtype = 'float32' # Default training data type if float32\n",
    "num_gpus = 1      # number of GPUs to use\n",
    "\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "net = get_model('cifar_resnet56_v1', classes=100)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "net.cast(dtype)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "hard_weight = 0.5\n",
    "# Teacher model for distillation training\n",
    "teacher_name = 'cifar_wideresnet28_10'\n",
    "# teacher_name = 'cifar_resnet110_v2_mixup'\n",
    "# teacher_name = 'cifar_wideresnet16_10_mixup'\n",
    "# teacher_name = 'cifar_wideresnet28_10'\n",
    "# teacher_name = 'cifar_wideresnet28_10_mixup'\n",
    "# teacher_name = 'cifar_wideresnet40_8'\n",
    "# teacher_name = 'cifar_wideresnet40_8_mixup'\n",
    "teacher = get_model(teacher_name, pretrained=True, classes=num_classes, ctx=ctx)\n",
    "teacher.cast(dtype)\n",
    "distillation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "\n",
    "# Number of data loader workers\n",
    "num_workers = 2\n",
    "\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n"
     ]
    }
   ],
   "source": [
    "# epochs = 120\n",
    "epochs = 200 # Mixup asks for longer training to converge better\n",
    "warmup_epochs = 10\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler('linear',\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler('cosine',\n",
    "                base_lr = 0.1,\n",
    "                target_lr = 0,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epoch,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistilllationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                            hard_weight=hard_weight,\n",
    "                                                            sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started:\n",
      "[Epoch 0] train=1.000000 val_top1=0.148300 val_top5=0.490500 loss=176216.252258 time: 35.611091\n",
      "[Epoch 1] train=1.000000 val_top1=0.197500 val_top5=0.573900 loss=157846.148499 time: 35.357724\n",
      "[Epoch 2] train=1.000000 val_top1=0.202100 val_top5=0.575300 loss=148869.603973 time: 35.380495\n",
      "[Epoch 3] train=1.000000 val_top1=0.333600 val_top5=0.710100 loss=139952.015930 time: 35.350016\n",
      "[Epoch 4] train=1.000000 val_top1=0.408900 val_top5=0.797700 loss=133324.805145 time: 35.213512\n",
      "[Epoch 5] train=1.000000 val_top1=0.432000 val_top5=0.801400 loss=128484.065735 time: 35.445710\n",
      "[Epoch 6] train=1.000000 val_top1=0.404700 val_top5=0.755100 loss=125510.620346 time: 35.556604\n",
      "[Epoch 7] train=1.000000 val_top1=0.494700 val_top5=0.850300 loss=122256.419617 time: 35.364410\n",
      "[Epoch 8] train=1.000000 val_top1=0.480600 val_top5=0.825600 loss=119315.876022 time: 35.408705\n",
      "[Epoch 9] train=1.000000 val_top1=0.558200 val_top5=0.885800 loss=117617.863281 time: 35.461397\n",
      "[Epoch 10] train=1.000000 val_top1=0.552500 val_top5=0.885700 loss=112663.588013 time: 35.625643\n",
      "[Epoch 11] train=1.000000 val_top1=0.557900 val_top5=0.887400 loss=112143.293335 time: 35.423729\n",
      "[Epoch 12] train=1.000000 val_top1=0.594800 val_top5=0.900800 loss=110323.314789 time: 35.616390\n",
      "[Epoch 13] train=1.000000 val_top1=0.605400 val_top5=0.907300 loss=106965.244583 time: 35.623169\n",
      "[Epoch 14] train=1.000000 val_top1=0.619300 val_top5=0.906000 loss=107717.104919 time: 35.672776\n",
      "[Epoch 15] train=1.000000 val_top1=0.619800 val_top5=0.907800 loss=105895.028748 time: 35.677939\n",
      "[Epoch 16] train=1.000000 val_top1=0.624900 val_top5=0.914600 loss=103986.025253 time: 35.509032\n",
      "[Epoch 17] train=1.000000 val_top1=0.603700 val_top5=0.916800 loss=104245.419647 time: 35.298378\n",
      "[Epoch 18] train=1.000000 val_top1=0.633100 val_top5=0.915500 loss=104583.856735 time: 35.561327\n",
      "[Epoch 19] train=1.000000 val_top1=0.627000 val_top5=0.905000 loss=103253.206512 time: 35.583506\n",
      "[Epoch 20] train=1.000000 val_top1=0.630300 val_top5=0.923200 loss=101000.086014 time: 35.801020\n",
      "[Epoch 21] train=1.000000 val_top1=0.669600 val_top5=0.929400 loss=100290.659912 time: 35.560337\n",
      "[Epoch 22] train=1.000000 val_top1=0.646900 val_top5=0.925200 loss=100216.310989 time: 35.321985\n",
      "[Epoch 23] train=1.000000 val_top1=0.663800 val_top5=0.923000 loss=100610.343048 time: 35.448223\n",
      "[Epoch 24] train=1.000000 val_top1=0.665100 val_top5=0.932200 loss=100159.057846 time: 35.639276\n",
      "[Epoch 25] train=1.000000 val_top1=0.644800 val_top5=0.927500 loss=98070.271011 time: 35.470927\n",
      "[Epoch 26] train=1.000000 val_top1=0.648800 val_top5=0.917500 loss=97729.882751 time: 35.285224\n",
      "[Epoch 27] train=1.000000 val_top1=0.653300 val_top5=0.925100 loss=97266.347198 time: 35.526685\n",
      "[Epoch 28] train=1.000000 val_top1=0.673100 val_top5=0.934300 loss=97861.125092 time: 35.561535\n",
      "[Epoch 29] train=1.000000 val_top1=0.635500 val_top5=0.914000 loss=94495.569717 time: 35.552501\n",
      "[Epoch 30] train=1.000000 val_top1=0.668300 val_top5=0.929500 loss=95704.681595 time: 35.841872\n",
      "[Epoch 31] train=1.000000 val_top1=0.675500 val_top5=0.931300 loss=95831.809464 time: 35.593569\n",
      "[Epoch 32] train=1.000000 val_top1=0.670100 val_top5=0.926400 loss=95337.095047 time: 35.243829\n",
      "[Epoch 33] train=1.000000 val_top1=0.667200 val_top5=0.924700 loss=97149.983078 time: 35.201274\n",
      "[Epoch 34] train=1.000000 val_top1=0.655400 val_top5=0.927400 loss=94461.347473 time: 35.898930\n",
      "[Epoch 35] train=1.000000 val_top1=0.673500 val_top5=0.932900 loss=94281.596436 time: 35.476433\n",
      "[Epoch 36] train=1.000000 val_top1=0.664300 val_top5=0.922700 loss=94800.486832 time: 35.374277\n",
      "[Epoch 37] train=1.000000 val_top1=0.667500 val_top5=0.927200 loss=95669.132599 time: 35.571661\n",
      "[Epoch 38] train=1.000000 val_top1=0.680800 val_top5=0.935300 loss=92600.107269 time: 35.529358\n",
      "[Epoch 39] train=1.000000 val_top1=0.689400 val_top5=0.934100 loss=93111.671661 time: 35.487485\n",
      "[Epoch 40] train=1.000000 val_top1=0.656900 val_top5=0.929300 loss=91378.752350 time: 35.795757\n",
      "[Epoch 41] train=1.000000 val_top1=0.683300 val_top5=0.934700 loss=88695.019165 time: 35.526221\n",
      "[Epoch 42] train=1.000000 val_top1=0.698700 val_top5=0.939000 loss=93087.929794 time: 35.819006\n",
      "[Epoch 43] train=1.000000 val_top1=0.671100 val_top5=0.934900 loss=90291.417206 time: 35.576199\n",
      "[Epoch 44] train=1.000000 val_top1=0.708600 val_top5=0.944600 loss=92446.102341 time: 35.357940\n",
      "[Epoch 45] train=1.000000 val_top1=0.682900 val_top5=0.936900 loss=90821.465820 time: 35.150565\n",
      "[Epoch 46] train=1.000000 val_top1=0.690400 val_top5=0.933600 loss=92027.297989 time: 35.207034\n",
      "[Epoch 47] train=1.000000 val_top1=0.679700 val_top5=0.932100 loss=90973.599091 time: 35.474061\n",
      "[Epoch 48] train=1.000000 val_top1=0.686500 val_top5=0.934000 loss=89739.585190 time: 35.117790\n",
      "[Epoch 49] train=1.000000 val_top1=0.697800 val_top5=0.934400 loss=91641.007614 time: 35.405375\n",
      "[Epoch 50] train=1.000000 val_top1=0.708200 val_top5=0.942000 loss=87711.815720 time: 35.634335\n",
      "[Epoch 51] train=1.000000 val_top1=0.696000 val_top5=0.932700 loss=88891.724365 time: 35.310773\n",
      "[Epoch 52] train=1.000000 val_top1=0.690400 val_top5=0.931800 loss=89018.482010 time: 35.647956\n",
      "[Epoch 53] train=1.000000 val_top1=0.692900 val_top5=0.937700 loss=90023.082108 time: 35.218151\n",
      "[Epoch 54] train=1.000000 val_top1=0.693600 val_top5=0.937200 loss=87463.724106 time: 35.526287\n",
      "[Epoch 55] train=1.000000 val_top1=0.693400 val_top5=0.934100 loss=89054.020996 time: 35.172794\n",
      "[Epoch 56] train=1.000000 val_top1=0.679100 val_top5=0.925800 loss=86284.696167 time: 35.671198\n",
      "[Epoch 57] train=1.000000 val_top1=0.726100 val_top5=0.950500 loss=88675.862030 time: 35.516274\n",
      "[Epoch 58] train=1.000000 val_top1=0.706700 val_top5=0.940000 loss=88652.581985 time: 35.699303\n",
      "[Epoch 59] train=1.000000 val_top1=0.707800 val_top5=0.941500 loss=88422.590225 time: 35.752781\n",
      "[Epoch 60] train=1.000000 val_top1=0.704600 val_top5=0.943400 loss=88646.101654 time: 35.356350\n",
      "[Epoch 61] train=1.000000 val_top1=0.700900 val_top5=0.938700 loss=87501.155869 time: 36.482659\n",
      "[Epoch 62] train=1.000000 val_top1=0.692300 val_top5=0.939900 loss=88495.594559 time: 36.209466\n",
      "[Epoch 63] train=1.000000 val_top1=0.708800 val_top5=0.945800 loss=86816.971512 time: 35.896356\n",
      "[Epoch 64] train=1.000000 val_top1=0.705300 val_top5=0.939800 loss=88079.535660 time: 36.253475\n",
      "[Epoch 65] train=1.000000 val_top1=0.693200 val_top5=0.936300 loss=87996.259430 time: 35.853084\n",
      "[Epoch 66] train=1.000000 val_top1=0.698200 val_top5=0.941100 loss=87174.356110 time: 36.093850\n",
      "[Epoch 67] train=1.000000 val_top1=0.716100 val_top5=0.945200 loss=86390.516953 time: 35.606410\n",
      "[Epoch 68] train=1.000000 val_top1=0.719000 val_top5=0.947500 loss=88541.971115 time: 35.843621\n",
      "[Epoch 69] train=1.000000 val_top1=0.678900 val_top5=0.925400 loss=84346.966568 time: 35.526091\n",
      "[Epoch 70] train=1.000000 val_top1=0.692300 val_top5=0.935700 loss=87397.656021 time: 35.759898\n",
      "[Epoch 71] train=1.000000 val_top1=0.704500 val_top5=0.941400 loss=86513.553574 time: 35.408475\n",
      "[Epoch 72] train=1.000000 val_top1=0.709300 val_top5=0.940000 loss=87425.925262 time: 35.677671\n",
      "[Epoch 73] train=1.000000 val_top1=0.703000 val_top5=0.938000 loss=84442.501617 time: 35.542414\n",
      "[Epoch 74] train=1.000000 val_top1=0.688200 val_top5=0.931500 loss=85584.805893 time: 35.796617\n",
      "[Epoch 75] train=1.000000 val_top1=0.709700 val_top5=0.942700 loss=86731.221222 time: 35.576081\n",
      "[Epoch 76] train=1.000000 val_top1=0.699300 val_top5=0.932600 loss=84634.621414 time: 35.585576\n",
      "[Epoch 77] train=1.000000 val_top1=0.728200 val_top5=0.949100 loss=85722.019867 time: 35.535692\n",
      "[Epoch 78] train=1.000000 val_top1=0.707200 val_top5=0.936400 loss=85296.360214 time: 35.703864\n",
      "[Epoch 79] train=1.000000 val_top1=0.709300 val_top5=0.941800 loss=84331.009750 time: 35.672770\n",
      "[Epoch 80] train=1.000000 val_top1=0.721500 val_top5=0.946400 loss=87475.107117 time: 35.979027\n",
      "[Epoch 81] train=1.000000 val_top1=0.715800 val_top5=0.943300 loss=83473.923615 time: 35.762283\n",
      "[Epoch 82] train=1.000000 val_top1=0.689800 val_top5=0.923400 loss=84900.984497 time: 35.722167\n",
      "[Epoch 83] train=1.000000 val_top1=0.715000 val_top5=0.942100 loss=84159.707779 time: 35.373109\n",
      "[Epoch 84] train=1.000000 val_top1=0.714800 val_top5=0.940000 loss=84733.797363 time: 35.499856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85] train=1.000000 val_top1=0.735200 val_top5=0.950200 loss=83828.924606 time: 35.362610\n",
      "[Epoch 86] train=1.000000 val_top1=0.712600 val_top5=0.938500 loss=86212.052734 time: 35.736502\n",
      "[Epoch 87] train=1.000000 val_top1=0.720800 val_top5=0.947700 loss=83926.329391 time: 35.962309\n",
      "[Epoch 88] train=1.000000 val_top1=0.709500 val_top5=0.933700 loss=82622.077835 time: 35.475480\n",
      "[Epoch 89] train=1.000000 val_top1=0.699800 val_top5=0.934100 loss=83486.264755 time: 36.023585\n",
      "[Epoch 90] train=1.000000 val_top1=0.718000 val_top5=0.943300 loss=81768.519150 time: 35.558774\n",
      "[Epoch 91] train=1.000000 val_top1=0.727200 val_top5=0.947700 loss=83394.033676 time: 35.695907\n",
      "[Epoch 92] train=1.000000 val_top1=0.703600 val_top5=0.936700 loss=81520.557877 time: 35.738380\n",
      "[Epoch 93] train=1.000000 val_top1=0.731100 val_top5=0.948100 loss=81520.261276 time: 35.773670\n",
      "[Epoch 94] train=1.000000 val_top1=0.738100 val_top5=0.947000 loss=81428.543457 time: 35.692850\n",
      "[Epoch 95] train=1.000000 val_top1=0.737800 val_top5=0.951400 loss=83563.595612 time: 35.796320\n",
      "[Epoch 96] train=1.000000 val_top1=0.730200 val_top5=0.946000 loss=80888.995682 time: 35.791989\n",
      "[Epoch 97] train=1.000000 val_top1=0.737800 val_top5=0.946800 loss=79385.232742 time: 35.493741\n",
      "[Epoch 98] train=1.000000 val_top1=0.726200 val_top5=0.949200 loss=82951.538147 time: 35.607888\n",
      "[Epoch 99] train=1.000000 val_top1=0.722600 val_top5=0.946100 loss=80145.409302 time: 35.309339\n",
      "[Epoch 100] train=1.000000 val_top1=0.739300 val_top5=0.949900 loss=80972.402802 time: 35.342991\n",
      "[Epoch 101] train=1.000000 val_top1=0.729600 val_top5=0.946600 loss=82054.222702 time: 35.542822\n",
      "[Epoch 102] train=1.000000 val_top1=0.744500 val_top5=0.948200 loss=77859.837448 time: 35.483779\n",
      "[Epoch 103] train=1.000000 val_top1=0.730800 val_top5=0.951900 loss=81367.265747 time: 35.728571\n",
      "[Epoch 104] train=1.000000 val_top1=0.744200 val_top5=0.946600 loss=78955.278244 time: 35.592033\n",
      "[Epoch 105] train=1.000000 val_top1=0.743300 val_top5=0.950900 loss=81524.986664 time: 35.664884\n",
      "[Epoch 106] train=1.000000 val_top1=0.740100 val_top5=0.944100 loss=79513.431519 time: 35.692519\n",
      "[Epoch 107] train=1.000000 val_top1=0.738700 val_top5=0.949000 loss=80274.487747 time: 35.528939\n",
      "[Epoch 108] train=1.000000 val_top1=0.746500 val_top5=0.951100 loss=80832.981789 time: 35.668974\n",
      "[Epoch 109] train=1.000000 val_top1=0.730100 val_top5=0.948700 loss=81626.459564 time: 35.444259\n",
      "[Epoch 110] train=1.000000 val_top1=0.753700 val_top5=0.955200 loss=79312.973419 time: 35.800652\n",
      "[Epoch 111] train=1.000000 val_top1=0.739400 val_top5=0.950600 loss=79970.089066 time: 35.605072\n",
      "[Epoch 112] train=1.000000 val_top1=0.731200 val_top5=0.940800 loss=77823.027611 time: 35.549089\n",
      "[Epoch 113] train=1.000000 val_top1=0.750600 val_top5=0.953200 loss=79034.042160 time: 35.460161\n",
      "[Epoch 114] train=1.000000 val_top1=0.741500 val_top5=0.948000 loss=78022.587036 time: 35.819526\n",
      "[Epoch 115] train=1.000000 val_top1=0.744500 val_top5=0.950100 loss=78594.066269 time: 35.737210\n",
      "[Epoch 116] train=1.000000 val_top1=0.743500 val_top5=0.949900 loss=76023.317619 time: 35.309359\n",
      "[Epoch 117] train=1.000000 val_top1=0.734000 val_top5=0.941100 loss=77932.654938 time: 35.492998\n",
      "[Epoch 118] train=1.000000 val_top1=0.739000 val_top5=0.949800 loss=78723.273392 time: 35.786880\n",
      "[Epoch 119] train=1.000000 val_top1=0.735000 val_top5=0.947600 loss=75633.945389 time: 35.842406\n",
      "[Epoch 120] train=1.000000 val_top1=0.763900 val_top5=0.956200 loss=80365.992683 time: 35.344213\n",
      "[Epoch 121] train=1.000000 val_top1=0.751400 val_top5=0.952200 loss=73923.947823 time: 35.884626\n",
      "[Epoch 122] train=1.000000 val_top1=0.755700 val_top5=0.952400 loss=75013.742577 time: 35.957963\n",
      "[Epoch 123] train=1.000000 val_top1=0.740000 val_top5=0.943900 loss=78271.556892 time: 35.492553\n",
      "[Epoch 124] train=1.000000 val_top1=0.750200 val_top5=0.949000 loss=76755.106468 time: 35.776861\n",
      "[Epoch 125] train=1.000000 val_top1=0.747900 val_top5=0.948500 loss=76037.125107 time: 35.512711\n",
      "[Epoch 126] train=1.000000 val_top1=0.762500 val_top5=0.953500 loss=76660.992195 time: 35.832134\n",
      "[Epoch 127] train=1.000000 val_top1=0.757900 val_top5=0.955000 loss=77028.041862 time: 35.396608\n",
      "[Epoch 128] train=1.000000 val_top1=0.765300 val_top5=0.949700 loss=75947.064407 time: 35.718063\n",
      "[Epoch 129] train=1.000000 val_top1=0.766300 val_top5=0.953700 loss=75592.489662 time: 35.514133\n",
      "[Epoch 130] train=1.000000 val_top1=0.763400 val_top5=0.956600 loss=74660.671249 time: 35.561539\n",
      "[Epoch 131] train=1.000000 val_top1=0.763700 val_top5=0.954200 loss=75810.001328 time: 35.668516\n",
      "[Epoch 132] train=1.000000 val_top1=0.754700 val_top5=0.950500 loss=74886.212326 time: 35.461767\n",
      "[Epoch 133] train=1.000000 val_top1=0.745100 val_top5=0.944600 loss=73945.161682 time: 35.764413\n",
      "[Epoch 134] train=1.000000 val_top1=0.770200 val_top5=0.957500 loss=75616.542221 time: 35.400780\n",
      "[Epoch 135] train=1.000000 val_top1=0.760900 val_top5=0.954600 loss=72667.921188 time: 35.369947\n",
      "[Epoch 136] train=1.000000 val_top1=0.760500 val_top5=0.952600 loss=73471.316299 time: 35.329355\n",
      "[Epoch 137] train=1.000000 val_top1=0.759000 val_top5=0.950900 loss=73901.255081 time: 36.175023\n",
      "[Epoch 138] train=1.000000 val_top1=0.769500 val_top5=0.955700 loss=72430.770683 time: 35.889020\n",
      "[Epoch 139] train=1.000000 val_top1=0.767400 val_top5=0.956800 loss=71665.845703 time: 35.445083\n",
      "[Epoch 140] train=1.000000 val_top1=0.770100 val_top5=0.953200 loss=74223.224968 time: 35.557367\n",
      "[Epoch 141] train=1.000000 val_top1=0.755800 val_top5=0.949300 loss=72894.053444 time: 35.373597\n",
      "[Epoch 142] train=1.000000 val_top1=0.769500 val_top5=0.953700 loss=71438.165955 time: 35.649123\n",
      "[Epoch 143] train=1.000000 val_top1=0.769200 val_top5=0.955800 loss=75059.756203 time: 36.182208\n",
      "[Epoch 144] train=1.000000 val_top1=0.768300 val_top5=0.954500 loss=71920.994888 time: 35.958841\n",
      "[Epoch 145] train=1.000000 val_top1=0.770100 val_top5=0.953300 loss=71899.146118 time: 35.813651\n",
      "[Epoch 146] train=1.000000 val_top1=0.764500 val_top5=0.951900 loss=71774.635849 time: 35.976946\n",
      "[Epoch 147] train=1.000000 val_top1=0.776800 val_top5=0.958100 loss=73711.195091 time: 35.186506\n",
      "[Epoch 148] train=1.000000 val_top1=0.771200 val_top5=0.954900 loss=73674.410637 time: 35.656264\n",
      "[Epoch 149] train=1.000000 val_top1=0.774100 val_top5=0.955900 loss=72086.227371 time: 35.864350\n",
      "[Epoch 150] train=1.000000 val_top1=0.777600 val_top5=0.956300 loss=72792.695847 time: 35.986368\n",
      "[Epoch 151] train=1.000000 val_top1=0.776200 val_top5=0.953300 loss=72036.016289 time: 35.947975\n",
      "[Epoch 152] train=1.000000 val_top1=0.776500 val_top5=0.957600 loss=69788.487740 time: 35.528546\n",
      "[Epoch 153] train=1.000000 val_top1=0.774000 val_top5=0.952900 loss=69537.333328 time: 35.649698\n",
      "[Epoch 154] train=1.000000 val_top1=0.782700 val_top5=0.953400 loss=70780.865021 time: 35.918939\n",
      "[Epoch 155] train=1.000000 val_top1=0.781900 val_top5=0.955800 loss=68350.619019 time: 35.786828\n",
      "[Epoch 156] train=1.000000 val_top1=0.783100 val_top5=0.956400 loss=71694.585014 time: 35.327442\n",
      "[Epoch 157] train=1.000000 val_top1=0.779100 val_top5=0.958400 loss=71654.587540 time: 35.424862\n",
      "[Epoch 158] train=1.000000 val_top1=0.786000 val_top5=0.958300 loss=69338.739441 time: 35.702953\n",
      "[Epoch 159] train=1.000000 val_top1=0.784000 val_top5=0.953500 loss=71654.859627 time: 35.642914\n",
      "[Epoch 160] train=1.000000 val_top1=0.780300 val_top5=0.957400 loss=69442.793945 time: 35.518993\n",
      "[Epoch 161] train=1.000000 val_top1=0.788200 val_top5=0.956200 loss=67034.327263 time: 35.715762\n",
      "[Epoch 162] train=1.000000 val_top1=0.787800 val_top5=0.957200 loss=69515.220665 time: 35.818284\n",
      "[Epoch 163] train=1.000000 val_top1=0.781500 val_top5=0.953800 loss=68823.697060 time: 35.740485\n",
      "[Epoch 164] train=1.000000 val_top1=0.789300 val_top5=0.957600 loss=67934.831528 time: 35.834048\n",
      "[Epoch 165] train=1.000000 val_top1=0.786900 val_top5=0.955100 loss=67517.141083 time: 35.609391\n",
      "[Epoch 166] train=1.000000 val_top1=0.786700 val_top5=0.955600 loss=68799.594238 time: 35.859536\n",
      "[Epoch 167] train=1.000000 val_top1=0.790700 val_top5=0.954500 loss=65199.530304 time: 35.782972\n",
      "[Epoch 168] train=1.000000 val_top1=0.789100 val_top5=0.955700 loss=65960.153763 time: 36.193333\n",
      "[Epoch 169] train=1.000000 val_top1=0.791300 val_top5=0.956400 loss=70039.817017 time: 36.107401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 170] train=1.000000 val_top1=0.795700 val_top5=0.957600 loss=65461.563560 time: 35.912941\n",
      "[Epoch 171] train=1.000000 val_top1=0.790000 val_top5=0.955200 loss=66418.806755 time: 36.028892\n",
      "[Epoch 172] train=1.000000 val_top1=0.793100 val_top5=0.957900 loss=66806.088196 time: 35.958694\n",
      "[Epoch 173] train=1.000000 val_top1=0.796300 val_top5=0.960000 loss=66230.226990 time: 35.978366\n",
      "[Epoch 174] train=1.000000 val_top1=0.797100 val_top5=0.961700 loss=66732.175987 time: 35.786523\n",
      "[Epoch 175] train=1.000000 val_top1=0.791000 val_top5=0.957400 loss=67656.295525 time: 35.787295\n",
      "[Epoch 176] train=1.000000 val_top1=0.796500 val_top5=0.956700 loss=63882.782669 time: 35.665722\n",
      "[Epoch 177] train=1.000000 val_top1=0.795800 val_top5=0.956800 loss=67143.247566 time: 35.446653\n",
      "[Epoch 178] train=1.000000 val_top1=0.794800 val_top5=0.956400 loss=64705.957458 time: 35.776904\n",
      "[Epoch 179] train=1.000000 val_top1=0.795700 val_top5=0.957600 loss=64302.986359 time: 35.655730\n",
      "[Epoch 180] train=1.000000 val_top1=0.795900 val_top5=0.958400 loss=68171.684464 time: 35.905740\n",
      "[Epoch 181] train=1.000000 val_top1=0.798600 val_top5=0.957500 loss=65786.720245 time: 36.210220\n",
      "[Epoch 182] train=1.000000 val_top1=0.797100 val_top5=0.958800 loss=67147.847252 time: 36.301463\n",
      "[Epoch 183] train=1.000000 val_top1=0.796000 val_top5=0.957600 loss=67266.110085 time: 36.061234\n",
      "[Epoch 184] train=1.000000 val_top1=0.796500 val_top5=0.959400 loss=66135.047157 time: 36.112658\n",
      "[Epoch 185] train=1.000000 val_top1=0.796400 val_top5=0.958900 loss=65993.982338 time: 36.030273\n",
      "[Epoch 186] train=1.000000 val_top1=0.798800 val_top5=0.958300 loss=65325.332794 time: 35.779496\n",
      "[Epoch 187] train=1.000000 val_top1=0.795500 val_top5=0.958700 loss=67164.242821 time: 35.789934\n",
      "[Epoch 188] train=1.000000 val_top1=0.796900 val_top5=0.958600 loss=69713.986389 time: 35.982323\n",
      "[Epoch 189] train=1.000000 val_top1=0.797300 val_top5=0.959100 loss=64970.785973 time: 35.751723\n",
      "[Epoch 190] train=1.000000 val_top1=0.798000 val_top5=0.958300 loss=67580.184074 time: 35.828334\n",
      "[Epoch 191] train=1.000000 val_top1=0.796300 val_top5=0.958200 loss=65798.092758 time: 35.531858\n",
      "[Epoch 192] train=1.000000 val_top1=0.797700 val_top5=0.958400 loss=63403.102699 time: 35.726320\n",
      "[Epoch 193] train=1.000000 val_top1=0.798700 val_top5=0.959000 loss=63908.586411 time: 35.598138\n",
      "[Epoch 194] train=1.000000 val_top1=0.797700 val_top5=0.958200 loss=65614.212967 time: 35.664268\n",
      "[Epoch 195] train=1.000000 val_top1=0.798500 val_top5=0.958300 loss=65390.288803 time: 35.861205\n",
      "[Epoch 196] train=1.000000 val_top1=0.795600 val_top5=0.957900 loss=63186.314857 time: 35.597865\n",
      "[Epoch 197] train=1.000000 val_top1=0.799200 val_top5=0.959300 loss=66119.979034 time: 35.841044\n",
      "[Epoch 198] train=1.000000 val_top1=0.795700 val_top5=0.958300 loss=64703.607910 time: 35.890059\n",
      "[Epoch 199] train=1.000000 val_top1=0.797900 val_top5=0.958300 loss=65404.658524 time: 35.610662\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3f778327ee87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# We can plot the metric scores with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d_%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m train_history.plot(['training-error', 'validation-error'], \n\u001b[1;32m     82\u001b[0m                     save_path=\"./cifar100_resnet56_v1_nag_errors_{}.png\".format(timestamp))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started:\")\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            if epoch >= epochs - mixup_off_epochs:\n",
    "                lam = 1\n",
    "            data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "            if label_smoothing:\n",
    "                eta = 0.1\n",
    "            else:\n",
    "                eta = 0.0\n",
    "\n",
    "            label = mixup_transform(label, num_classes, lam, eta)\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            print(\"Running Distillation :)\")\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype('float32', copy=False),\n",
    "                                y.astype('float32', copy=False),\n",
    "                                p.astype('float32', copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                       label, \n",
    "                                                                                       teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                    save_path=\"./cifar100_resnet56_v1_nag_errors_{}.png\".format(timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                     save_path=\"./cifar100_resnet56_v1_nag_accuracies_{}.png\".format(timestamp))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
