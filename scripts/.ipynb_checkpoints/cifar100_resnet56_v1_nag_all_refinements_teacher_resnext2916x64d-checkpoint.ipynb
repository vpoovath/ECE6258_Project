{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 256 # Batch Size for Each GPU\n",
    "num_workers = 2             # Number of data loader workers\n",
    "dtype = 'float32'           # Default training data type if float32\n",
    "num_gpus = 1                # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = False\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = False\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using mixup: {}\".format(mixup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# last_gamma = False\n",
    "# print(\"Using last gamma: {}\".format(last_gamma))\n",
    "# kwargs = {'ctx':ctx, 'classes':num_classes, 'last_gamma':last_gamma}\n",
    "\n",
    "kwargs = {'ctx':ctx, 'classes':num_classes}\n",
    "\n",
    "use_group_norm = False\n",
    "if use_group_norm:\n",
    "    kwargs['norm_layer'] = gcv.nn.GroupNorm\n",
    "    print(\"Using Group Normalization: {}\".format(use_group_norm))\n",
    "\n",
    "default_init = True\n",
    "net = get_model('cifar_resnet56_v1', **kwargs)\n",
    "\n",
    "if default_init:\n",
    "    net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "else:\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    print(\"Using MSRA Prelu Init.\")\n",
    "\n",
    "    net.cast(dtype)\n",
    "print(\"\\nModel Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "Load the pre-trained CIFAR10 models and replace the final output layer with 100 classes instead of 10. This is demonstrated at this website: https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/packages/gluon/image/pretrained_models.html\n",
    "\n",
    "Need to investigate WideResNet issue of having Top1-Val Accuracies becoming 0.00000 at the very beginning of training. \n",
    "\n",
    "Additionally, need to understand the two ResNeXt architectures to understand why training time is much longer...:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation = False\n",
    "if distillation:\n",
    "    T = 20\n",
    "    hard_weight = 0.5\n",
    "    # Teacher model for distillation training\n",
    "    # teacher_name = 'cifar_resnet110_v2'\n",
    "    # teacher_name = 'cifar_resnet56_v2'\n",
    "    teacher_name = 'cifar_resnext29_16x64d'\n",
    "    # teacher_name = 'cifar_wideresnet28_10' # Top1-Val is 0...\n",
    "    # teacher_name = 'cifar_wideresnet40_8'  # Might cause the same problem\n",
    "    # teacher_name = 'cifar_resnext29_32x4d' # This is apparently not available...?\n",
    "    \n",
    "    teacher = get_model(teacher_name, pretrained=True, ctx=ctx)\n",
    "    #teacher.collect_params().initialize(ctx=ctx, force_reinit=True) # Don't do this.\n",
    "    teacher.cast(dtype)\n",
    "\n",
    "    with teacher.name_scope():\n",
    "        teacher.output = gluon.nn.Dense(num_classes)\n",
    "        teacher.output.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "    print(teacher.output)\n",
    "    print(\"\\nTeacher Model Init Done!\")\n",
    "else:\n",
    "    print(\"\\nNot using distillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")\n",
    "print(\"Per Device Batch Size: {}\".format(per_device_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mixup:\n",
    "    epochs = 200 # Mixup asks for longer training to converge better\n",
    "else:\n",
    "    epochs = 120\n",
    "    \n",
    "warmup_epochs = 10\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "lr_decay = 0.1 # Learning rate decay factor\n",
    "\n",
    "# Epochs where learning rate decays\n",
    "# Note that the default reference paper's code sets the lr_decay_epoch to [40,60]\n",
    "lr_decay_epochs = [30, 60, 90, np.inf]\n",
    "\n",
    "warmup_lr_mode = 'linear'\n",
    "lr_mode = 'cosine'\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler(warmup_lr_mode,\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler(lr_mode,\n",
    "                base_lr = 0.1,\n",
    "                target_lr = 0,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epochs,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistillationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                           hard_weight=hard_weight,\n",
    "                                                           sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))\n",
    "print(\"Using mixup: {}\".format(mixup))\n",
    "\n",
    "print(\"\\nUsing {} Optimizer\".format(optimizer))\n",
    "print(optimizer_params)\n",
    "print(\"\\nNumber of warmup epochs: {}\".format(warmup_epochs))\n",
    "print(\"Warmup Learning Rate Mode: {}\".format(warmup_lr_mode))\n",
    "print(\"Learing Rate Mode: {}\".format(lr_mode))\n",
    "print(\"Learing Rate Decay: {}\".format(lr_decay))\n",
    "print(\"Learning Rate Decay Epochs: {}\".format(lr_decay_epochs))\n",
    "print(\"\\nTraining Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started for {} epochs:\".format(epochs))\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            if epoch >= epochs - mixup_off_epochs:\n",
    "                lam = 1\n",
    "            data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "            if label_smoothing:\n",
    "                eta = 0.1\n",
    "            else:\n",
    "                eta = 0.0\n",
    "\n",
    "            label = mixup_transform(label, num_classes, lam, eta)\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype(dtype, copy=False),\n",
    "                                y.astype(dtype, copy=False),\n",
    "                                p.astype(dtype, copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                       label, \n",
    "                                                                                       teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, outputs)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_errors_{t}.png\".format(o=optimizer,\n",
    "                                                                                           ep=epochs,\n",
    "                                                                                           t=timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_accuracies_{t}.png\".format(o=optimizer,\n",
    "                                                                                               ep=epochs,\n",
    "                                                                                               t=timestamp))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
