{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 128 # Batch Size for Each GPU\n",
    "num_workers = 2 # Number of data loader workers\n",
    "dtype = 'float32' # Default training data type if float32\n",
    "num_gpus = 1      # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = True\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "net = get_model('cifar_resnet56_v1', classes=100)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "net.cast(dtype)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "Load the pre-trained CIFAR10 models and replace the final output layer with 100 classes instead of 10. This is demonstrated at this website: https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/packages/gluon/image/pretrained_models.html\n",
    "\n",
    "Need to investigate WideResNet issue of having Top1-Val Accuracies becoming 0.00000 at the very beginning of training. \n",
    "\n",
    "Additionally, need to understand the two ResNeXt architectures to understand why training time is much longer...:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(None -> 100, linear)\n",
      "Teacher Model Init Done!\n"
     ]
    }
   ],
   "source": [
    "distillation = True\n",
    "T = 20\n",
    "hard_weight = 0.5\n",
    "# Teacher model for distillation training\n",
    "# teacher_name = 'cifar_resnet110_v2'\n",
    "# teacher_name = 'cifar_wideresnet28_10' # Top1-Val is 0...\n",
    "# teacher_name = 'cifar_wideresnet40_8'  # Might cause the same problem\n",
    "teacher_name = 'cifar_resnext29_16x64d'\n",
    "# teacher_name = 'cifar_resnext29_32x4d'\n",
    "teacher = get_model(teacher_name, pretrained=True, ctx=ctx)\n",
    "teacher.collect_params().initialize(ctx=ctx, force_reinit=True)\n",
    "teacher.cast(dtype)\n",
    "\n",
    "with teacher.name_scope():\n",
    "    teacher.output = gluon.nn.Dense(num_classes)\n",
    "    teacher.output.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "print(teacher.output)\n",
    "print(\"Teacher Model Init Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n"
     ]
    }
   ],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n",
      "Training Settings Set Successfully.\n"
     ]
    }
   ],
   "source": [
    "# epochs = 120\n",
    "epochs = 200 # Mixup asks for longer training to converge better\n",
    "warmup_epochs = 10\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler('linear',\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler('cosine',\n",
    "                base_lr = 0.1,\n",
    "                target_lr = 0,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epoch,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistillationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                           hard_weight=hard_weight,\n",
    "                                                           sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "print(\"Training Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started:\n",
      "[Epoch 0] train=0.999629 val_top1=0.315700 val_top5=0.729000 loss=47618498.984375 time: 107.969184\n",
      "[Epoch 1] train=1.000000 val_top1=0.373600 val_top5=0.778000 loss=46053679.453125 time: 101.846538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f381ff6caa4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/gpu/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, retain_graph, train_mode)\u001b[0m\n\u001b[1;32m   2871\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2873\u001b[0;31m             ctypes.c_void_p(0)))\n\u001b[0m\u001b[1;32m   2874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtostype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started:\")\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            if epoch >= epochs - mixup_off_epochs:\n",
    "                lam = 1\n",
    "            data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "            if label_smoothing:\n",
    "                eta = 0.1\n",
    "            else:\n",
    "                eta = 0.0\n",
    "\n",
    "            label = mixup_transform(label, num_classes, lam, eta)\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype('float32', copy=False),\n",
    "                                y.astype('float32', copy=False),\n",
    "                                p.astype('float32', copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                       label, \n",
    "                                                                                       teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                    save_path=\"./cifar100_resnet56_v1_nag_errors_{}.png\".format(timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                     save_path=\"./cifar100_resnet56_v1_nag_accuracies_{}.png\".format(timestamp))\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
