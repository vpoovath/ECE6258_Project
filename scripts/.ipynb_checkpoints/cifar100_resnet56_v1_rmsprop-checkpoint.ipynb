{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "net = get_model('cifar_resnet56_v1', classes=100)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "\n",
    "# Number of data loader workers\n",
    "num_workers = 2\n",
    "\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started:\n",
      "[Epoch 0] train=0.101703 val=0.146800 loss=148368.800110 time: 32.974701\n",
      "[Epoch 1] train=0.188041 val=0.238300 loss=131713.777344 time: 33.013875\n",
      "[Epoch 2] train=0.248297 val=0.290300 loss=121829.123901 time: 33.150916\n",
      "[Epoch 3] train=0.294030 val=0.303600 loss=114043.763504 time: 32.901604\n",
      "[Epoch 4] train=0.332212 val=0.356000 loss=107722.392685 time: 32.855907\n",
      "[Epoch 5] train=0.365645 val=0.388600 loss=102232.970261 time: 32.983361\n",
      "[Epoch 6] train=0.389163 val=0.395500 loss=97764.474487 time: 33.045797\n",
      "[Epoch 7] train=0.416326 val=0.410400 loss=93143.438995 time: 32.993798\n",
      "[Epoch 8] train=0.448217 val=0.464600 loss=88603.311691 time: 32.941602\n",
      "[Epoch 9] train=0.473217 val=0.499000 loss=83912.801743 time: 32.834863\n",
      "[Epoch 10] train=0.493750 val=0.492200 loss=80064.118958 time: 32.962049\n",
      "[Epoch 11] train=0.514724 val=0.506300 loss=76745.274597 time: 32.982924\n",
      "[Epoch 12] train=0.528886 val=0.525300 loss=74456.694046 time: 33.059269\n",
      "[Epoch 13] train=0.545353 val=0.529700 loss=72114.416504 time: 33.212428\n",
      "[Epoch 14] train=0.553846 val=0.527200 loss=69858.419052 time: 33.127896\n",
      "[Epoch 15] train=0.571855 val=0.540400 loss=67737.035080 time: 32.867809\n",
      "[Epoch 16] train=0.582432 val=0.537800 loss=65849.067230 time: 33.049862\n",
      "[Epoch 17] train=0.593990 val=0.553000 loss=63971.705208 time: 32.908655\n",
      "[Epoch 18] train=0.605789 val=0.570100 loss=62265.526154 time: 33.191444\n",
      "[Epoch 19] train=0.614884 val=0.574200 loss=60560.038521 time: 33.157485\n",
      "[Epoch 20] train=0.623478 val=0.543200 loss=59274.513229 time: 32.891907\n",
      "[Epoch 21] train=0.628385 val=0.565400 loss=58049.236481 time: 32.934252\n",
      "[Epoch 22] train=0.639423 val=0.598600 loss=56551.079208 time: 33.059974\n",
      "[Epoch 23] train=0.644371 val=0.609100 loss=55605.111465 time: 33.180604\n",
      "[Epoch 24] train=0.650621 val=0.606200 loss=54535.735039 time: 32.892435\n",
      "[Epoch 25] train=0.657512 val=0.600100 loss=53408.070175 time: 33.032624\n",
      "[Epoch 26] train=0.663522 val=0.618800 loss=52447.229141 time: 33.009530\n",
      "[Epoch 27] train=0.669832 val=0.605200 loss=51665.743118 time: 32.789643\n",
      "[Epoch 28] train=0.676562 val=0.630000 loss=50518.609467 time: 32.994222\n",
      "[Epoch 29] train=0.680248 val=0.623800 loss=49722.569794 time: 33.228498\n",
      "[Epoch 30] train=0.735817 val=0.698900 loss=41189.128059 time: 32.887890\n",
      "[Epoch 31] train=0.761558 val=0.703200 loss=36864.037872 time: 33.218798\n",
      "[Epoch 32] train=0.769551 val=0.704700 loss=35473.278774 time: 32.879559\n",
      "[Epoch 33] train=0.776082 val=0.707200 loss=34567.288467 time: 32.914418\n",
      "[Epoch 34] train=0.780709 val=0.710300 loss=33720.110889 time: 32.824850\n",
      "[Epoch 35] train=0.785357 val=0.707100 loss=32826.159416 time: 33.054240\n",
      "[Epoch 36] train=0.789583 val=0.709400 loss=32261.973381 time: 33.192026\n",
      "[Epoch 37] train=0.794631 val=0.715100 loss=31524.802525 time: 33.225479\n",
      "[Epoch 38] train=0.796014 val=0.714700 loss=31165.069107 time: 32.834966\n",
      "[Epoch 39] train=0.800801 val=0.714000 loss=30575.261509 time: 33.060184\n",
      "[Epoch 40] train=0.804968 val=0.716300 loss=30034.452190 time: 34.421473\n",
      "[Epoch 41] train=0.805248 val=0.713900 loss=29547.276669 time: 32.932229\n",
      "[Epoch 42] train=0.807432 val=0.709200 loss=29415.015312 time: 32.971130\n",
      "[Epoch 43] train=0.813101 val=0.710700 loss=28611.851910 time: 32.826712\n",
      "[Epoch 44] train=0.813101 val=0.712400 loss=28260.055309 time: 32.830086\n",
      "[Epoch 45] train=0.818870 val=0.711100 loss=27831.244881 time: 33.219007\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "lr_decay_count = 0\n",
    "\n",
    "print(\"Training loop started:\")\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot(['training-error', 'validation-error'], save_path=\"./cifar100_resnet56_v1_rmsprop.png\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
