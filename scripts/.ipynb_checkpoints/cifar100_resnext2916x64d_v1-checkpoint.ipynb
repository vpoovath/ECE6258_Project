{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Model\n",
    "This notebook trains the ResNext29_16x64d teacher model. \n",
    "The reference paper mentions that the teacher model they used was trained using label smoothing and cosine learning rate decay. \n",
    "The same is done here, but, as seen before on ResNet56 v1 for CIFAR100, the warmup period is increased to T=20 to see if that improves the results of the teacher model, and later, on the student which the teacher trains. \n",
    "\n",
    "The parameters of the teacher model is saved to a .parmas file.\n",
    "\n",
    "The time it takes to train this specific teacher model is roughly ~10 hours...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math, os, sys\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 64 # Batch Size for Each GPU\n",
    "num_workers = 2             # Number of data loader workers\n",
    "dtype = 'float32'           # Default training data type if float32\n",
    "num_gpus = 1                # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using label smoothing: True\n"
     ]
    }
   ],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mixup: False\n"
     ]
    }
   ],
   "source": [
    "mixup = False\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using mixup: {}\".format(mixup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# last_gamma = False\n",
    "# print(\"Using last gamma: {}\".format(last_gamma))\n",
    "# kwargs = {'ctx':ctx, 'classes':num_classes, 'last_gamma':last_gamma}\n",
    "\n",
    "kwargs = {'ctx':ctx, 'classes':num_classes}\n",
    "\n",
    "use_group_norm = False\n",
    "if use_group_norm:\n",
    "    kwargs['norm_layer'] = gcv.nn.GroupNorm\n",
    "    print(\"Using Group Normalization: {}\".format(use_group_norm))\n",
    "\n",
    "default_init = True\n",
    "net = get_model('cifar_resnext29_16x64d', **kwargs)\n",
    "\n",
    "if default_init:\n",
    "    net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "else:\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    print(\"Using MSRA Prelu Init.\")\n",
    "\n",
    "    net.cast(dtype)\n",
    "print(\"\\nModel Init Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n",
      "Per Device Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")\n",
    "print(\"Per Device Batch Size: {}\".format(per_device_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n",
      "\n",
      "Using label smoothing: True\n",
      "Using mixup: False\n",
      "\n",
      "Using nag Optimizer\n",
      "{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7fe79ed84c90>, 'wd': 0.0001, 'momentum': 0.9}\n",
      "\n",
      "Number of warmup epochs: 5\n",
      "Warmup Learning Rate Mode: linear\n",
      "Learing Rate Mode: cosine\n",
      "Learing Rate Decay: 0.1\n",
      "Learning Rate Decay Epochs: [30, 60, 90, inf]\n",
      "\n",
      "Training Settings Set Successfully.\n"
     ]
    }
   ],
   "source": [
    "if mixup:\n",
    "    epochs = 200 # Mixup asks for longer training to converge better\n",
    "else:\n",
    "#     epochs = 120\n",
    "    epochs = 1\n",
    "    \n",
    "warmup_epochs = 20\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "lr_decay_epochs = [30, 60, 90, np.inf] # Epochs where learning rate decays\n",
    "# lr_decay_epochs = [40, 80]\n",
    "\n",
    "warmup_lr_mode = 'linear'\n",
    "lr_mode = 'cosine'\n",
    "lr_decay = 0.1 # Learning rate decay factor\n",
    "target_lr = 0\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler(warmup_lr_mode,\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler(lr_mode,\n",
    "                base_lr = 0.1,\n",
    "                target_lr = target_lr,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epochs,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))\n",
    "print(\"Using mixup: {}\".format(mixup))\n",
    "\n",
    "print(\"\\nUsing {} Optimizer\".format(optimizer))\n",
    "print(optimizer_params)\n",
    "print(\"\\nNumber of warmup epochs: {}\".format(warmup_epochs))\n",
    "print(\"Warmup Learning Rate Mode: {}\".format(warmup_lr_mode))\n",
    "print(\"Learing Rate Mode: {}\".format(lr_mode))\n",
    "print(\"Learing Rate Decay: {}\".format(lr_decay))\n",
    "print(\"Learning Rate Decay Epochs: {}\".format(lr_decay_epochs))\n",
    "print(\"\\nTraining Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started for 1 epochs:\n",
      "[Epoch 0] train=0.172775 val_top1=0.308900 val_top5=0.681300 loss=161583.934113 time: 300.703773\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started for {} epochs:\".format(epochs))\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            if epoch >= epochs - mixup_off_epochs:\n",
    "                lam = 1\n",
    "            data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "            if label_smoothing:\n",
    "                eta = 0.1\n",
    "            else:\n",
    "                eta = 0.0\n",
    "\n",
    "            label = mixup_transform(label, num_classes, lam, eta)\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, outputs)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_errors_{t}.png\".format(o=optimizer,\n",
    "                                                                                           ep=epochs,\n",
    "                                                                                           t=timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_accuracies_{t}.png\".format(o=optimizer,\n",
    "                                                                                               ep=epochs,\n",
    "                                                                                               t=timestamp))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Net and Parameters to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Parameters saved to /home/ubuntu/ECE6258_Project/scripts/resnext29_teacher.params\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(os.getcwd(), \"resnext29_teacher.params\")\n",
    "net.save_parameters(filename)\n",
    "print(\"Teacher Parameters saved to {}\".format(filename))\n",
    "\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Loading Model Params\n",
    "Will have to do this when using this CNN model for distillation training with the CIFAR_ResNet56_v1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(1024 -> 100, linear)\n"
     ]
    }
   ],
   "source": [
    "teacher_name = 'cifar_resnext29_16x64d'\n",
    "teacher = get_model(teacher_name, classes=num_classes, ctx=ctx)\n",
    "teacher.load_parameters(filename)\n",
    "\n",
    "teacher.cast(dtype)\n",
    "print(teacher.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
