{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model CIFAR_ResNet20_v1, with 10 output classes, without pre-trained weights\n",
    "#net = get_model('cifar_resnet20_v1', classes=10)\n",
    "net = get_model('ResNet50_v2', classes=10)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent\n",
    "optimizer = 'nag'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.767428 val=0.738800 loss=33607.867222 time: 16.096704\n",
      "[Epoch 1] train=0.787800 val=0.795400 loss=30360.378284 time: 16.108787\n",
      "[Epoch 2] train=0.805589 val=0.768500 loss=28007.304749 time: 16.030437\n",
      "[Epoch 3] train=0.818790 val=0.796200 loss=26165.128380 time: 16.034077\n",
      "[Epoch 4] train=0.826362 val=0.809300 loss=24844.467133 time: 16.104359\n",
      "[Epoch 5] train=0.836338 val=0.798100 loss=23512.778351 time: 16.008881\n",
      "[Epoch 6] train=0.844010 val=0.831400 loss=22572.682369 time: 16.041172\n",
      "[Epoch 7] train=0.850641 val=0.833700 loss=21622.834566 time: 16.152831\n",
      "[Epoch 8] train=0.854948 val=0.816500 loss=20915.366274 time: 16.123063\n",
      "[Epoch 9] train=0.859756 val=0.805400 loss=20184.807514 time: 16.130968\n",
      "[Epoch 10] train=0.862560 val=0.818800 loss=19796.422306 time: 16.249558\n",
      "[Epoch 11] train=0.868950 val=0.831200 loss=18993.856321 time: 16.116868\n",
      "[Epoch 12] train=0.870893 val=0.800600 loss=18612.637062 time: 16.112872\n",
      "[Epoch 13] train=0.871454 val=0.850500 loss=18452.056797 time: 16.076822\n",
      "[Epoch 14] train=0.878666 val=0.835400 loss=17783.587023 time: 16.269795\n",
      "[Epoch 15] train=0.878546 val=0.830800 loss=17395.110334 time: 16.191911\n",
      "[Epoch 16] train=0.881150 val=0.824800 loss=17015.902596 time: 16.160480\n",
      "[Epoch 17] train=0.884315 val=0.843200 loss=16570.814104 time: 16.039003\n",
      "[Epoch 18] train=0.888421 val=0.850500 loss=16255.039520 time: 16.077260\n",
      "[Epoch 19] train=0.889283 val=0.830900 loss=15959.261524 time: 16.111027\n",
      "[Epoch 20] train=0.888962 val=0.841000 loss=15757.684864 time: 16.089633\n",
      "[Epoch 21] train=0.890966 val=0.850900 loss=15556.038412 time: 16.167104\n",
      "[Epoch 22] train=0.890825 val=0.848500 loss=15523.326508 time: 16.200968\n",
      "[Epoch 23] train=0.894091 val=0.844500 loss=15145.192177 time: 16.228815\n",
      "[Epoch 24] train=0.897756 val=0.861800 loss=14947.025776 time: 16.266896\n",
      "[Epoch 25] train=0.898938 val=0.836800 loss=14573.071413 time: 16.372338\n",
      "[Epoch 26] train=0.896575 val=0.846300 loss=14676.207369 time: 16.194828\n",
      "[Epoch 27] train=0.898878 val=0.839900 loss=14385.228186 time: 16.278641\n",
      "[Epoch 28] train=0.902925 val=0.858200 loss=14086.689660 time: 16.219330\n",
      "[Epoch 29] train=0.901743 val=0.867200 loss=14002.049226 time: 16.390303\n",
      "[Epoch 30] train=0.902764 val=0.854900 loss=13829.213318 time: 16.216118\n",
      "[Epoch 31] train=0.903886 val=0.846100 loss=13814.393303 time: 16.137396\n",
      "[Epoch 32] train=0.905569 val=0.865400 loss=13343.197233 time: 16.298457\n",
      "[Epoch 33] train=0.903646 val=0.853900 loss=13720.005774 time: 16.376786\n",
      "[Epoch 34] train=0.906130 val=0.854400 loss=13268.753654 time: 16.267706\n",
      "[Epoch 35] train=0.905168 val=0.865500 loss=13350.705827 time: 16.196838\n",
      "[Epoch 36] train=0.908233 val=0.854200 loss=13153.293342 time: 16.079393\n",
      "[Epoch 37] train=0.908714 val=0.871100 loss=13031.366835 time: 16.361937\n",
      "[Epoch 38] train=0.908734 val=0.831200 loss=12865.624822 time: 16.144727\n",
      "[Epoch 39] train=0.908634 val=0.859500 loss=12778.452309 time: 16.179627\n",
      "[Epoch 40] train=0.910537 val=0.844900 loss=12813.664438 time: 16.262556\n",
      "[Epoch 41] train=0.910357 val=0.860900 loss=12755.205951 time: 16.241747\n",
      "[Epoch 42] train=0.912821 val=0.845300 loss=12569.096712 time: 16.332053\n",
      "[Epoch 43] train=0.912480 val=0.855400 loss=12559.800220 time: 16.115785\n",
      "[Epoch 44] train=0.913001 val=0.864900 loss=12482.626590 time: 16.437915\n",
      "[Epoch 45] train=0.914062 val=0.870900 loss=12218.413013 time: 16.135483\n",
      "[Epoch 46] train=0.914303 val=0.862800 loss=12214.064649 time: 16.138781\n",
      "[Epoch 47] train=0.912760 val=0.862600 loss=12345.110075 time: 16.179436\n",
      "[Epoch 48] train=0.915064 val=0.855800 loss=12072.478457 time: 16.268057\n",
      "[Epoch 49] train=0.914243 val=0.867500 loss=12228.571527 time: 16.349516\n",
      "[Epoch 50] train=0.916386 val=0.858000 loss=11899.213649 time: 16.366969\n",
      "[Epoch 51] train=0.916506 val=0.872200 loss=11951.131744 time: 16.182591\n",
      "[Epoch 52] train=0.917067 val=0.871200 loss=11812.991045 time: 16.353278\n",
      "[Epoch 53] train=0.916326 val=0.873700 loss=11922.291748 time: 16.223053\n",
      "[Epoch 54] train=0.919331 val=0.855100 loss=11584.865931 time: 16.155271\n",
      "[Epoch 55] train=0.916887 val=0.866000 loss=11849.079445 time: 16.347519\n",
      "[Epoch 56] train=0.920012 val=0.872600 loss=11552.789871 time: 16.370836\n",
      "[Epoch 57] train=0.919411 val=0.867400 loss=11507.566732 time: 16.296432\n",
      "[Epoch 58] train=0.920072 val=0.854300 loss=11456.244586 time: 16.032566\n",
      "[Epoch 59] train=0.918770 val=0.871800 loss=11712.055549 time: 16.110547\n",
      "[Epoch 60] train=0.918249 val=0.861700 loss=11538.627846 time: 16.420945\n",
      "[Epoch 61] train=0.919892 val=0.872000 loss=11396.117173 time: 16.165001\n",
      "[Epoch 62] train=0.920994 val=0.866000 loss=11311.573685 time: 16.320767\n",
      "[Epoch 63] train=0.922356 val=0.861400 loss=11103.018582 time: 16.307141\n",
      "[Epoch 64] train=0.919692 val=0.859800 loss=11281.691761 time: 16.151295\n",
      "[Epoch 65] train=0.920272 val=0.858400 loss=11278.452368 time: 16.154611\n",
      "[Epoch 66] train=0.922636 val=0.852400 loss=11076.272791 time: 16.159627\n",
      "[Epoch 67] train=0.922516 val=0.873100 loss=10925.516868 time: 16.121876\n",
      "[Epoch 68] train=0.919631 val=0.852000 loss=11317.424711 time: 16.118157\n",
      "[Epoch 69] train=0.922476 val=0.870100 loss=11057.906440 time: 16.091996\n",
      "[Epoch 70] train=0.922857 val=0.849900 loss=10878.925168 time: 16.105920\n",
      "[Epoch 71] train=0.920954 val=0.847400 loss=11249.968149 time: 16.280173\n",
      "[Epoch 72] train=0.922216 val=0.846600 loss=10883.060144 time: 16.179677\n",
      "[Epoch 73] train=0.923417 val=0.862700 loss=10919.277473 time: 16.296593\n",
      "[Epoch 74] train=0.923377 val=0.880600 loss=10819.709249 time: 16.164838\n",
      "[Epoch 75] train=0.921855 val=0.863400 loss=10995.761676 time: 16.160634\n",
      "[Epoch 76] train=0.923377 val=0.877600 loss=10753.766781 time: 16.069869\n",
      "[Epoch 77] train=0.923658 val=0.841900 loss=10792.547899 time: 16.186619\n",
      "[Epoch 78] train=0.923157 val=0.860900 loss=10867.263745 time: 16.234128\n",
      "[Epoch 79] train=0.923938 val=0.875100 loss=10871.038634 time: 16.123175\n",
      "[Epoch 80] train=0.950481 val=0.906600 loss=7309.597989 time: 16.205792\n",
      "[Epoch 81] train=0.962320 val=0.909700 loss=5616.217185 time: 16.174491\n",
      "[Epoch 82] train=0.966587 val=0.908800 loss=4990.384204 time: 16.251608\n",
      "[Epoch 83] train=0.968590 val=0.910500 loss=4673.253169 time: 16.235662\n",
      "[Epoch 84] train=0.972596 val=0.911300 loss=4159.755115 time: 16.202957\n",
      "[Epoch 85] train=0.972336 val=0.910600 loss=4137.129186 time: 16.151212\n",
      "[Epoch 86] train=0.973417 val=0.912300 loss=3891.124412 time: 16.236488\n",
      "[Epoch 87] train=0.975120 val=0.915400 loss=3734.289097 time: 16.172685\n",
      "[Epoch 88] train=0.976562 val=0.914600 loss=3468.386917 time: 16.194150\n",
      "[Epoch 89] train=0.976002 val=0.912000 loss=3466.911605 time: 16.322278\n",
      "[Epoch 90] train=0.977724 val=0.914000 loss=3348.679429 time: 16.325270\n",
      "[Epoch 91] train=0.977945 val=0.914300 loss=3302.685071 time: 16.083365\n",
      "[Epoch 92] train=0.977965 val=0.912900 loss=3247.344206 time: 16.173341\n",
      "[Epoch 93] train=0.980068 val=0.911300 loss=2982.066743 time: 16.265387\n",
      "[Epoch 94] train=0.980569 val=0.913500 loss=2925.927369 time: 16.155651\n",
      "[Epoch 95] train=0.981370 val=0.914700 loss=2802.565063 time: 16.434745\n",
      "[Epoch 96] train=0.981530 val=0.914100 loss=2718.589366 time: 16.193238\n",
      "[Epoch 97] train=0.982091 val=0.912600 loss=2684.736314 time: 16.051935\n",
      "[Epoch 98] train=0.983133 val=0.913000 loss=2581.524631 time: 16.148180\n",
      "[Epoch 99] train=0.983113 val=0.912400 loss=2534.618926 time: 16.124313\n",
      "[Epoch 100] train=0.982812 val=0.914800 loss=2524.467073 time: 16.324591\n",
      "[Epoch 101] train=0.983353 val=0.913400 loss=2488.075244 time: 16.353146\n",
      "[Epoch 102] train=0.983213 val=0.914300 loss=2389.949516 time: 16.194866\n",
      "[Epoch 103] train=0.984796 val=0.914300 loss=2296.315614 time: 16.139939\n",
      "[Epoch 104] train=0.984615 val=0.914700 loss=2323.188092 time: 16.150506\n",
      "[Epoch 105] train=0.984896 val=0.914500 loss=2276.908093 time: 16.162014\n",
      "[Epoch 106] train=0.985116 val=0.913300 loss=2229.882738 time: 16.149350\n",
      "[Epoch 107] train=0.985096 val=0.913500 loss=2165.915789 time: 16.083117\n",
      "[Epoch 108] train=0.986739 val=0.912200 loss=2054.936226 time: 16.255057\n",
      "[Epoch 109] train=0.986138 val=0.915500 loss=2110.445376 time: 16.245631\n",
      "[Epoch 110] train=0.986218 val=0.912900 loss=2053.142495 time: 16.306826\n",
      "[Epoch 111] train=0.986358 val=0.913500 loss=1973.977391 time: 16.282403\n",
      "[Epoch 112] train=0.986318 val=0.913400 loss=1995.817816 time: 16.238734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 113] train=0.986859 val=0.911900 loss=1962.128677 time: 16.267088\n",
      "[Epoch 114] train=0.987320 val=0.910900 loss=1972.736976 time: 16.364694\n",
      "[Epoch 115] train=0.988041 val=0.911400 loss=1855.651257 time: 16.322829\n",
      "[Epoch 116] train=0.987961 val=0.912800 loss=1859.722674 time: 16.288213\n",
      "[Epoch 117] train=0.987720 val=0.912700 loss=1831.402850 time: 16.168266\n",
      "[Epoch 118] train=0.988782 val=0.910700 loss=1725.597312 time: 16.252010\n",
      "[Epoch 119] train=0.987660 val=0.910500 loss=1787.131518 time: 16.208196\n",
      "[Epoch 120] train=0.989163 val=0.909800 loss=1693.024771 time: 16.269358\n",
      "[Epoch 121] train=0.988862 val=0.913700 loss=1722.250781 time: 16.165674\n",
      "[Epoch 122] train=0.988562 val=0.912700 loss=1661.053533 time: 16.231755\n",
      "[Epoch 123] train=0.988702 val=0.913900 loss=1728.066147 time: 16.205270\n",
      "[Epoch 124] train=0.989944 val=0.913500 loss=1603.979865 time: 16.147305\n",
      "[Epoch 125] train=0.988642 val=0.909900 loss=1702.840081 time: 16.248926\n",
      "[Epoch 126] train=0.989083 val=0.914200 loss=1674.581398 time: 16.249616\n",
      "[Epoch 127] train=0.990505 val=0.912500 loss=1509.744376 time: 16.317034\n",
      "[Epoch 128] train=0.989563 val=0.913800 loss=1571.341926 time: 16.079605\n",
      "[Epoch 129] train=0.990224 val=0.912900 loss=1528.824329 time: 16.198195\n",
      "[Epoch 130] train=0.989643 val=0.913400 loss=1607.673607 time: 16.172400\n",
      "[Epoch 131] train=0.990224 val=0.911300 loss=1536.694806 time: 16.308323\n",
      "[Epoch 132] train=0.990044 val=0.908000 loss=1525.523172 time: 16.234430\n",
      "[Epoch 133] train=0.990124 val=0.911600 loss=1528.893610 time: 16.086447\n",
      "[Epoch 134] train=0.989804 val=0.910400 loss=1508.344474 time: 16.216544\n",
      "[Epoch 135] train=0.990445 val=0.912200 loss=1495.360797 time: 16.271034\n",
      "[Epoch 136] train=0.990144 val=0.908000 loss=1480.182336 time: 16.139900\n",
      "[Epoch 137] train=0.990665 val=0.912700 loss=1385.766345 time: 16.178417\n",
      "[Epoch 138] train=0.989764 val=0.909900 loss=1555.259137 time: 16.200030\n",
      "[Epoch 139] train=0.990385 val=0.911600 loss=1502.163880 time: 16.084497\n",
      "[Epoch 140] train=0.990445 val=0.912100 loss=1447.310315 time: 16.332185\n",
      "[Epoch 141] train=0.990845 val=0.907000 loss=1420.016295 time: 16.484046\n",
      "[Epoch 142] train=0.990365 val=0.909300 loss=1388.111205 time: 16.356600\n",
      "[Epoch 143] train=0.991226 val=0.909700 loss=1360.325065 time: 16.342402\n",
      "[Epoch 144] train=0.990685 val=0.909600 loss=1443.770917 time: 16.378132\n",
      "[Epoch 145] train=0.990605 val=0.908800 loss=1426.261108 time: 16.432058\n",
      "[Epoch 146] train=0.990425 val=0.909000 loss=1469.427802 time: 16.377557\n",
      "[Epoch 147] train=0.990204 val=0.910200 loss=1440.372730 time: 16.377431\n",
      "[Epoch 148] train=0.990986 val=0.906000 loss=1353.654400 time: 16.274039\n",
      "[Epoch 149] train=0.990825 val=0.908900 loss=1407.618210 time: 16.300581\n",
      "[Epoch 150] train=0.991466 val=0.907200 loss=1336.081889 time: 16.272829\n",
      "[Epoch 151] train=0.990986 val=0.909600 loss=1414.361863 time: 16.191399\n",
      "[Epoch 152] train=0.990605 val=0.909400 loss=1474.076171 time: 16.261618\n",
      "[Epoch 153] train=0.991266 val=0.908500 loss=1383.323649 time: 16.210257\n",
      "[Epoch 154] train=0.991506 val=0.909700 loss=1361.531654 time: 16.173144\n",
      "[Epoch 155] train=0.991266 val=0.909600 loss=1396.656095 time: 16.199517\n",
      "[Epoch 156] train=0.991607 val=0.909400 loss=1325.941580 time: 16.202446\n",
      "[Epoch 157] train=0.991466 val=0.909600 loss=1368.665112 time: 16.205000\n",
      "[Epoch 158] train=0.991326 val=0.907800 loss=1322.681688 time: 16.378780\n",
      "[Epoch 159] train=0.991827 val=0.905600 loss=1299.348951 time: 16.269289\n",
      "[Epoch 160] train=0.993870 val=0.911800 loss=1040.467860 time: 16.285651\n",
      "[Epoch 161] train=0.994010 val=0.911600 loss=938.426685 time: 16.303759\n",
      "[Epoch 162] train=0.995192 val=0.912800 loss=853.193622 time: 16.292596\n",
      "[Epoch 163] train=0.995893 val=0.911900 loss=801.692316 time: 16.384578\n",
      "[Epoch 164] train=0.995933 val=0.911500 loss=776.398200 time: 16.293632\n",
      "[Epoch 165] train=0.996154 val=0.912300 loss=765.236719 time: 16.231205\n",
      "[Epoch 166] train=0.995833 val=0.911900 loss=784.903074 time: 16.248861\n",
      "[Epoch 167] train=0.996234 val=0.911700 loss=740.262477 time: 16.374485\n",
      "[Epoch 168] train=0.996394 val=0.912900 loss=746.618496 time: 16.551345\n",
      "[Epoch 169] train=0.996374 val=0.912000 loss=700.345996 time: 16.503714\n",
      "[Epoch 170] train=0.996534 val=0.911600 loss=713.009048 time: 16.161796\n",
      "[Epoch 171] train=0.995994 val=0.913000 loss=731.730765 time: 16.107249\n",
      "[Epoch 172] train=0.996494 val=0.912800 loss=705.308777 time: 16.298242\n",
      "[Epoch 173] train=0.995753 val=0.912600 loss=739.763286 time: 16.370393\n",
      "[Epoch 174] train=0.996394 val=0.912100 loss=716.401535 time: 16.496278\n",
      "[Epoch 175] train=0.996575 val=0.912800 loss=705.096746 time: 16.349631\n",
      "[Epoch 176] train=0.996474 val=0.912500 loss=688.469406 time: 16.300267\n",
      "[Epoch 177] train=0.996955 val=0.912100 loss=643.147286 time: 16.279844\n",
      "[Epoch 178] train=0.996775 val=0.912400 loss=672.805428 time: 16.253256\n",
      "[Epoch 179] train=0.996534 val=0.912100 loss=667.148274 time: 16.723031\n",
      "[Epoch 180] train=0.996715 val=0.912800 loss=661.505454 time: 16.329886\n",
      "[Epoch 181] train=0.996855 val=0.913800 loss=634.436419 time: 16.302986\n",
      "[Epoch 182] train=0.996895 val=0.912300 loss=622.532089 time: 16.097457\n",
      "[Epoch 183] train=0.996554 val=0.912700 loss=653.225477 time: 16.076220\n",
      "[Epoch 184] train=0.997175 val=0.913900 loss=624.647528 time: 16.164424\n",
      "[Epoch 185] train=0.996595 val=0.913700 loss=643.511112 time: 16.279990\n",
      "[Epoch 186] train=0.997075 val=0.911600 loss=638.317012 time: 16.228216\n",
      "[Epoch 187] train=0.996955 val=0.913000 loss=611.540011 time: 16.277059\n",
      "[Epoch 188] train=0.996715 val=0.913000 loss=635.904086 time: 16.252075\n",
      "[Epoch 189] train=0.996995 val=0.913000 loss=617.144586 time: 16.265930\n",
      "[Epoch 190] train=0.997456 val=0.913800 loss=613.102280 time: 16.446212\n",
      "[Epoch 191] train=0.997216 val=0.913400 loss=612.799424 time: 16.333718\n",
      "[Epoch 192] train=0.997155 val=0.913400 loss=585.334661 time: 16.145848\n",
      "[Epoch 193] train=0.997095 val=0.913400 loss=587.906535 time: 16.243170\n",
      "[Epoch 194] train=0.996675 val=0.912900 loss=609.533987 time: 16.096908\n",
      "[Epoch 195] train=0.996895 val=0.912800 loss=610.438220 time: 16.123556\n",
      "[Epoch 196] train=0.997196 val=0.913700 loss=596.961060 time: 16.121237\n",
      "[Epoch 197] train=0.997396 val=0.912900 loss=563.886876 time: 16.232092\n",
      "[Epoch 198] train=0.997296 val=0.913600 loss=574.234416 time: 16.118469\n",
      "[Epoch 199] train=0.996995 val=0.912800 loss=616.113574 time: 16.369234\n",
      "[Epoch 200] train=0.997616 val=0.912400 loss=562.680688 time: 16.202940\n",
      "[Epoch 201] train=0.997516 val=0.914100 loss=578.011115 time: 16.178499\n",
      "[Epoch 202] train=0.996995 val=0.913400 loss=581.627704 time: 16.167331\n",
      "[Epoch 203] train=0.997196 val=0.912400 loss=560.882259 time: 16.147075\n",
      "[Epoch 204] train=0.997256 val=0.914200 loss=605.919201 time: 16.105014\n",
      "[Epoch 205] train=0.997917 val=0.914100 loss=507.995537 time: 16.297623\n",
      "[Epoch 206] train=0.997476 val=0.912900 loss=573.066490 time: 16.112079\n",
      "[Epoch 207] train=0.997095 val=0.912900 loss=594.642963 time: 16.263941\n",
      "[Epoch 208] train=0.997356 val=0.913400 loss=560.298169 time: 16.324072\n",
      "[Epoch 209] train=0.997376 val=0.914000 loss=567.548200 time: 16.233226\n",
      "[Epoch 210] train=0.997115 val=0.913000 loss=559.955070 time: 16.508281\n",
      "[Epoch 211] train=0.997316 val=0.914400 loss=568.373888 time: 16.402038\n",
      "[Epoch 212] train=0.997516 val=0.913200 loss=569.873366 time: 16.395316\n",
      "[Epoch 213] train=0.996935 val=0.912900 loss=600.154862 time: 16.390284\n",
      "[Epoch 214] train=0.996995 val=0.913400 loss=576.129070 time: 16.347672\n",
      "[Epoch 215] train=0.997736 val=0.913700 loss=556.332719 time: 16.500898\n",
      "[Epoch 216] train=0.997776 val=0.913200 loss=540.531425 time: 16.157015\n",
      "[Epoch 217] train=0.997837 val=0.913100 loss=533.863525 time: 16.039523\n",
      "[Epoch 218] train=0.997496 val=0.914500 loss=552.947520 time: 16.186763\n",
      "[Epoch 219] train=0.997516 val=0.914400 loss=542.007615 time: 16.210147\n",
      "[Epoch 220] train=0.997596 val=0.912900 loss=531.750109 time: 16.281165\n",
      "[Epoch 221] train=0.997576 val=0.913200 loss=547.133742 time: 16.264518\n",
      "[Epoch 222] train=0.997796 val=0.913900 loss=521.367417 time: 16.229914\n",
      "[Epoch 223] train=0.997496 val=0.914000 loss=562.460903 time: 16.105453\n",
      "[Epoch 224] train=0.997376 val=0.912200 loss=555.593061 time: 16.180421\n",
      "[Epoch 225] train=0.997496 val=0.913500 loss=529.881434 time: 16.202770\n",
      "[Epoch 226] train=0.997837 val=0.913600 loss=490.899666 time: 16.339191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 227] train=0.997696 val=0.912700 loss=520.242700 time: 16.164641\n",
      "[Epoch 228] train=0.997496 val=0.914700 loss=543.277616 time: 16.267015\n",
      "[Epoch 229] train=0.997957 val=0.914200 loss=491.469038 time: 16.302396\n",
      "[Epoch 230] train=0.997496 val=0.914700 loss=520.648330 time: 16.284395\n",
      "[Epoch 231] train=0.997736 val=0.915200 loss=521.236540 time: 16.257567\n",
      "[Epoch 232] train=0.997356 val=0.914900 loss=548.616859 time: 16.230944\n",
      "[Epoch 233] train=0.997877 val=0.913000 loss=544.982603 time: 16.474244\n",
      "[Epoch 234] train=0.997796 val=0.912900 loss=491.296718 time: 16.270092\n",
      "[Epoch 235] train=0.997897 val=0.911500 loss=510.922922 time: 16.351708\n",
      "[Epoch 236] train=0.997817 val=0.913900 loss=517.657984 time: 16.417597\n",
      "[Epoch 237] train=0.997877 val=0.914100 loss=508.055361 time: 16.347787\n",
      "[Epoch 238] train=0.997596 val=0.913800 loss=513.890281 time: 16.491621\n",
      "[Epoch 239] train=0.998017 val=0.913300 loss=498.409484 time: 16.341497\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "train_history.plot()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
