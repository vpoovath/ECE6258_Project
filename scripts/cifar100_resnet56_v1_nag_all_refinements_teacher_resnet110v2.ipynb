{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 128 # Batch Size for Each GPU\n",
    "num_workers = 2 # Number of data loader workers\n",
    "dtype = 'float32' # Default training data type if float32\n",
    "num_gpus = 1      # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = True\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "net = get_model('cifar_resnet56_v1', classes=100)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "net.cast(dtype)\n",
    "print(\"Model Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "Load the pre-trained CIFAR10 models and replace the final output layer with 100 classes instead of 10. This is demonstrated at this website: https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/packages/gluon/image/pretrained_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(None -> 100, linear)\n",
      "Teacher Model Init Done!\n"
     ]
    }
   ],
   "source": [
    "distillation = True\n",
    "T = 20\n",
    "hard_weight = 0.5\n",
    "# Teacher model for distillation training\n",
    "#teacher_name = 'cifar_wideresnet28_10'\n",
    "teacher_name = 'cifar_resnet110_v2'\n",
    "# teacher_name = 'cifar_wideresnet28_10'\n",
    "# teacher_name = 'cifar_wideresnet40_8'\n",
    "teacher = get_model(teacher_name, pretrained=True, ctx=ctx)\n",
    "teacher.collect_params().initialize(ctx=ctx, force_reinit=True)\n",
    "teacher.cast(dtype)\n",
    "\n",
    "with teacher.name_scope():\n",
    "    teacher.output = gluon.nn.Dense(num_classes)\n",
    "    teacher.output.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "print(teacher.output)\n",
    "print(\"Teacher Model Init Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "        # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n"
     ]
    }
   ],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n",
      "Training Settings Set Successfully.\n"
     ]
    }
   ],
   "source": [
    "# epochs = 120\n",
    "epochs = 200 # Mixup asks for longer training to converge better\n",
    "warmup_epochs = 10\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler('linear',\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler('cosine',\n",
    "                base_lr = 0.1,\n",
    "                target_lr = 0,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epoch,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistillationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                           hard_weight=hard_weight,\n",
    "                                                           sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "print(\"Training Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started:\n",
      "[Epoch 0] train=1.000000 val_top1=0.104600 val_top5=0.398000 loss=46082944.859375 time: 58.285605\n",
      "[Epoch 1] train=1.000000 val_top1=0.172000 val_top5=0.525300 loss=46071959.437500 time: 56.817250\n",
      "[Epoch 2] train=1.000000 val_top1=0.256500 val_top5=0.644000 loss=46067837.453125 time: 56.433728\n",
      "[Epoch 3] train=1.000000 val_top1=0.305800 val_top5=0.698700 loss=46064250.765625 time: 55.840825\n",
      "[Epoch 4] train=1.000000 val_top1=0.336200 val_top5=0.722500 loss=46060673.585938 time: 55.602539\n",
      "[Epoch 5] train=1.000000 val_top1=0.376200 val_top5=0.753900 loss=46057972.929688 time: 57.089852\n",
      "[Epoch 6] train=1.000000 val_top1=0.450900 val_top5=0.821200 loss=46054848.710938 time: 56.507239\n",
      "[Epoch 7] train=1.000000 val_top1=0.459800 val_top5=0.827800 loss=46052723.140625 time: 57.239414\n",
      "[Epoch 8] train=1.000000 val_top1=0.496700 val_top5=0.848300 loss=46049993.859375 time: 57.927612\n",
      "[Epoch 9] train=1.000000 val_top1=0.483200 val_top5=0.835800 loss=46049375.804688 time: 57.485024\n",
      "[Epoch 10] train=1.000000 val_top1=0.498700 val_top5=0.858200 loss=46047318.398438 time: 57.801224\n",
      "[Epoch 11] train=1.000000 val_top1=0.503500 val_top5=0.829600 loss=46045047.640625 time: 57.481338\n",
      "[Epoch 12] train=1.000000 val_top1=0.587700 val_top5=0.895700 loss=46045398.726562 time: 57.950269\n",
      "[Epoch 13] train=1.000000 val_top1=0.572600 val_top5=0.886500 loss=46043351.468750 time: 59.258366\n",
      "[Epoch 14] train=1.000000 val_top1=0.583600 val_top5=0.892200 loss=46042496.757812 time: 56.884080\n",
      "[Epoch 15] train=1.000000 val_top1=0.589400 val_top5=0.892400 loss=46041797.562500 time: 58.369841\n",
      "[Epoch 16] train=1.000000 val_top1=0.615500 val_top5=0.907500 loss=46040533.320312 time: 56.953508\n",
      "[Epoch 17] train=1.000000 val_top1=0.597400 val_top5=0.904400 loss=46040548.921875 time: 56.995831\n",
      "[Epoch 18] train=1.000000 val_top1=0.627400 val_top5=0.916300 loss=46039394.609375 time: 55.705610\n",
      "[Epoch 19] train=1.000000 val_top1=0.643900 val_top5=0.918500 loss=46039624.796875 time: 56.552298\n",
      "[Epoch 20] train=1.000000 val_top1=0.605400 val_top5=0.897800 loss=46039037.140625 time: 56.042441\n",
      "[Epoch 21] train=1.000000 val_top1=0.610700 val_top5=0.899400 loss=46039019.914062 time: 56.728081\n",
      "[Epoch 22] train=1.000000 val_top1=0.607600 val_top5=0.911600 loss=46038314.039062 time: 55.784868\n",
      "[Epoch 23] train=1.000000 val_top1=0.627100 val_top5=0.907700 loss=46036918.687500 time: 55.986581\n",
      "[Epoch 24] train=1.000000 val_top1=0.669600 val_top5=0.929400 loss=46036289.390625 time: 56.283541\n",
      "[Epoch 25] train=1.000000 val_top1=0.654900 val_top5=0.921500 loss=46036839.000000 time: 55.597055\n",
      "[Epoch 26] train=1.000000 val_top1=0.656700 val_top5=0.918500 loss=46036157.453125 time: 56.486969\n",
      "[Epoch 27] train=1.000000 val_top1=0.677100 val_top5=0.934500 loss=46034811.468750 time: 55.429002\n",
      "[Epoch 28] train=1.000000 val_top1=0.667100 val_top5=0.924300 loss=46035811.882812 time: 56.968077\n",
      "[Epoch 29] train=1.000000 val_top1=0.665400 val_top5=0.918900 loss=46034961.828125 time: 57.780487\n",
      "[Epoch 30] train=1.000000 val_top1=0.671000 val_top5=0.926900 loss=46035946.718750 time: 57.566835\n",
      "[Epoch 31] train=1.000000 val_top1=0.667200 val_top5=0.925700 loss=46035299.062500 time: 57.588453\n",
      "[Epoch 32] train=1.000000 val_top1=0.676500 val_top5=0.928800 loss=46034893.125000 time: 57.526914\n",
      "[Epoch 33] train=1.000000 val_top1=0.682400 val_top5=0.929600 loss=46034883.289062 time: 57.969398\n",
      "[Epoch 34] train=1.000000 val_top1=0.689800 val_top5=0.937100 loss=46034502.593750 time: 58.028217\n",
      "[Epoch 35] train=1.000000 val_top1=0.639800 val_top5=0.910900 loss=46033936.648438 time: 58.313949\n",
      "[Epoch 36] train=1.000000 val_top1=0.652200 val_top5=0.919000 loss=46033010.031250 time: 57.564298\n",
      "[Epoch 37] train=1.000000 val_top1=0.684100 val_top5=0.933400 loss=46034480.750000 time: 58.065912\n",
      "[Epoch 38] train=1.000000 val_top1=0.669300 val_top5=0.923200 loss=46034271.734375 time: 57.640795\n",
      "[Epoch 39] train=1.000000 val_top1=0.681700 val_top5=0.931700 loss=46032996.523438 time: 57.129656\n",
      "[Epoch 40] train=1.000000 val_top1=0.684000 val_top5=0.928600 loss=46034424.070312 time: 56.409740\n",
      "[Epoch 41] train=1.000000 val_top1=0.649000 val_top5=0.918700 loss=46033398.460938 time: 55.886757\n",
      "[Epoch 42] train=1.000000 val_top1=0.665800 val_top5=0.925800 loss=46033555.218750 time: 55.492010\n",
      "[Epoch 43] train=1.000000 val_top1=0.684600 val_top5=0.932300 loss=46033496.312500 time: 55.570511\n",
      "[Epoch 44] train=1.000000 val_top1=0.701300 val_top5=0.936100 loss=46033127.585938 time: 56.968038\n",
      "[Epoch 45] train=1.000000 val_top1=0.679900 val_top5=0.928800 loss=46032888.406250 time: 56.033205\n",
      "[Epoch 46] train=1.000000 val_top1=0.672300 val_top5=0.923100 loss=46032612.312500 time: 56.100686\n",
      "[Epoch 47] train=1.000000 val_top1=0.675800 val_top5=0.921100 loss=46031871.421875 time: 55.858379\n",
      "[Epoch 48] train=1.000000 val_top1=0.697800 val_top5=0.939200 loss=46032353.867188 time: 55.913509\n",
      "[Epoch 49] train=1.000000 val_top1=0.696100 val_top5=0.928800 loss=46032227.039062 time: 56.548575\n",
      "[Epoch 50] train=1.000000 val_top1=0.693200 val_top5=0.938400 loss=46032829.382812 time: 56.544713\n",
      "[Epoch 51] train=1.000000 val_top1=0.672000 val_top5=0.923900 loss=46032105.898438 time: 56.313571\n",
      "[Epoch 52] train=1.000000 val_top1=0.706600 val_top5=0.939200 loss=46032400.054688 time: 56.694284\n",
      "[Epoch 53] train=1.000000 val_top1=0.690100 val_top5=0.937000 loss=46032400.164062 time: 57.508451\n",
      "[Epoch 54] train=1.000000 val_top1=0.683300 val_top5=0.935700 loss=46031766.898438 time: 57.251366\n",
      "[Epoch 55] train=1.000000 val_top1=0.700100 val_top5=0.936400 loss=46031298.601562 time: 56.914629\n",
      "[Epoch 56] train=1.000000 val_top1=0.675200 val_top5=0.923700 loss=46030329.671875 time: 56.955461\n",
      "[Epoch 57] train=1.000000 val_top1=0.695700 val_top5=0.935500 loss=46032949.046875 time: 56.779740\n",
      "[Epoch 58] train=1.000000 val_top1=0.692200 val_top5=0.936900 loss=46031375.835938 time: 56.827079\n",
      "[Epoch 59] train=1.000000 val_top1=0.702700 val_top5=0.934000 loss=46031018.664062 time: 56.958711\n",
      "[Epoch 60] train=1.000000 val_top1=0.707000 val_top5=0.939100 loss=46029682.023438 time: 56.608015\n",
      "[Epoch 61] train=1.000000 val_top1=0.702800 val_top5=0.935000 loss=46029627.367188 time: 56.358055\n",
      "[Epoch 62] train=1.000000 val_top1=0.723200 val_top5=0.939700 loss=46031403.312500 time: 56.484314\n",
      "[Epoch 63] train=1.000000 val_top1=0.709000 val_top5=0.937200 loss=46030321.320312 time: 55.870185\n",
      "[Epoch 64] train=1.000000 val_top1=0.696900 val_top5=0.939100 loss=46030489.578125 time: 56.164792\n",
      "[Epoch 65] train=1.000000 val_top1=0.698500 val_top5=0.935000 loss=46030565.664062 time: 56.346139\n",
      "[Epoch 66] train=1.000000 val_top1=0.712400 val_top5=0.941900 loss=46030847.906250 time: 55.652490\n",
      "[Epoch 67] train=1.000000 val_top1=0.709000 val_top5=0.940700 loss=46030358.281250 time: 56.400167\n",
      "[Epoch 68] train=1.000000 val_top1=0.693300 val_top5=0.937000 loss=46030630.679688 time: 56.543622\n",
      "[Epoch 69] train=1.000000 val_top1=0.708400 val_top5=0.936000 loss=46030020.609375 time: 57.304047\n",
      "[Epoch 70] train=1.000000 val_top1=0.699600 val_top5=0.937700 loss=46029050.429688 time: 57.147251\n",
      "[Epoch 71] train=1.000000 val_top1=0.707600 val_top5=0.939200 loss=46029375.148438 time: 57.021306\n",
      "[Epoch 72] train=1.000000 val_top1=0.694100 val_top5=0.936500 loss=46030649.976562 time: 57.307328\n",
      "[Epoch 73] train=1.000000 val_top1=0.726700 val_top5=0.945400 loss=46029692.117188 time: 56.979271\n",
      "[Epoch 74] train=1.000000 val_top1=0.695000 val_top5=0.935300 loss=46028938.414062 time: 57.184199\n",
      "[Epoch 75] train=1.000000 val_top1=0.717500 val_top5=0.943800 loss=46029348.101562 time: 56.713828\n",
      "[Epoch 76] train=1.000000 val_top1=0.730200 val_top5=0.945600 loss=46031491.679688 time: 56.378514\n",
      "[Epoch 77] train=1.000000 val_top1=0.705700 val_top5=0.936900 loss=46028820.765625 time: 55.811031\n",
      "[Epoch 78] train=1.000000 val_top1=0.724900 val_top5=0.942900 loss=46028664.273438 time: 57.225763\n",
      "[Epoch 79] train=1.000000 val_top1=0.712800 val_top5=0.940300 loss=46029947.093750 time: 57.921381\n",
      "[Epoch 80] train=1.000000 val_top1=0.725900 val_top5=0.944100 loss=46029800.062500 time: 57.860157\n",
      "[Epoch 81] train=1.000000 val_top1=0.723400 val_top5=0.941700 loss=46027763.859375 time: 58.244741\n",
      "[Epoch 82] train=1.000000 val_top1=0.726700 val_top5=0.946900 loss=46028679.007812 time: 57.697038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 83] train=1.000000 val_top1=0.714000 val_top5=0.941100 loss=46028886.882812 time: 58.235751\n",
      "[Epoch 84] train=1.000000 val_top1=0.705600 val_top5=0.936000 loss=46028242.000000 time: 58.939791\n",
      "[Epoch 85] train=1.000000 val_top1=0.730800 val_top5=0.944500 loss=46029417.734375 time: 57.921637\n",
      "[Epoch 86] train=1.000000 val_top1=0.731200 val_top5=0.945700 loss=46028030.554688 time: 57.057632\n",
      "[Epoch 87] train=1.000000 val_top1=0.715500 val_top5=0.946100 loss=46027443.367188 time: 56.115501\n",
      "[Epoch 88] train=1.000000 val_top1=0.707600 val_top5=0.937800 loss=46027770.390625 time: 55.970149\n",
      "[Epoch 89] train=1.000000 val_top1=0.737100 val_top5=0.946100 loss=46028639.937500 time: 56.429415\n",
      "[Epoch 90] train=1.000000 val_top1=0.741100 val_top5=0.947700 loss=46029019.531250 time: 56.362247\n",
      "[Epoch 91] train=1.000000 val_top1=0.733500 val_top5=0.943900 loss=46027319.664062 time: 56.461018\n",
      "[Epoch 92] train=1.000000 val_top1=0.721800 val_top5=0.942500 loss=46028372.625000 time: 55.837984\n",
      "[Epoch 93] train=1.000000 val_top1=0.740600 val_top5=0.947500 loss=46028011.828125 time: 56.575977\n",
      "[Epoch 94] train=1.000000 val_top1=0.727100 val_top5=0.944000 loss=46027780.515625 time: 56.259662\n",
      "[Epoch 95] train=1.000000 val_top1=0.732200 val_top5=0.946300 loss=46027510.351562 time: 56.057259\n",
      "[Epoch 96] train=1.000000 val_top1=0.735100 val_top5=0.945800 loss=46027205.671875 time: 56.314550\n",
      "[Epoch 97] train=1.000000 val_top1=0.726100 val_top5=0.946600 loss=46027797.781250 time: 56.420322\n",
      "[Epoch 98] train=1.000000 val_top1=0.737600 val_top5=0.946000 loss=46026598.953125 time: 56.532147\n",
      "[Epoch 99] train=1.000000 val_top1=0.728100 val_top5=0.943700 loss=46028527.359375 time: 56.503104\n",
      "[Epoch 100] train=1.000000 val_top1=0.745000 val_top5=0.948900 loss=46027545.226562 time: 57.121156\n",
      "[Epoch 101] train=1.000000 val_top1=0.745600 val_top5=0.951900 loss=46027823.054688 time: 56.101325\n",
      "[Epoch 102] train=1.000000 val_top1=0.744900 val_top5=0.953000 loss=46027313.578125 time: 56.214350\n",
      "[Epoch 103] train=1.000000 val_top1=0.735600 val_top5=0.945900 loss=46026329.523438 time: 55.615534\n",
      "[Epoch 104] train=1.000000 val_top1=0.735900 val_top5=0.946800 loss=46027281.382812 time: 55.820824\n",
      "[Epoch 105] train=1.000000 val_top1=0.738600 val_top5=0.948100 loss=46026634.164062 time: 55.697067\n",
      "[Epoch 106] train=1.000000 val_top1=0.738000 val_top5=0.945200 loss=46027413.312500 time: 56.686682\n",
      "[Epoch 107] train=1.000000 val_top1=0.742900 val_top5=0.946500 loss=46025207.953125 time: 56.153681\n",
      "[Epoch 108] train=1.000000 val_top1=0.743100 val_top5=0.948000 loss=46026667.703125 time: 56.147842\n",
      "[Epoch 109] train=1.000000 val_top1=0.723500 val_top5=0.938500 loss=46027636.546875 time: 55.796891\n",
      "[Epoch 110] train=1.000000 val_top1=0.729200 val_top5=0.941000 loss=46025306.335938 time: 56.447251\n",
      "[Epoch 111] train=1.000000 val_top1=0.737400 val_top5=0.945700 loss=46026510.757812 time: 56.470304\n",
      "[Epoch 112] train=1.000000 val_top1=0.752300 val_top5=0.951700 loss=46026795.273438 time: 56.542035\n",
      "[Epoch 113] train=1.000000 val_top1=0.742700 val_top5=0.948500 loss=46025875.226562 time: 56.148662\n",
      "[Epoch 114] train=1.000000 val_top1=0.748600 val_top5=0.949500 loss=46024513.367188 time: 56.649859\n",
      "[Epoch 115] train=1.000000 val_top1=0.736400 val_top5=0.947200 loss=46027613.210938 time: 56.783701\n",
      "[Epoch 116] train=1.000000 val_top1=0.741500 val_top5=0.949100 loss=46026236.000000 time: 56.426527\n",
      "[Epoch 117] train=1.000000 val_top1=0.756400 val_top5=0.954600 loss=46024838.562500 time: 55.448688\n",
      "[Epoch 118] train=1.000000 val_top1=0.731400 val_top5=0.946700 loss=46024324.734375 time: 56.177514\n",
      "[Epoch 119] train=1.000000 val_top1=0.748800 val_top5=0.942800 loss=46025312.562500 time: 56.469305\n",
      "[Epoch 120] train=1.000000 val_top1=0.751000 val_top5=0.945300 loss=46024414.953125 time: 56.123001\n",
      "[Epoch 121] train=1.000000 val_top1=0.752600 val_top5=0.952900 loss=46025450.742188 time: 56.162537\n",
      "[Epoch 122] train=1.000000 val_top1=0.744100 val_top5=0.949400 loss=46025526.593750 time: 55.981962\n",
      "[Epoch 123] train=1.000000 val_top1=0.746600 val_top5=0.947800 loss=46025413.156250 time: 56.367744\n",
      "[Epoch 124] train=1.000000 val_top1=0.755400 val_top5=0.948600 loss=46024376.648438 time: 56.165816\n",
      "[Epoch 125] train=1.000000 val_top1=0.756000 val_top5=0.951800 loss=46024160.531250 time: 55.968124\n",
      "[Epoch 126] train=1.000000 val_top1=0.772900 val_top5=0.954900 loss=46024220.039062 time: 56.021730\n",
      "[Epoch 127] train=1.000000 val_top1=0.762000 val_top5=0.946100 loss=46024097.757812 time: 56.692937\n",
      "[Epoch 128] train=1.000000 val_top1=0.760700 val_top5=0.947000 loss=46025376.171875 time: 56.501720\n",
      "[Epoch 129] train=1.000000 val_top1=0.760200 val_top5=0.950700 loss=46024534.117188 time: 56.441293\n",
      "[Epoch 130] train=1.000000 val_top1=0.762600 val_top5=0.952900 loss=46023926.335938 time: 56.261219\n",
      "[Epoch 131] train=1.000000 val_top1=0.759100 val_top5=0.949600 loss=46024512.726562 time: 55.739736\n",
      "[Epoch 132] train=1.000000 val_top1=0.758500 val_top5=0.952700 loss=46023209.109375 time: 55.588304\n",
      "[Epoch 133] train=1.000000 val_top1=0.753100 val_top5=0.947200 loss=46022254.273438 time: 56.565198\n",
      "[Epoch 134] train=1.000000 val_top1=0.756800 val_top5=0.948000 loss=46023726.617188 time: 56.014259\n",
      "[Epoch 135] train=1.000000 val_top1=0.765700 val_top5=0.955100 loss=46021484.312500 time: 56.283720\n",
      "[Epoch 136] train=1.000000 val_top1=0.765500 val_top5=0.952700 loss=46023359.820312 time: 56.421469\n",
      "[Epoch 137] train=1.000000 val_top1=0.764100 val_top5=0.955000 loss=46021618.671875 time: 55.828407\n",
      "[Epoch 138] train=1.000000 val_top1=0.751300 val_top5=0.950100 loss=46023294.570312 time: 56.808623\n",
      "[Epoch 139] train=1.000000 val_top1=0.771400 val_top5=0.955800 loss=46023121.421875 time: 56.506590\n",
      "[Epoch 140] train=1.000000 val_top1=0.771400 val_top5=0.956000 loss=46024181.820312 time: 56.607303\n",
      "[Epoch 141] train=1.000000 val_top1=0.771800 val_top5=0.955700 loss=46022339.664062 time: 56.207216\n",
      "[Epoch 142] train=1.000000 val_top1=0.773500 val_top5=0.951200 loss=46021521.093750 time: 56.294322\n",
      "[Epoch 143] train=1.000000 val_top1=0.753800 val_top5=0.949400 loss=46022013.742188 time: 56.746912\n",
      "[Epoch 144] train=1.000000 val_top1=0.768500 val_top5=0.950400 loss=46021806.062500 time: 55.991296\n",
      "[Epoch 145] train=1.000000 val_top1=0.771000 val_top5=0.952700 loss=46022213.039062 time: 56.283922\n",
      "[Epoch 146] train=1.000000 val_top1=0.771200 val_top5=0.951300 loss=46022164.195312 time: 56.189984\n",
      "[Epoch 147] train=1.000000 val_top1=0.776800 val_top5=0.954800 loss=46022306.007812 time: 57.726510\n",
      "[Epoch 148] train=1.000000 val_top1=0.773200 val_top5=0.952600 loss=46021394.359375 time: 56.135884\n",
      "[Epoch 149] train=1.000000 val_top1=0.779700 val_top5=0.953300 loss=46022567.218750 time: 55.967092\n",
      "[Epoch 150] train=1.000000 val_top1=0.768700 val_top5=0.951200 loss=46020789.734375 time: 56.130442\n",
      "[Epoch 151] train=1.000000 val_top1=0.776700 val_top5=0.952800 loss=46021313.242188 time: 55.761701\n",
      "[Epoch 152] train=1.000000 val_top1=0.785100 val_top5=0.955800 loss=46021042.632812 time: 55.974854\n",
      "[Epoch 153] train=1.000000 val_top1=0.770500 val_top5=0.951000 loss=46019177.515625 time: 55.779426\n",
      "[Epoch 154] train=1.000000 val_top1=0.780900 val_top5=0.953500 loss=46021205.187500 time: 56.782180\n",
      "[Epoch 155] train=1.000000 val_top1=0.773900 val_top5=0.954600 loss=46019918.625000 time: 55.983842\n",
      "[Epoch 156] train=1.000000 val_top1=0.774400 val_top5=0.951100 loss=46021326.046875 time: 56.185675\n",
      "[Epoch 157] train=1.000000 val_top1=0.780100 val_top5=0.954600 loss=46021092.578125 time: 56.680547\n",
      "[Epoch 158] train=1.000000 val_top1=0.777400 val_top5=0.952000 loss=46019942.750000 time: 55.606858\n",
      "[Epoch 159] train=1.000000 val_top1=0.783300 val_top5=0.956000 loss=46019749.726562 time: 56.731300\n",
      "[Epoch 160] train=1.000000 val_top1=0.783300 val_top5=0.957400 loss=46019363.046875 time: 56.683726\n",
      "[Epoch 161] train=1.000000 val_top1=0.782600 val_top5=0.952500 loss=46019463.101562 time: 56.664905\n",
      "[Epoch 162] train=1.000000 val_top1=0.787500 val_top5=0.954300 loss=46020723.359375 time: 56.469231\n",
      "[Epoch 163] train=1.000000 val_top1=0.791700 val_top5=0.954500 loss=46018158.164062 time: 56.703745\n",
      "[Epoch 164] train=1.000000 val_top1=0.789900 val_top5=0.952700 loss=46018440.000000 time: 56.353384\n",
      "[Epoch 165] train=1.000000 val_top1=0.788200 val_top5=0.954200 loss=46019290.554688 time: 55.873312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 166] train=1.000000 val_top1=0.791600 val_top5=0.955500 loss=46019797.906250 time: 55.915579\n",
      "[Epoch 167] train=1.000000 val_top1=0.791600 val_top5=0.954600 loss=46018045.906250 time: 56.183091\n",
      "[Epoch 168] train=1.000000 val_top1=0.791300 val_top5=0.951700 loss=46019729.398438 time: 56.612638\n",
      "[Epoch 169] train=1.000000 val_top1=0.791800 val_top5=0.955800 loss=46019874.429688 time: 56.208182\n",
      "[Epoch 170] train=1.000000 val_top1=0.793800 val_top5=0.957300 loss=46019761.578125 time: 56.506516\n",
      "[Epoch 171] train=1.000000 val_top1=0.789700 val_top5=0.954700 loss=46018686.875000 time: 55.830763\n",
      "[Epoch 172] train=1.000000 val_top1=0.792500 val_top5=0.958200 loss=46018737.968750 time: 57.620954\n",
      "[Epoch 173] train=1.000000 val_top1=0.794300 val_top5=0.958700 loss=46019061.187500 time: 55.449728\n",
      "[Epoch 174] train=1.000000 val_top1=0.790300 val_top5=0.957200 loss=46017535.578125 time: 56.136874\n",
      "[Epoch 175] train=1.000000 val_top1=0.791800 val_top5=0.955800 loss=46018985.132812 time: 56.546250\n",
      "[Epoch 176] train=1.000000 val_top1=0.794700 val_top5=0.955500 loss=46016591.250000 time: 55.733364\n",
      "[Epoch 177] train=1.000000 val_top1=0.794500 val_top5=0.956500 loss=46018762.601562 time: 56.298450\n",
      "[Epoch 178] train=1.000000 val_top1=0.796200 val_top5=0.959500 loss=46019253.171875 time: 56.356905\n",
      "[Epoch 179] train=1.000000 val_top1=0.793000 val_top5=0.955700 loss=46017582.906250 time: 56.390156\n",
      "[Epoch 180] train=1.000000 val_top1=0.798600 val_top5=0.957200 loss=46018751.578125 time: 56.365007\n",
      "[Epoch 181] train=1.000000 val_top1=0.793800 val_top5=0.957400 loss=46019671.960938 time: 56.150985\n",
      "[Epoch 182] train=1.000000 val_top1=0.794100 val_top5=0.956000 loss=46017118.921875 time: 57.891326\n",
      "[Epoch 183] train=1.000000 val_top1=0.797000 val_top5=0.955200 loss=46016804.992188 time: 56.413416\n",
      "[Epoch 184] train=1.000000 val_top1=0.797200 val_top5=0.956000 loss=46018138.734375 time: 56.159120\n",
      "[Epoch 185] train=1.000000 val_top1=0.799100 val_top5=0.954400 loss=46017927.578125 time: 55.923319\n",
      "[Epoch 186] train=1.000000 val_top1=0.795900 val_top5=0.956000 loss=46017562.101562 time: 56.225650\n",
      "[Epoch 187] train=1.000000 val_top1=0.796900 val_top5=0.957200 loss=46017607.710938 time: 56.864451\n",
      "[Epoch 188] train=1.000000 val_top1=0.793100 val_top5=0.957400 loss=46018286.515625 time: 55.905194\n",
      "[Epoch 189] train=1.000000 val_top1=0.796400 val_top5=0.957700 loss=46018213.867188 time: 58.261177\n",
      "[Epoch 190] train=1.000000 val_top1=0.792900 val_top5=0.958000 loss=46018548.296875 time: 55.553876\n",
      "[Epoch 191] train=1.000000 val_top1=0.798400 val_top5=0.956700 loss=46017990.476562 time: 56.707412\n",
      "[Epoch 192] train=1.000000 val_top1=0.797700 val_top5=0.957000 loss=46018471.757812 time: 56.306646\n",
      "[Epoch 193] train=1.000000 val_top1=0.793000 val_top5=0.957600 loss=46017243.609375 time: 56.871889\n",
      "[Epoch 194] train=1.000000 val_top1=0.797400 val_top5=0.957500 loss=46016076.289062 time: 56.499064\n",
      "[Epoch 195] train=1.000000 val_top1=0.795500 val_top5=0.957600 loss=46016565.890625 time: 56.160234\n",
      "[Epoch 196] train=1.000000 val_top1=0.796800 val_top5=0.955200 loss=46017673.140625 time: 56.357360\n",
      "[Epoch 197] train=1.000000 val_top1=0.796200 val_top5=0.957000 loss=46018355.234375 time: 56.717563\n",
      "[Epoch 198] train=1.000000 val_top1=0.795100 val_top5=0.955100 loss=46017573.781250 time: 56.482913\n",
      "[Epoch 199] train=1.000000 val_top1=0.797500 val_top5=0.957200 loss=46017818.757812 time: 56.782155\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started:\")\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            if epoch >= epochs - mixup_off_epochs:\n",
    "                lam = 1\n",
    "            data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "            if label_smoothing:\n",
    "                eta = 0.1\n",
    "            else:\n",
    "                eta = 0.0\n",
    "\n",
    "            label = mixup_transform(label, num_classes, lam, eta)\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype('float32', copy=False),\n",
    "                                y.astype('float32', copy=False),\n",
    "                                p.astype('float32', copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                       label, \n",
    "                                                                                       teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                    save_path=\"./cifar100_resnet56_v1_nag_errors_{}.png\".format(timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                     save_path=\"./cifar100_resnet56_v1_nag_accuracies_{}.png\".format(timestamp))\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
