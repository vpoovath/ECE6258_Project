{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math, os, sys\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from skimage import util as sk_util\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 128 # Batch Size for Each GPU\n",
    "num_workers = 2             # Number of data loader workers\n",
    "dtype = 'float32'           # Default training data type if float32\n",
    "num_gpus = 1                # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using label smoothing: True\n"
     ]
    }
   ],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mixup: False\n"
     ]
    }
   ],
   "source": [
    "mixup = False\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using mixup: {}\".format(mixup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using spatial mixup: True\n",
      "Using vertical spatial mixup.\n"
     ]
    }
   ],
   "source": [
    "stitch_mixup = True\n",
    "stitch_type = 'vertical'\n",
    "def stitch_mixup_transform(X, lam, stitch_type='horizontal'):\n",
    "    h1 = 32\n",
    "    w1 = 32\n",
    "    if lam < 1:\n",
    "        Y = nd.zeros_like(X)\n",
    "        if stitch_type == 'horizontal':\n",
    "            Y[:,:,0:int(h1*lam),:] = X[:,:,0:int(h1*lam),:]\n",
    "            Y[:,:,int(h1*lam):h1,:] = X[::-1,:,int(h1*lam):h1,:]\n",
    "        elif stitch_type == 'vertical':\n",
    "            Y[:,:,:,0:int(h1*lam)] = X[:,:,:,0:int(h1*lam)]\n",
    "            Y[:,:,:,int(h1*lam):h1] = X[::-1,:,:,int(h1*lam):h1]\n",
    "        else:\n",
    "            raise(\"Invalid stitch type passed in!\")\n",
    "            return\n",
    "        return Y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def stitch_mixup_label_transform(label, num_classes, lam=1, eta=0.0, stitch_type='horizontal'):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using spatial mixup: {}\".format(stitch_mixup))\n",
    "if stitch_mixup:\n",
    "    print(\"Using {} spatial mixup.\".format(stitch_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_img_noise(img_tensor, noise_type='gaussian'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    reshaped_img = nd.reshape(img_tensor, (32, 32, 3))\n",
    "    noise_img_numpy = sk_util.random_noise(reshaped_img.asnumpy(), \n",
    "                                           mode=noise_type, \n",
    "                                           seed=None, \n",
    "                                           clip=True)\n",
    "    noise_img = nd.reshape(nd.array(noise_img_numpy), (3, 32, 32))\n",
    "    return noise_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# last_gamma = False\n",
    "# print(\"Using last gamma: {}\".format(last_gamma))\n",
    "# kwargs = {'ctx':ctx, 'classes':num_classes, 'last_gamma':last_gamma}\n",
    "\n",
    "kwargs = {'ctx':ctx, 'classes':num_classes}\n",
    "\n",
    "use_group_norm = False\n",
    "if use_group_norm:\n",
    "    kwargs['norm_layer'] = gcv.nn.GroupNorm\n",
    "    print(\"Using Group Normalization: {}\".format(use_group_norm))\n",
    "\n",
    "default_init = True\n",
    "net = get_model('cifar_resnet56_v1', **kwargs)\n",
    "\n",
    "if default_init:\n",
    "    net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "else:\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    print(\"Using MSRA Prelu Init.\")\n",
    "\n",
    "net.cast(dtype)\n",
    "print(\"\\nModel Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "Note that the reference paper uses a teacher model that was trained using cosine learning rate decay and label smoothing. \n",
    "\n",
    "Therefore I have created a separate notebook that trains the ResNext29_16x64d model using cosine learning rate decay and label smoothing on the CIFAR100 dataset. Training this ResNext29 architecture took nearly 10 hours...\n",
    "\n",
    "What is left to do is now just to load the model architecture again, and load the parameters from the saved file.\n",
    "\n",
    "### The old approach, when not using my own trained model is:\n",
    "Load the pre-trained CIFAR10 models and replace the final output layer with 100 classes instead of 10. This is demonstrated at this website: https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/packages/gluon/image/pretrained_models.html\n",
    "\n",
    "Need to investigate WideResNet issue of having Top1-Val Accuracies becoming 0.00000 at the very beginning of training. \n",
    "\n",
    "Additionally, need to understand the two ResNeXt architectures to understand why training time is much longer...:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher Model Init Done!\n"
     ]
    }
   ],
   "source": [
    "distillation = True\n",
    "\n",
    "if distillation:\n",
    "    curr_dir = os.getcwd()\n",
    "    param_file = os.path.join(curr_dir, \"resnext29_teacher_mixupFalse.params\") \n",
    "    T = 20\n",
    "    hard_weight = 0.5\n",
    "    # Teacher model for distillation training\n",
    "    # teacher_name = 'cifar_resnet110_v2'\n",
    "    # teacher_name = 'cifar_resnet56_v2'\n",
    "    # teacher_name = 'cifar_wideresnet28_10' # Top1-Val is 0...\n",
    "    # teacher_name = 'cifar_wideresnet40_8'  # Might cause the same problem\n",
    "    # teacher_name = 'cifar_resnext29_32x4d' # This is apparently not available...?\n",
    "    \n",
    "#     teacher = get_model(teacher_name, pretrained=True, ctx=ctx)\n",
    "#     teacher.collect_params().initialize(ctx=ctx, force_reinit=True) # Don't do this.\n",
    "#     teacher.cast(dtype)\n",
    "#     with teacher.name_scope():\n",
    "#         teacher.output = gluon.nn.Dense(num_classes)\n",
    "#         teacher.output.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    teacher_name = 'cifar_resnext29_16x64d'\n",
    "    teacher = get_model(teacher_name, classes=num_classes, ctx=ctx)\n",
    "    teacher.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    teacher.load_parameters(param_file)\n",
    "    teacher.cast(dtype)\n",
    "    print(\"\\nTeacher Model Init Done!\")\n",
    "else:\n",
    "    print(\"\\nNot using distillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n",
      "Per Device Batch Size: 128\n"
     ]
    }
   ],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")\n",
    "print(\"Per Device Batch Size: {}\".format(per_device_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n",
      "\n",
      "Using label smoothing: True\n",
      "\n",
      "Using mixup: False\n",
      "Number of no-mixup epochs:0\n",
      "\n",
      "Using nag Optimizer\n",
      "{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7ff098113c90>, 'wd': 0.0001, 'momentum': 0.9}\n",
      "\n",
      "Number of warmup epochs: 5\n",
      "Warmup Learning Rate Mode: linear\n",
      "Learing Rate Mode: cosine\n",
      "Learing Rate Decay: 0.1\n",
      "Learning Rate Decay Epochs: [30, 60, 90, inf]\n",
      "\n",
      "Training Settings Set Successfully.\n"
     ]
    }
   ],
   "source": [
    "if mixup or stitch_mixup:\n",
    "    epochs = 200 # Mixup asks for longer training to converge better\n",
    "else:\n",
    "    epochs = 120\n",
    "    \n",
    "warmup_epochs = 5\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "lr_decay_epochs = [30, 60, 90, np.inf] # Epochs where learning rate decays\n",
    "# lr_decay_epochs = [40, 80]\n",
    "\n",
    "warmup_lr_mode = 'linear'\n",
    "lr_mode = 'cosine'\n",
    "lr_decay = 0.1 # Learning rate decay factor\n",
    "target_lr = 0\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler(warmup_lr_mode,\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler(lr_mode,\n",
    "                base_lr = 0.1,\n",
    "                target_lr = target_lr,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epochs,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistillationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                           hard_weight=hard_weight,\n",
    "                                                           sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))\n",
    "print(\"\\nUsing mixup: {}\".format(mixup))\n",
    "print(\"Number of no-mixup epochs:{}\".format(mixup_off_epochs))\n",
    "\n",
    "print(\"\\nUsing {} Optimizer\".format(optimizer))\n",
    "print(optimizer_params)\n",
    "print(\"\\nNumber of warmup epochs: {}\".format(warmup_epochs))\n",
    "print(\"Warmup Learning Rate Mode: {}\".format(warmup_lr_mode))\n",
    "print(\"Learing Rate Mode: {}\".format(lr_mode))\n",
    "print(\"Learing Rate Decay: {}\".format(lr_decay))\n",
    "print(\"Learning Rate Decay Epochs: {}\".format(lr_decay_epochs))\n",
    "print(\"\\nTraining Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started for 200 epochs:\n",
      "[Epoch 0] train=1.000000 val_top1=0.142900 val_top5=0.465600 loss=46076205.757812 time: 178.070246\n",
      "[Epoch 1] train=1.000000 val_top1=0.241200 val_top5=0.620900 loss=46064843.179688 time: 173.180571\n",
      "[Epoch 2] train=1.000000 val_top1=0.301800 val_top5=0.713000 loss=46059284.109375 time: 173.127849\n",
      "[Epoch 3] train=1.000000 val_top1=0.349900 val_top5=0.759100 loss=46053881.570312 time: 174.720312\n",
      "[Epoch 4] train=1.000000 val_top1=0.427500 val_top5=0.810800 loss=46048947.671875 time: 179.203099\n",
      "[Epoch 5] train=1.000000 val_top1=0.439900 val_top5=0.815300 loss=46045148.851562 time: 180.324555\n",
      "[Epoch 6] train=1.000000 val_top1=0.498100 val_top5=0.848100 loss=46042072.000000 time: 174.626389\n",
      "[Epoch 7] train=1.000000 val_top1=0.488200 val_top5=0.830600 loss=46039083.421875 time: 177.415295\n",
      "[Epoch 8] train=1.000000 val_top1=0.540500 val_top5=0.874300 loss=46037346.687500 time: 174.893756\n",
      "[Epoch 9] train=1.000000 val_top1=0.552200 val_top5=0.887200 loss=46035891.140625 time: 173.027475\n",
      "[Epoch 10] train=1.000000 val_top1=0.580800 val_top5=0.895700 loss=46035009.343750 time: 175.795792\n",
      "[Epoch 11] train=1.000000 val_top1=0.559200 val_top5=0.882100 loss=46032982.039062 time: 172.917941\n",
      "[Epoch 12] train=1.000000 val_top1=0.561800 val_top5=0.889600 loss=46032080.460938 time: 176.423349\n",
      "[Epoch 13] train=1.000000 val_top1=0.620700 val_top5=0.914100 loss=46031641.671875 time: 174.472253\n",
      "[Epoch 14] train=1.000000 val_top1=0.609300 val_top5=0.904800 loss=46030743.632812 time: 177.974595\n",
      "[Epoch 15] train=1.000000 val_top1=0.615700 val_top5=0.912800 loss=46030170.828125 time: 176.222960\n",
      "[Epoch 16] train=1.000000 val_top1=0.638200 val_top5=0.918000 loss=46030469.804688 time: 176.997677\n",
      "[Epoch 17] train=1.000000 val_top1=0.599300 val_top5=0.894700 loss=46028703.265625 time: 176.310467\n",
      "[Epoch 18] train=1.000000 val_top1=0.613000 val_top5=0.914800 loss=46028904.414062 time: 173.346358\n",
      "[Epoch 19] train=1.000000 val_top1=0.618400 val_top5=0.913300 loss=46028128.078125 time: 175.814914\n",
      "[Epoch 20] train=1.000000 val_top1=0.661400 val_top5=0.931900 loss=46027875.968750 time: 174.116679\n",
      "[Epoch 21] train=1.000000 val_top1=0.643900 val_top5=0.915600 loss=46027272.023438 time: 177.542023\n",
      "[Epoch 22] train=1.000000 val_top1=0.652500 val_top5=0.926100 loss=46026831.062500 time: 176.676357\n",
      "[Epoch 23] train=1.000000 val_top1=0.676800 val_top5=0.934800 loss=46025798.054688 time: 182.699374\n",
      "[Epoch 24] train=1.000000 val_top1=0.686000 val_top5=0.939800 loss=46025668.156250 time: 182.135146\n",
      "[Epoch 25] train=1.000000 val_top1=0.645000 val_top5=0.923800 loss=46025798.984375 time: 176.230511\n",
      "[Epoch 26] train=1.000000 val_top1=0.674500 val_top5=0.930800 loss=46026149.554688 time: 175.262863\n",
      "[Epoch 27] train=1.000000 val_top1=0.665600 val_top5=0.930400 loss=46023737.914062 time: 176.763492\n",
      "[Epoch 28] train=1.000000 val_top1=0.668800 val_top5=0.929400 loss=46024421.226562 time: 173.681555\n",
      "[Epoch 29] train=1.000000 val_top1=0.656900 val_top5=0.912800 loss=46023691.250000 time: 179.194737\n",
      "[Epoch 30] train=1.000000 val_top1=0.656600 val_top5=0.920700 loss=46024254.179688 time: 174.235683\n",
      "[Epoch 31] train=1.000000 val_top1=0.654700 val_top5=0.917300 loss=46022868.398438 time: 174.240688\n",
      "[Epoch 32] train=1.000000 val_top1=0.679200 val_top5=0.934100 loss=46024375.789062 time: 173.747565\n",
      "[Epoch 33] train=1.000000 val_top1=0.686900 val_top5=0.931500 loss=46022780.593750 time: 177.553746\n",
      "[Epoch 34] train=1.000000 val_top1=0.682000 val_top5=0.932500 loss=46022874.640625 time: 180.944347\n",
      "[Epoch 35] train=1.000000 val_top1=0.632300 val_top5=0.896900 loss=46023236.726562 time: 174.865609\n",
      "[Epoch 36] train=1.000000 val_top1=0.673000 val_top5=0.930800 loss=46023335.898438 time: 176.869651\n",
      "[Epoch 37] train=1.000000 val_top1=0.697000 val_top5=0.937800 loss=46021813.687500 time: 175.150877\n",
      "[Epoch 38] train=1.000000 val_top1=0.657800 val_top5=0.916200 loss=46023698.328125 time: 178.658632\n",
      "[Epoch 39] train=1.000000 val_top1=0.688000 val_top5=0.938500 loss=46022619.554688 time: 179.200213\n",
      "[Epoch 40] train=1.000000 val_top1=0.687500 val_top5=0.930800 loss=46020648.414062 time: 176.336267\n",
      "[Epoch 41] train=1.000000 val_top1=0.718700 val_top5=0.945900 loss=46021281.343750 time: 178.399943\n",
      "[Epoch 42] train=1.000000 val_top1=0.697400 val_top5=0.937800 loss=46021177.968750 time: 174.758134\n",
      "[Epoch 43] train=1.000000 val_top1=0.680300 val_top5=0.933900 loss=46021136.250000 time: 177.548974\n",
      "[Epoch 44] train=1.000000 val_top1=0.699500 val_top5=0.944000 loss=46020817.085938 time: 175.436790\n",
      "[Epoch 45] train=1.000000 val_top1=0.693000 val_top5=0.938400 loss=46020804.671875 time: 176.695519\n",
      "[Epoch 46] train=1.000000 val_top1=0.716900 val_top5=0.938900 loss=46020794.054688 time: 176.077487\n",
      "[Epoch 47] train=1.000000 val_top1=0.688900 val_top5=0.932800 loss=46021277.578125 time: 177.162634\n",
      "[Epoch 48] train=1.000000 val_top1=0.700500 val_top5=0.933900 loss=46019929.414062 time: 174.963673\n",
      "[Epoch 49] train=1.000000 val_top1=0.711800 val_top5=0.944700 loss=46020550.664062 time: 176.712897\n",
      "[Epoch 50] train=1.000000 val_top1=0.722200 val_top5=0.947800 loss=46020704.828125 time: 175.927194\n",
      "[Epoch 51] train=1.000000 val_top1=0.723400 val_top5=0.944200 loss=46020625.375000 time: 174.624634\n",
      "[Epoch 52] train=1.000000 val_top1=0.721300 val_top5=0.940100 loss=46019436.796875 time: 176.534216\n",
      "[Epoch 53] train=1.000000 val_top1=0.710200 val_top5=0.944300 loss=46019619.093750 time: 174.120772\n",
      "[Epoch 54] train=1.000000 val_top1=0.695700 val_top5=0.930800 loss=46018464.000000 time: 178.523194\n",
      "[Epoch 55] train=1.000000 val_top1=0.668600 val_top5=0.921500 loss=46019606.757812 time: 179.260935\n",
      "[Epoch 56] train=1.000000 val_top1=0.693800 val_top5=0.932500 loss=46019444.921875 time: 178.373665\n",
      "[Epoch 57] train=1.000000 val_top1=0.727700 val_top5=0.943300 loss=46018627.742188 time: 175.236948\n",
      "[Epoch 58] train=1.000000 val_top1=0.710300 val_top5=0.943700 loss=46020171.375000 time: 177.220596\n",
      "[Epoch 59] train=1.000000 val_top1=0.716400 val_top5=0.941400 loss=46019748.906250 time: 173.722184\n",
      "[Epoch 60] train=1.000000 val_top1=0.723000 val_top5=0.943800 loss=46018306.984375 time: 177.209112\n",
      "[Epoch 61] train=1.000000 val_top1=0.713800 val_top5=0.945900 loss=46018246.171875 time: 175.890900\n",
      "[Epoch 62] train=1.000000 val_top1=0.728200 val_top5=0.946700 loss=46018226.437500 time: 173.705831\n",
      "[Epoch 63] train=1.000000 val_top1=0.699700 val_top5=0.940100 loss=46018746.359375 time: 177.194492\n",
      "[Epoch 64] train=1.000000 val_top1=0.714700 val_top5=0.941500 loss=46016579.406250 time: 175.681231\n",
      "[Epoch 65] train=1.000000 val_top1=0.732800 val_top5=0.947200 loss=46018365.609375 time: 174.812919\n",
      "[Epoch 66] train=1.000000 val_top1=0.721600 val_top5=0.943200 loss=46018110.640625 time: 177.939844\n",
      "[Epoch 67] train=1.000000 val_top1=0.721900 val_top5=0.945700 loss=46016690.585938 time: 174.673461\n",
      "[Epoch 68] train=1.000000 val_top1=0.738200 val_top5=0.950300 loss=46018033.320312 time: 179.182335\n",
      "[Epoch 69] train=1.000000 val_top1=0.701200 val_top5=0.930700 loss=46016374.085938 time: 175.083157\n",
      "[Epoch 70] train=1.000000 val_top1=0.681000 val_top5=0.919500 loss=46017873.226562 time: 177.157015\n",
      "[Epoch 71] train=1.000000 val_top1=0.711300 val_top5=0.941100 loss=46016596.593750 time: 173.583621\n",
      "[Epoch 72] train=1.000000 val_top1=0.706300 val_top5=0.932900 loss=46017346.679688 time: 181.874445\n",
      "[Epoch 73] train=1.000000 val_top1=0.706200 val_top5=0.943500 loss=46016061.015625 time: 182.211823\n",
      "[Epoch 74] train=1.000000 val_top1=0.711000 val_top5=0.941500 loss=46016294.296875 time: 176.791224\n",
      "[Epoch 75] train=1.000000 val_top1=0.723000 val_top5=0.941300 loss=46017084.210938 time: 176.540712\n",
      "[Epoch 76] train=1.000000 val_top1=0.732600 val_top5=0.946800 loss=46017082.023438 time: 176.559686\n",
      "[Epoch 77] train=1.000000 val_top1=0.750200 val_top5=0.954800 loss=46016467.929688 time: 174.444642\n",
      "[Epoch 78] train=1.000000 val_top1=0.716400 val_top5=0.936900 loss=46016286.117188 time: 177.215145\n",
      "[Epoch 79] train=1.000000 val_top1=0.725200 val_top5=0.943900 loss=46016203.320312 time: 179.915291\n",
      "[Epoch 80] train=1.000000 val_top1=0.736900 val_top5=0.951100 loss=46015923.726562 time: 177.837508\n",
      "[Epoch 81] train=1.000000 val_top1=0.726700 val_top5=0.946300 loss=46016626.453125 time: 176.111646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82] train=1.000000 val_top1=0.727100 val_top5=0.943800 loss=46016652.132812 time: 176.959264\n",
      "[Epoch 83] train=1.000000 val_top1=0.690200 val_top5=0.936100 loss=46015158.367188 time: 174.433114\n",
      "[Epoch 84] train=1.000000 val_top1=0.727000 val_top5=0.943300 loss=46015598.781250 time: 175.664763\n",
      "[Epoch 85] train=1.000000 val_top1=0.741500 val_top5=0.948200 loss=46016141.781250 time: 175.300522\n",
      "[Epoch 86] train=1.000000 val_top1=0.747100 val_top5=0.949900 loss=46016532.687500 time: 174.436261\n",
      "[Epoch 87] train=1.000000 val_top1=0.737000 val_top5=0.947400 loss=46015226.351562 time: 173.667072\n",
      "[Epoch 88] train=1.000000 val_top1=0.745700 val_top5=0.949600 loss=46015554.898438 time: 177.662644\n",
      "[Epoch 89] train=1.000000 val_top1=0.752300 val_top5=0.951000 loss=46015470.523438 time: 173.291411\n",
      "[Epoch 90] train=1.000000 val_top1=0.737600 val_top5=0.947200 loss=46014647.859375 time: 173.118110\n",
      "[Epoch 91] train=1.000000 val_top1=0.750000 val_top5=0.949900 loss=46015420.882812 time: 173.742399\n",
      "[Epoch 92] train=1.000000 val_top1=0.738200 val_top5=0.945000 loss=46015165.226562 time: 179.275971\n",
      "[Epoch 93] train=1.000000 val_top1=0.739200 val_top5=0.944800 loss=46015564.296875 time: 181.033913\n",
      "[Epoch 94] train=1.000000 val_top1=0.750700 val_top5=0.950500 loss=46014316.031250 time: 182.885528\n",
      "[Epoch 95] train=1.000000 val_top1=0.737000 val_top5=0.945400 loss=46015006.476562 time: 179.243636\n",
      "[Epoch 96] train=1.000000 val_top1=0.747600 val_top5=0.947500 loss=46014107.968750 time: 176.417724\n",
      "[Epoch 97] train=1.000000 val_top1=0.748900 val_top5=0.948400 loss=46014364.062500 time: 176.045675\n",
      "[Epoch 98] train=1.000000 val_top1=0.739300 val_top5=0.948400 loss=46013857.773438 time: 176.561397\n",
      "[Epoch 99] train=1.000000 val_top1=0.744200 val_top5=0.950800 loss=46014084.867188 time: 174.649564\n",
      "[Epoch 100] train=1.000000 val_top1=0.748400 val_top5=0.950900 loss=46015581.773438 time: 176.175779\n",
      "[Epoch 101] train=1.000000 val_top1=0.752300 val_top5=0.947100 loss=46014131.078125 time: 173.771201\n",
      "[Epoch 102] train=1.000000 val_top1=0.729100 val_top5=0.938000 loss=46013257.640625 time: 175.969798\n",
      "[Epoch 103] train=1.000000 val_top1=0.736500 val_top5=0.951100 loss=46013855.382812 time: 174.917945\n",
      "[Epoch 104] train=1.000000 val_top1=0.747800 val_top5=0.943700 loss=46013021.937500 time: 175.501462\n",
      "[Epoch 105] train=1.000000 val_top1=0.751800 val_top5=0.950100 loss=46012769.601562 time: 178.340811\n",
      "[Epoch 106] train=1.000000 val_top1=0.764900 val_top5=0.956000 loss=46014626.062500 time: 179.946295\n",
      "[Epoch 107] train=1.000000 val_top1=0.771300 val_top5=0.953900 loss=46012343.015625 time: 177.434987\n",
      "[Epoch 108] train=1.000000 val_top1=0.758800 val_top5=0.953400 loss=46013567.710938 time: 176.441097\n",
      "[Epoch 109] train=1.000000 val_top1=0.746100 val_top5=0.948500 loss=46014235.062500 time: 178.137101\n",
      "[Epoch 110] train=1.000000 val_top1=0.757800 val_top5=0.954900 loss=46012510.593750 time: 173.333948\n",
      "[Epoch 111] train=1.000000 val_top1=0.751700 val_top5=0.944300 loss=46012806.625000 time: 176.081318\n",
      "[Epoch 112] train=1.000000 val_top1=0.760900 val_top5=0.955500 loss=46012797.304688 time: 176.217317\n",
      "[Epoch 113] train=1.000000 val_top1=0.757000 val_top5=0.954600 loss=46012533.125000 time: 173.753096\n",
      "[Epoch 114] train=1.000000 val_top1=0.762500 val_top5=0.953700 loss=46012106.640625 time: 174.832984\n",
      "[Epoch 115] train=1.000000 val_top1=0.760600 val_top5=0.952300 loss=46012879.070312 time: 180.805783\n",
      "[Epoch 116] train=1.000000 val_top1=0.761200 val_top5=0.948600 loss=46012107.445312 time: 178.477036\n",
      "[Epoch 117] train=1.000000 val_top1=0.757000 val_top5=0.947100 loss=46012449.179688 time: 177.808950\n",
      "[Epoch 118] train=1.000000 val_top1=0.763200 val_top5=0.948500 loss=46011748.554688 time: 177.043989\n",
      "[Epoch 119] train=1.000000 val_top1=0.748700 val_top5=0.947400 loss=46010967.726562 time: 173.439921\n",
      "[Epoch 120] train=1.000000 val_top1=0.764900 val_top5=0.952800 loss=46012146.765625 time: 179.164293\n",
      "[Epoch 121] train=1.000000 val_top1=0.766400 val_top5=0.954200 loss=46010697.835938 time: 177.399560\n",
      "[Epoch 122] train=1.000000 val_top1=0.767300 val_top5=0.949800 loss=46010680.625000 time: 177.116764\n",
      "[Epoch 123] train=1.000000 val_top1=0.741600 val_top5=0.943800 loss=46011472.656250 time: 174.553436\n",
      "[Epoch 124] train=1.000000 val_top1=0.753000 val_top5=0.942700 loss=46012532.093750 time: 175.439819\n",
      "[Epoch 125] train=1.000000 val_top1=0.772300 val_top5=0.953600 loss=46010644.710938 time: 177.459364\n",
      "[Epoch 126] train=1.000000 val_top1=0.767900 val_top5=0.954300 loss=46010777.625000 time: 174.116671\n",
      "[Epoch 127] train=1.000000 val_top1=0.772600 val_top5=0.956000 loss=46011249.109375 time: 177.359990\n",
      "[Epoch 128] train=1.000000 val_top1=0.761800 val_top5=0.950200 loss=46010085.679688 time: 181.680912\n",
      "[Epoch 129] train=1.000000 val_top1=0.765100 val_top5=0.949700 loss=46009293.062500 time: 193.618412\n",
      "[Epoch 130] train=1.000000 val_top1=0.773300 val_top5=0.955700 loss=46009775.718750 time: 188.708889\n",
      "[Epoch 131] train=1.000000 val_top1=0.767800 val_top5=0.955000 loss=46009646.914062 time: 186.252132\n",
      "[Epoch 132] train=1.000000 val_top1=0.763800 val_top5=0.948500 loss=46010802.164062 time: 186.028840\n",
      "[Epoch 133] train=1.000000 val_top1=0.772700 val_top5=0.956700 loss=46010119.625000 time: 185.095661\n",
      "[Epoch 134] train=1.000000 val_top1=0.781700 val_top5=0.954600 loss=46010336.226562 time: 191.543131\n",
      "[Epoch 135] train=1.000000 val_top1=0.780200 val_top5=0.956500 loss=46008566.851562 time: 193.881076\n",
      "[Epoch 136] train=1.000000 val_top1=0.765900 val_top5=0.954100 loss=46010382.585938 time: 187.121629\n",
      "[Epoch 137] train=1.000000 val_top1=0.784400 val_top5=0.953400 loss=46009568.945312 time: 186.816907\n",
      "[Epoch 138] train=1.000000 val_top1=0.774400 val_top5=0.955200 loss=46010179.367188 time: 186.462000\n",
      "[Epoch 139] train=1.000000 val_top1=0.783500 val_top5=0.951100 loss=46009193.242188 time: 188.663437\n",
      "[Epoch 140] train=1.000000 val_top1=0.780100 val_top5=0.956800 loss=46008498.625000 time: 191.702905\n",
      "[Epoch 141] train=1.000000 val_top1=0.788000 val_top5=0.956200 loss=46006907.570312 time: 192.335526\n",
      "[Epoch 142] train=1.000000 val_top1=0.779200 val_top5=0.956000 loss=46007785.445312 time: 187.366384\n",
      "[Epoch 143] train=1.000000 val_top1=0.784300 val_top5=0.956400 loss=46008064.484375 time: 179.388964\n",
      "[Epoch 144] train=1.000000 val_top1=0.783900 val_top5=0.952900 loss=46006700.890625 time: 179.843841\n",
      "[Epoch 145] train=1.000000 val_top1=0.784200 val_top5=0.954700 loss=46007656.632812 time: 179.640530\n",
      "[Epoch 146] train=1.000000 val_top1=0.791200 val_top5=0.956900 loss=46008149.828125 time: 177.596707\n",
      "[Epoch 147] train=1.000000 val_top1=0.782500 val_top5=0.955200 loss=46006870.984375 time: 178.209731\n",
      "[Epoch 148] train=1.000000 val_top1=0.788500 val_top5=0.957900 loss=46006346.937500 time: 179.253200\n",
      "[Epoch 149] train=1.000000 val_top1=0.788600 val_top5=0.957600 loss=46009019.195312 time: 178.156750\n",
      "[Epoch 150] train=1.000000 val_top1=0.794800 val_top5=0.960000 loss=46007299.718750 time: 179.016586\n",
      "[Epoch 151] train=1.000000 val_top1=0.796600 val_top5=0.958100 loss=46007723.875000 time: 177.480136\n",
      "[Epoch 152] train=1.000000 val_top1=0.790900 val_top5=0.956300 loss=46005900.195312 time: 176.471746\n",
      "[Epoch 153] train=1.000000 val_top1=0.789400 val_top5=0.953900 loss=46006485.820312 time: 180.429027\n",
      "[Epoch 154] train=1.000000 val_top1=0.795300 val_top5=0.956400 loss=46006991.859375 time: 177.347117\n",
      "[Epoch 155] train=1.000000 val_top1=0.790100 val_top5=0.955000 loss=46007453.859375 time: 181.010113\n",
      "[Epoch 156] train=1.000000 val_top1=0.795000 val_top5=0.956300 loss=46005763.773438 time: 183.515414\n",
      "[Epoch 157] train=1.000000 val_top1=0.792100 val_top5=0.959600 loss=46006908.343750 time: 177.204899\n",
      "[Epoch 158] train=1.000000 val_top1=0.803700 val_top5=0.958700 loss=46006305.835938 time: 180.860090\n",
      "[Epoch 159] train=1.000000 val_top1=0.802100 val_top5=0.956600 loss=46006578.640625 time: 177.559439\n",
      "[Epoch 160] train=1.000000 val_top1=0.803900 val_top5=0.961300 loss=46006961.046875 time: 179.086791\n",
      "[Epoch 161] train=1.000000 val_top1=0.797600 val_top5=0.956500 loss=46004801.117188 time: 177.310117\n",
      "[Epoch 162] train=1.000000 val_top1=0.801000 val_top5=0.959900 loss=46006780.734375 time: 174.947018\n",
      "[Epoch 163] train=1.000000 val_top1=0.803400 val_top5=0.956600 loss=46004788.296875 time: 175.274004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 164] train=1.000000 val_top1=0.797900 val_top5=0.958100 loss=46006127.773438 time: 175.639162\n",
      "[Epoch 165] train=1.000000 val_top1=0.803500 val_top5=0.956300 loss=46005196.984375 time: 182.219224\n",
      "[Epoch 166] train=1.000000 val_top1=0.801900 val_top5=0.959600 loss=46005412.875000 time: 179.044964\n",
      "[Epoch 167] train=1.000000 val_top1=0.806300 val_top5=0.960400 loss=46004522.382812 time: 179.142330\n",
      "[Epoch 168] train=1.000000 val_top1=0.803000 val_top5=0.959500 loss=46005407.250000 time: 179.816833\n",
      "[Epoch 169] train=1.000000 val_top1=0.806300 val_top5=0.958100 loss=46004027.617188 time: 176.574837\n",
      "[Epoch 170] train=1.000000 val_top1=0.806200 val_top5=0.960600 loss=46005374.328125 time: 177.386111\n",
      "[Epoch 171] train=1.000000 val_top1=0.807200 val_top5=0.959600 loss=46004748.781250 time: 177.619010\n",
      "[Epoch 172] train=1.000000 val_top1=0.806700 val_top5=0.961900 loss=46004890.414062 time: 180.511774\n",
      "[Epoch 173] train=1.000000 val_top1=0.806500 val_top5=0.959300 loss=46003951.492188 time: 176.207751\n",
      "[Epoch 174] train=1.000000 val_top1=0.808000 val_top5=0.959200 loss=46004249.203125 time: 180.649519\n",
      "[Epoch 175] train=1.000000 val_top1=0.808100 val_top5=0.959100 loss=46004149.234375 time: 177.234397\n",
      "[Epoch 176] train=1.000000 val_top1=0.809400 val_top5=0.958600 loss=46004846.726562 time: 176.487700\n",
      "[Epoch 177] train=1.000000 val_top1=0.811200 val_top5=0.961100 loss=46005407.171875 time: 178.408851\n",
      "[Epoch 178] train=1.000000 val_top1=0.811700 val_top5=0.961200 loss=46004325.289062 time: 181.537561\n",
      "[Epoch 179] train=1.000000 val_top1=0.810300 val_top5=0.961800 loss=46005729.046875 time: 179.403706\n",
      "[Epoch 180] train=1.000000 val_top1=0.809400 val_top5=0.959700 loss=46004389.023438 time: 178.352943\n",
      "[Epoch 181] train=1.000000 val_top1=0.811500 val_top5=0.960200 loss=46004177.539062 time: 179.100626\n",
      "[Epoch 182] train=1.000000 val_top1=0.812400 val_top5=0.959700 loss=46003322.843750 time: 176.361403\n",
      "[Epoch 183] train=1.000000 val_top1=0.810300 val_top5=0.960600 loss=46004366.484375 time: 177.649944\n",
      "[Epoch 184] train=1.000000 val_top1=0.813200 val_top5=0.958100 loss=46002012.546875 time: 176.776336\n",
      "[Epoch 185] train=1.000000 val_top1=0.812700 val_top5=0.961000 loss=46003348.320312 time: 177.783929\n",
      "[Epoch 186] train=1.000000 val_top1=0.814500 val_top5=0.961200 loss=46004883.976562 time: 179.629207\n",
      "[Epoch 187] train=1.000000 val_top1=0.811700 val_top5=0.962700 loss=46005770.671875 time: 179.499675\n",
      "[Epoch 188] train=1.000000 val_top1=0.814300 val_top5=0.960100 loss=46004008.546875 time: 184.511513\n",
      "[Epoch 189] train=1.000000 val_top1=0.813900 val_top5=0.961000 loss=46004303.179688 time: 179.688868\n",
      "[Epoch 190] train=1.000000 val_top1=0.813600 val_top5=0.960300 loss=46003564.664062 time: 180.518985\n",
      "[Epoch 191] train=1.000000 val_top1=0.812500 val_top5=0.960700 loss=46003307.101562 time: 178.458840\n",
      "[Epoch 192] train=1.000000 val_top1=0.813700 val_top5=0.962400 loss=46004115.773438 time: 178.904698\n",
      "[Epoch 193] train=1.000000 val_top1=0.812400 val_top5=0.960300 loss=46003714.898438 time: 178.790593\n",
      "[Epoch 194] train=1.000000 val_top1=0.813200 val_top5=0.960500 loss=46004019.210938 time: 177.851139\n",
      "[Epoch 195] train=1.000000 val_top1=0.813400 val_top5=0.960100 loss=46004810.968750 time: 176.537244\n",
      "[Epoch 196] train=1.000000 val_top1=0.814400 val_top5=0.960900 loss=46004373.101562 time: 176.368271\n",
      "[Epoch 197] train=1.000000 val_top1=0.814200 val_top5=0.961300 loss=46003287.273438 time: 179.187012\n",
      "[Epoch 198] train=1.000000 val_top1=0.814200 val_top5=0.961400 loss=46003694.875000 time: 178.486762\n",
      "[Epoch 199] train=1.000000 val_top1=0.814200 val_top5=0.959600 loss=46004443.187500 time: 178.457108\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started for {} epochs:\".format(epochs))\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data  = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup or stitch_mixup:\n",
    "            if mixup and not(stitch_mixup):\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                if epoch >= epochs - mixup_off_epochs:\n",
    "                    lam = 1\n",
    "                data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "                if label_smoothing:\n",
    "                    eta = 0.1\n",
    "                else:\n",
    "                    eta = 0.0\n",
    "                label = mixup_transform(label, num_classes, lam, eta)\n",
    "            \n",
    "            elif stitch_mixup and not(mixup):\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                if lam < 0.1:\n",
    "                    lam = 1\n",
    "                if epoch >= epochs - mixup_off_epochs:\n",
    "                    lam = 1\n",
    "                data = [stitch_mixup_transform(X, lam) for X in data]\n",
    "                \n",
    "                if label_smoothing:\n",
    "                    eta = 0.1\n",
    "                else:\n",
    "                    eta = 0.0\n",
    "                label = stitch_mixup_label_transform(label, num_classes, lam, eta)\n",
    "            \n",
    "            else:\n",
    "                print(\"No Mixup.\")\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype(dtype, copy=False),\n",
    "                                y.astype(dtype, copy=False),\n",
    "                                p.astype(dtype, copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                   label, \n",
    "                                                                                   teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup or stitch_mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, outputs)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_errors_{t}.png\".format(o=optimizer,\n",
    "                                                                                           ep=epochs,\n",
    "                                                                                           t=timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_accuracies_{t}.png\".format(o=optimizer,\n",
    "                                                                                               ep=epochs,\n",
    "                                                                                               t=timestamp))\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mxnet_latest_p37] *",
   "language": "python",
   "name": "conda-env-mxnet_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
