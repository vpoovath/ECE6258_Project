{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math, os, sys\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from skimage import util as sk_util\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory, LRSequential, LRScheduler\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size = 256 # Batch Size for Each GPU\n",
    "num_workers = 2             # Number of data loader workers\n",
    "dtype = 'float32'           # Default training data type if float32\n",
    "num_gpus = 1                # number of GPUs to use\n",
    "batch_size = per_device_batch_size * num_gpus # Calculate effective total batch size\n",
    "\n",
    "# For CIFAR100 Dataset:\n",
    "num_classes = 100\n",
    "num_images_per_class = 500\n",
    "num_training_samples = num_classes * num_images_per_class\n",
    "num_batches = num_training_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using label smoothing: True\n"
     ]
    }
   ],
   "source": [
    "label_smoothing = True\n",
    "def smooth(label, num_classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        res = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                     off_value = eta/num_classes)\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mixup: False\n"
     ]
    }
   ],
   "source": [
    "mixup = False\n",
    "def mixup_transform(label, num_classes, lam=1, eta=0.0):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using mixup: {}\".format(mixup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using spatial mixup: True\n",
      "Using vertical spatial mixup.\n"
     ]
    }
   ],
   "source": [
    "stitch_mixup = True\n",
    "stitch_type = 'vertical'\n",
    "def stitch_mixup_transform(X, lam, stitch_type='horizontal'):\n",
    "    h1 = 32\n",
    "    w1 = 32\n",
    "    if lam < 1:\n",
    "        Y = nd.zeros_like(X)\n",
    "        if stitch_type == 'horizontal':\n",
    "            Y[:,:,0:int(h1*lam),:] = X[:,:,0:int(h1*lam),:]\n",
    "            Y[:,:,int(h1*lam):h1,:] = X[::-1,:,int(h1*lam):h1,:]\n",
    "        elif stitch_type == 'vertical':\n",
    "            Y[:,:,:,0:int(h1*lam)] = X[:,:,:,0:int(h1*lam)]\n",
    "            Y[:,:,:,int(h1*lam):h1] = X[::-1,:,:,int(h1*lam):h1]\n",
    "        else:\n",
    "            raise(\"Invalid stitch type passed in!\")\n",
    "            return\n",
    "        return Y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def stitch_mixup_label_transform(label, num_classes, lam=1, eta=0.0, stitch_type='horizontal'):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        print(\"Label changed to list\")\n",
    "        label = [label]\n",
    "    res = []\n",
    "    for l in label:\n",
    "        y1 = l.one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                    off_value = eta/num_classes)\n",
    "        y2 = l[::-1].one_hot(num_classes, on_value = 1 - eta + eta/num_classes, \n",
    "                                          off_value = eta/num_classes)\n",
    "        res.append(lam*y1 + (1-lam)*y2)\n",
    "    return res\n",
    "print(\"Using spatial mixup: {}\".format(stitch_mixup))\n",
    "if stitch_mixup:\n",
    "    print(\"Using {} spatial mixup.\".format(stitch_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_img_noise(img_tensor, noise_type='gaussian'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    reshaped_img = nd.reshape(img_tensor, (32, 32, 3))\n",
    "    noise_img_numpy = sk_util.random_noise(reshaped_img.asnumpy(), \n",
    "                                           mode=noise_type, \n",
    "                                           seed=None, \n",
    "                                           clip=True)\n",
    "    noise_img = nd.reshape(nd.array(noise_img_numpy), (3, 32, 32))\n",
    "    return noise_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Init Done.\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# last_gamma = False\n",
    "# print(\"Using last gamma: {}\".format(last_gamma))\n",
    "# kwargs = {'ctx':ctx, 'classes':num_classes, 'last_gamma':last_gamma}\n",
    "\n",
    "kwargs = {'ctx':ctx, 'classes':num_classes}\n",
    "\n",
    "use_group_norm = False\n",
    "if use_group_norm:\n",
    "    kwargs['norm_layer'] = gcv.nn.GroupNorm\n",
    "    print(\"Using Group Normalization: {}\".format(use_group_norm))\n",
    "\n",
    "default_init = True\n",
    "net = get_model('cifar_resnet56_v1', **kwargs)\n",
    "\n",
    "if default_init:\n",
    "    net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "else:\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    print(\"Using MSRA Prelu Init.\")\n",
    "\n",
    "    net.cast(dtype)\n",
    "print(\"\\nModel Init Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "Note that the reference paper uses a teacher model that was trained using cosine learning rate decay and label smoothing. \n",
    "\n",
    "Therefore I have created a separate notebook that trains the ResNext29_16x64d model using cosine learning rate decay and label smoothing on the CIFAR100 dataset. Training this ResNext29 architecture took nearly 10 hours...\n",
    "\n",
    "What is left to do is now just to load the model architecture again, and load the parameters from the saved file.\n",
    "\n",
    "### The old approach, when not using my own trained model is:\n",
    "Load the pre-trained CIFAR10 models and replace the final output layer with 100 classes instead of 10. This is demonstrated at this website: https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/packages/gluon/image/pretrained_models.html\n",
    "\n",
    "Need to investigate WideResNet issue of having Top1-Val Accuracies becoming 0.00000 at the very beginning of training. \n",
    "\n",
    "Additionally, need to understand the two ResNeXt architectures to understand why training time is much longer...:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not using distillation\n"
     ]
    }
   ],
   "source": [
    "distillation = False\n",
    "\n",
    "if distillation:\n",
    "    curr_dir = os.getcwd()\n",
    "    param_file = os.path.join(curr_dir, \"resnext29_teacher.params\") \n",
    "    T = 20\n",
    "    hard_weight = 0.5\n",
    "    # Teacher model for distillation training\n",
    "    # teacher_name = 'cifar_resnet110_v2'\n",
    "    # teacher_name = 'cifar_resnet56_v2'\n",
    "    # teacher_name = 'cifar_wideresnet28_10' # Top1-Val is 0...\n",
    "    # teacher_name = 'cifar_wideresnet40_8'  # Might cause the same problem\n",
    "    # teacher_name = 'cifar_resnext29_32x4d' # This is apparently not available...?\n",
    "    \n",
    "    teacher_name = 'cifar_resnext29_16x64d'\n",
    "#     teacher = get_model(teacher_name, pretrained=True, ctx=ctx)\n",
    "#     #teacher.collect_params().initialize(ctx=ctx, force_reinit=True) # Don't do this.\n",
    "#     teacher.cast(dtype)\n",
    "#     with teacher.name_scope():\n",
    "#         teacher.output = gluon.nn.Dense(num_classes)\n",
    "#         teacher.output.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    \n",
    "    teacher = get_model(teacher_name, classes=num_classes, ctx=ctx)\n",
    "    teacher.load_parameters(param_file)\n",
    "    teacher.cast(dtype)\n",
    "        \n",
    "    print(teacher.output)\n",
    "    print(\"\\nTeacher Model Init Done!\")\n",
    "else:\n",
    "    print(\"\\nNot using distillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Step Successful.\n"
     ]
    }
   ],
   "source": [
    "resize = 32\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "max_aspect_ratio = 4.0 / 3.0\n",
    "min_aspect_ratio = 3.0 / 4.0\n",
    "max_random_area = 1\n",
    "min_random_area = 0.08\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([    \n",
    "#     transforms.RandomResizedCrop(resize,\n",
    "#                                  scale=(min_random_area, max_random_area), \n",
    "#                                  ratio=(min_aspect_ratio, max_aspect_ratio)),\n",
    "    \n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    \n",
    "    transforms.RandomBrightness(brightness=jitter_param),\n",
    "    transforms.RandomSaturation(saturation=jitter_param),\n",
    "    transforms.RandomHue(hue=jitter_param),\n",
    "    \n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    \n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "        \n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_rgb, std_rgb),\n",
    "])\n",
    "print(\"Preprocessing Step Successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of train_data and val_data successful.\n",
      "Per Device Batch Size: 256\n"
     ]
    }
   ],
   "source": [
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    last_batch='discard', \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "# Set shuffle=False to shuffle the testing data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers)\n",
    "print(\"Initialization of train_data and val_data successful.\")\n",
    "print(\"Per Device Batch Size: {}\".format(per_device_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse label loss: False\n",
      "\n",
      "Using label smoothing: True\n",
      "\n",
      "Using mixup: False\n",
      "Number of no-mixup epochs:0\n",
      "\n",
      "Using nag Optimizer\n",
      "{'lr_scheduler': <gluoncv.utils.lr_scheduler.LRSequential object at 0x7f20fc273110>, 'wd': 0.0001, 'momentum': 0.9}\n",
      "\n",
      "Number of warmup epochs: 5\n",
      "Warmup Learning Rate Mode: linear\n",
      "Learing Rate Mode: cosine\n",
      "Learing Rate Decay: 0.1\n",
      "Learning Rate Decay Epochs: [30, 60, 90, inf]\n",
      "\n",
      "Training Settings Set Successfully.\n"
     ]
    }
   ],
   "source": [
    "if mixup or stitch_mixup:\n",
    "    epochs = 200 # Mixup asks for longer training to converge better\n",
    "else:\n",
    "    epochs = 120\n",
    "    \n",
    "warmup_epochs = 5\n",
    "mixup_off_epochs = 0\n",
    "\n",
    "alpha = 0.2 # For Beta distribution sampling\n",
    "\n",
    "lr_decay_epochs = [30, 60, 90, np.inf] # Epochs where learning rate decays\n",
    "# lr_decay_epochs = [40, 80]\n",
    "\n",
    "warmup_lr_mode = 'linear'\n",
    "lr_mode = 'cosine'\n",
    "lr_decay = 0.1 # Learning rate decay factor\n",
    "target_lr = 0\n",
    "\n",
    "# Sets up a linear warmup scheduler, followed by a cosine rate decay.\n",
    "# Consult the paper for the proper parameters (base_lr, target_lr, warmup_epochs, etc.)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler(warmup_lr_mode,\n",
    "                base_lr = 0,\n",
    "                target_lr = 0.1,\n",
    "                nepochs = warmup_epochs,\n",
    "                iters_per_epoch = num_batches),\n",
    "    \n",
    "    LRScheduler(lr_mode,\n",
    "                base_lr = 0.1,\n",
    "                target_lr = target_lr,\n",
    "                nepochs = epochs - warmup_epochs,\n",
    "                iters_per_epoch = num_batches,\n",
    "                step_epoch = lr_decay_epochs,\n",
    "                step_factor = lr_decay,\n",
    "                power = 2)\n",
    "])\n",
    "\n",
    "# Nesterov accelerated gradient descent and set parameters (based of off \n",
    "# reference papers and default values):\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'lr_scheduler': lr_scheduler, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "if label_smoothing or mixup:\n",
    "    sparse_label_loss = False\n",
    "else:\n",
    "    sparse_label_loss = True\n",
    "\n",
    "print(\"sparse label loss: {}\".format(sparse_label_loss))\n",
    "\n",
    "if distillation:\n",
    "    loss_fn = gcv.loss.DistillationSoftmaxCrossEntropyLoss(temperature=T,\n",
    "                                                           hard_weight=hard_weight,\n",
    "                                                           sparse_label=sparse_label_loss)\n",
    "else:\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"\\nUsing label smoothing: {}\".format(label_smoothing))\n",
    "print(\"\\nUsing mixup: {}\".format(mixup))\n",
    "print(\"Number of no-mixup epochs:{}\".format(mixup_off_epochs))\n",
    "\n",
    "print(\"\\nUsing {} Optimizer\".format(optimizer))\n",
    "print(optimizer_params)\n",
    "print(\"\\nNumber of warmup epochs: {}\".format(warmup_epochs))\n",
    "print(\"Warmup Learning Rate Mode: {}\".format(warmup_lr_mode))\n",
    "print(\"Learing Rate Mode: {}\".format(lr_mode))\n",
    "print(\"Learing Rate Decay: {}\".format(lr_decay))\n",
    "print(\"Learning Rate Decay Epochs: {}\".format(lr_decay_epochs))\n",
    "print(\"\\nTraining Settings Set Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    \n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "    \n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    \n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop started for 200 epochs:\n",
      "[Epoch 0] train=1.000000 val_top1=0.152100 val_top5=0.475000 loss=179960.969727 time: 32.293359\n",
      "[Epoch 1] train=1.000000 val_top1=0.213900 val_top5=0.598400 loss=157662.215210 time: 32.012706\n",
      "[Epoch 2] train=1.000000 val_top1=0.268000 val_top5=0.649000 loss=149371.923096 time: 31.900178\n",
      "[Epoch 3] train=1.000000 val_top1=0.353700 val_top5=0.746700 loss=139892.794312 time: 31.688125\n",
      "[Epoch 4] train=1.000000 val_top1=0.335300 val_top5=0.727900 loss=133986.734314 time: 31.794324\n",
      "[Epoch 5] train=1.000000 val_top1=0.405000 val_top5=0.788700 loss=125524.783936 time: 31.731749\n",
      "[Epoch 6] train=1.000000 val_top1=0.507300 val_top5=0.864700 loss=121413.661926 time: 31.817410\n",
      "[Epoch 7] train=1.000000 val_top1=0.523500 val_top5=0.866700 loss=118927.159576 time: 31.927963\n",
      "[Epoch 8] train=1.000000 val_top1=0.547900 val_top5=0.882300 loss=116309.581055 time: 31.802178\n",
      "[Epoch 9] train=1.000000 val_top1=0.545300 val_top5=0.875200 loss=112836.269562 time: 31.880050\n",
      "[Epoch 10] train=1.000000 val_top1=0.575800 val_top5=0.899400 loss=109585.080597 time: 31.849941\n",
      "[Epoch 11] train=1.000000 val_top1=0.556100 val_top5=0.881300 loss=106066.233521 time: 31.752012\n",
      "[Epoch 12] train=1.000000 val_top1=0.584000 val_top5=0.900100 loss=105789.510162 time: 31.986250\n",
      "[Epoch 13] train=1.000000 val_top1=0.575900 val_top5=0.894500 loss=106154.897003 time: 31.716550\n",
      "[Epoch 14] train=1.000000 val_top1=0.511600 val_top5=0.851400 loss=106172.599243 time: 32.003430\n",
      "[Epoch 15] train=1.000000 val_top1=0.597600 val_top5=0.900600 loss=103943.390533 time: 31.936455\n",
      "[Epoch 16] train=1.000000 val_top1=0.639600 val_top5=0.922200 loss=101274.989929 time: 33.297032\n",
      "[Epoch 17] train=1.000000 val_top1=0.628100 val_top5=0.922800 loss=98932.345642 time: 32.058071\n",
      "[Epoch 18] train=1.000000 val_top1=0.645800 val_top5=0.923800 loss=97885.357117 time: 31.865542\n",
      "[Epoch 19] train=1.000000 val_top1=0.639700 val_top5=0.927000 loss=98098.814789 time: 33.158919\n",
      "[Epoch 20] train=1.000000 val_top1=0.615800 val_top5=0.919800 loss=97175.293152 time: 31.848566\n",
      "[Epoch 21] train=1.000000 val_top1=0.656200 val_top5=0.930300 loss=97232.848846 time: 31.915354\n",
      "[Epoch 22] train=1.000000 val_top1=0.625000 val_top5=0.912600 loss=96832.400085 time: 32.006734\n",
      "[Epoch 23] train=1.000000 val_top1=0.638100 val_top5=0.918900 loss=96466.382629 time: 32.901927\n",
      "[Epoch 24] train=1.000000 val_top1=0.665800 val_top5=0.929000 loss=95394.181152 time: 32.171744\n",
      "[Epoch 25] train=1.000000 val_top1=0.677900 val_top5=0.934400 loss=92444.147156 time: 32.006229\n",
      "[Epoch 26] train=1.000000 val_top1=0.676100 val_top5=0.932300 loss=94820.548401 time: 32.017092\n",
      "[Epoch 27] train=1.000000 val_top1=0.696900 val_top5=0.943100 loss=93963.285339 time: 31.960454\n",
      "[Epoch 28] train=1.000000 val_top1=0.668700 val_top5=0.924400 loss=93743.339172 time: 32.229792\n",
      "[Epoch 29] train=1.000000 val_top1=0.687800 val_top5=0.934600 loss=91487.127625 time: 32.151536\n",
      "[Epoch 30] train=1.000000 val_top1=0.694100 val_top5=0.940600 loss=88970.252441 time: 33.631358\n",
      "[Epoch 31] train=1.000000 val_top1=0.679300 val_top5=0.934000 loss=91983.277191 time: 32.150442\n",
      "[Epoch 32] train=1.000000 val_top1=0.689200 val_top5=0.936700 loss=90099.674805 time: 33.496374\n",
      "[Epoch 33] train=1.000000 val_top1=0.642300 val_top5=0.924600 loss=90174.381287 time: 32.127795\n",
      "[Epoch 34] train=1.000000 val_top1=0.701200 val_top5=0.942900 loss=90663.010223 time: 32.603032\n",
      "[Epoch 35] train=1.000000 val_top1=0.677600 val_top5=0.936200 loss=90401.416901 time: 32.082134\n",
      "[Epoch 36] train=1.000000 val_top1=0.700600 val_top5=0.936300 loss=88378.728729 time: 32.290746\n",
      "[Epoch 37] train=1.000000 val_top1=0.691500 val_top5=0.933900 loss=87823.330200 time: 32.221958\n",
      "[Epoch 38] train=1.000000 val_top1=0.707800 val_top5=0.946300 loss=88564.118225 time: 32.253353\n",
      "[Epoch 39] train=1.000000 val_top1=0.706200 val_top5=0.946100 loss=87773.234650 time: 32.236305\n",
      "[Epoch 40] train=1.000000 val_top1=0.682200 val_top5=0.931800 loss=87758.793823 time: 32.230810\n",
      "[Epoch 41] train=1.000000 val_top1=0.708100 val_top5=0.946200 loss=86906.865662 time: 32.541210\n",
      "[Epoch 42] train=1.000000 val_top1=0.678400 val_top5=0.939300 loss=84603.411591 time: 31.940312\n",
      "[Epoch 43] train=1.000000 val_top1=0.689800 val_top5=0.935900 loss=86338.193726 time: 31.876749\n",
      "[Epoch 44] train=1.000000 val_top1=0.679600 val_top5=0.926200 loss=84737.600952 time: 32.369054\n",
      "[Epoch 45] train=1.000000 val_top1=0.713500 val_top5=0.941600 loss=84919.371002 time: 33.645034\n",
      "[Epoch 46] train=1.000000 val_top1=0.699600 val_top5=0.940600 loss=82586.647614 time: 31.989266\n",
      "[Epoch 47] train=1.000000 val_top1=0.689400 val_top5=0.932600 loss=83375.796082 time: 31.991675\n",
      "[Epoch 48] train=1.000000 val_top1=0.704100 val_top5=0.942400 loss=85574.298004 time: 32.096307\n",
      "[Epoch 49] train=1.000000 val_top1=0.684700 val_top5=0.928900 loss=85196.111877 time: 32.009251\n",
      "[Epoch 50] train=1.000000 val_top1=0.714900 val_top5=0.944300 loss=85526.875854 time: 32.009532\n",
      "[Epoch 51] train=1.000000 val_top1=0.710800 val_top5=0.941200 loss=84513.786469 time: 32.151648\n",
      "[Epoch 52] train=1.000000 val_top1=0.704300 val_top5=0.931700 loss=84446.492828 time: 32.032541\n",
      "[Epoch 53] train=1.000000 val_top1=0.686900 val_top5=0.933700 loss=82929.013458 time: 31.794820\n",
      "[Epoch 54] train=1.000000 val_top1=0.693700 val_top5=0.931000 loss=83655.593964 time: 31.847534\n",
      "[Epoch 55] train=1.000000 val_top1=0.715600 val_top5=0.945600 loss=78842.596588 time: 32.321773\n",
      "[Epoch 56] train=1.000000 val_top1=0.724300 val_top5=0.946300 loss=79449.114349 time: 32.332198\n",
      "[Epoch 57] train=1.000000 val_top1=0.661300 val_top5=0.914600 loss=79204.028778 time: 32.492945\n",
      "[Epoch 58] train=1.000000 val_top1=0.715800 val_top5=0.941000 loss=80943.297668 time: 33.401145\n",
      "[Epoch 59] train=1.000000 val_top1=0.703400 val_top5=0.940200 loss=84263.425446 time: 32.211805\n",
      "[Epoch 60] train=1.000000 val_top1=0.700000 val_top5=0.937800 loss=79461.958679 time: 32.116323\n",
      "[Epoch 61] train=1.000000 val_top1=0.706600 val_top5=0.944500 loss=80319.999115 time: 32.080679\n",
      "[Epoch 62] train=1.000000 val_top1=0.708400 val_top5=0.940300 loss=82527.459106 time: 32.029179\n",
      "[Epoch 63] train=1.000000 val_top1=0.701600 val_top5=0.941300 loss=80127.640900 time: 31.893680\n",
      "[Epoch 64] train=1.000000 val_top1=0.723600 val_top5=0.943600 loss=79068.632263 time: 33.567352\n",
      "[Epoch 65] train=1.000000 val_top1=0.718900 val_top5=0.944200 loss=79190.825348 time: 31.985624\n",
      "[Epoch 66] train=1.000000 val_top1=0.715200 val_top5=0.946900 loss=79011.769409 time: 31.885793\n",
      "[Epoch 67] train=1.000000 val_top1=0.726200 val_top5=0.947200 loss=82969.301544 time: 31.917897\n",
      "[Epoch 68] train=1.000000 val_top1=0.730900 val_top5=0.947700 loss=80608.793335 time: 33.512828\n",
      "[Epoch 69] train=1.000000 val_top1=0.718200 val_top5=0.942700 loss=81687.873016 time: 31.736853\n",
      "[Epoch 70] train=1.000000 val_top1=0.694000 val_top5=0.927500 loss=81105.947021 time: 31.820247\n",
      "[Epoch 71] train=1.000000 val_top1=0.713600 val_top5=0.934800 loss=78776.571838 time: 31.891780\n",
      "[Epoch 72] train=1.000000 val_top1=0.693900 val_top5=0.935700 loss=81759.040436 time: 32.069637\n",
      "[Epoch 73] train=1.000000 val_top1=0.716000 val_top5=0.943200 loss=82059.102814 time: 32.119108\n",
      "[Epoch 74] train=1.000000 val_top1=0.716800 val_top5=0.942000 loss=80287.884094 time: 31.988050\n",
      "[Epoch 75] train=1.000000 val_top1=0.704500 val_top5=0.933100 loss=78765.432312 time: 32.033323\n",
      "[Epoch 76] train=1.000000 val_top1=0.729100 val_top5=0.951600 loss=79597.255676 time: 31.824862\n",
      "[Epoch 77] train=1.000000 val_top1=0.726100 val_top5=0.946900 loss=77322.063141 time: 31.722932\n",
      "[Epoch 78] train=1.000000 val_top1=0.713400 val_top5=0.942400 loss=76874.217682 time: 31.731679\n",
      "[Epoch 79] train=1.000000 val_top1=0.727800 val_top5=0.948000 loss=78905.566650 time: 31.885933\n",
      "[Epoch 80] train=1.000000 val_top1=0.722600 val_top5=0.950900 loss=75973.910553 time: 32.015492\n",
      "[Epoch 81] train=1.000000 val_top1=0.746600 val_top5=0.955200 loss=78547.625519 time: 31.822628\n",
      "[Epoch 82] train=1.000000 val_top1=0.718700 val_top5=0.935200 loss=78411.196899 time: 33.530273\n",
      "[Epoch 83] train=1.000000 val_top1=0.745700 val_top5=0.951400 loss=79650.359192 time: 31.935361\n",
      "[Epoch 84] train=1.000000 val_top1=0.715700 val_top5=0.941100 loss=76657.307434 time: 32.018219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85] train=1.000000 val_top1=0.726800 val_top5=0.942000 loss=78110.879944 time: 31.949689\n",
      "[Epoch 86] train=1.000000 val_top1=0.724900 val_top5=0.943500 loss=74358.573303 time: 31.942442\n",
      "[Epoch 87] train=1.000000 val_top1=0.742300 val_top5=0.952800 loss=74346.252563 time: 32.058476\n",
      "[Epoch 88] train=1.000000 val_top1=0.737200 val_top5=0.947100 loss=73618.073914 time: 33.465186\n",
      "[Epoch 89] train=1.000000 val_top1=0.735200 val_top5=0.947600 loss=76136.959503 time: 31.899421\n",
      "[Epoch 90] train=1.000000 val_top1=0.738200 val_top5=0.949200 loss=79220.204224 time: 33.321655\n",
      "[Epoch 91] train=1.000000 val_top1=0.728100 val_top5=0.940300 loss=73927.732513 time: 32.469772\n",
      "[Epoch 92] train=1.000000 val_top1=0.746700 val_top5=0.951200 loss=76103.478363 time: 33.798953\n",
      "[Epoch 93] train=1.000000 val_top1=0.743100 val_top5=0.949300 loss=75405.713837 time: 32.172733\n",
      "[Epoch 94] train=1.000000 val_top1=0.741600 val_top5=0.946200 loss=72693.828522 time: 32.378382\n",
      "[Epoch 95] train=1.000000 val_top1=0.743000 val_top5=0.950400 loss=77142.376984 time: 32.299177\n",
      "[Epoch 96] train=1.000000 val_top1=0.743300 val_top5=0.950500 loss=73512.495636 time: 32.112304\n",
      "[Epoch 97] train=1.000000 val_top1=0.740600 val_top5=0.948600 loss=73674.384460 time: 32.079642\n",
      "[Epoch 98] train=1.000000 val_top1=0.751900 val_top5=0.956900 loss=75041.410324 time: 32.391833\n",
      "[Epoch 99] train=1.000000 val_top1=0.740400 val_top5=0.946900 loss=71891.845001 time: 34.047691\n",
      "[Epoch 100] train=1.000000 val_top1=0.730800 val_top5=0.944500 loss=71183.999527 time: 32.136830\n",
      "[Epoch 101] train=1.000000 val_top1=0.743500 val_top5=0.943900 loss=72301.222733 time: 32.242706\n",
      "[Epoch 102] train=1.000000 val_top1=0.741800 val_top5=0.951000 loss=73182.870682 time: 32.272230\n",
      "[Epoch 103] train=1.000000 val_top1=0.750100 val_top5=0.951500 loss=72908.962860 time: 32.202012\n",
      "[Epoch 104] train=1.000000 val_top1=0.725100 val_top5=0.945600 loss=73079.115509 time: 32.091969\n",
      "[Epoch 105] train=1.000000 val_top1=0.756600 val_top5=0.951300 loss=76674.843338 time: 32.456982\n",
      "[Epoch 106] train=1.000000 val_top1=0.752400 val_top5=0.950400 loss=70845.806870 time: 32.258287\n",
      "[Epoch 107] train=1.000000 val_top1=0.748800 val_top5=0.952000 loss=71595.709930 time: 32.393511\n",
      "[Epoch 108] train=1.000000 val_top1=0.754600 val_top5=0.950700 loss=74871.341537 time: 32.314180\n",
      "[Epoch 109] train=1.000000 val_top1=0.752300 val_top5=0.948700 loss=75085.183502 time: 32.236043\n",
      "[Epoch 110] train=1.000000 val_top1=0.752800 val_top5=0.952800 loss=71945.245163 time: 32.038498\n",
      "[Epoch 111] train=1.000000 val_top1=0.761200 val_top5=0.954000 loss=76571.950958 time: 33.726668\n",
      "[Epoch 112] train=1.000000 val_top1=0.751600 val_top5=0.948900 loss=71792.629181 time: 32.031476\n",
      "[Epoch 113] train=1.000000 val_top1=0.744600 val_top5=0.949300 loss=68209.702988 time: 32.143975\n",
      "[Epoch 114] train=1.000000 val_top1=0.757600 val_top5=0.953500 loss=68239.923584 time: 31.958972\n",
      "[Epoch 115] train=1.000000 val_top1=0.748200 val_top5=0.942900 loss=70329.074600 time: 32.062671\n",
      "[Epoch 116] train=1.000000 val_top1=0.756100 val_top5=0.951400 loss=68930.202438 time: 32.035024\n",
      "[Epoch 117] train=1.000000 val_top1=0.747500 val_top5=0.951500 loss=71120.396210 time: 32.129424\n",
      "[Epoch 118] train=1.000000 val_top1=0.749400 val_top5=0.950700 loss=70240.107346 time: 31.840715\n",
      "[Epoch 119] train=1.000000 val_top1=0.749600 val_top5=0.948500 loss=71041.835220 time: 32.114615\n",
      "[Epoch 120] train=1.000000 val_top1=0.756100 val_top5=0.947900 loss=69056.915268 time: 32.002200\n",
      "[Epoch 121] train=1.000000 val_top1=0.762100 val_top5=0.949300 loss=69481.171829 time: 31.983916\n",
      "[Epoch 122] train=1.000000 val_top1=0.765100 val_top5=0.956100 loss=71318.141159 time: 32.190284\n",
      "[Epoch 123] train=1.000000 val_top1=0.758200 val_top5=0.952100 loss=70143.510757 time: 31.995043\n",
      "[Epoch 124] train=1.000000 val_top1=0.762000 val_top5=0.953500 loss=70780.887955 time: 32.302028\n",
      "[Epoch 125] train=1.000000 val_top1=0.747700 val_top5=0.948600 loss=68738.880066 time: 32.077271\n",
      "[Epoch 126] train=1.000000 val_top1=0.759500 val_top5=0.952400 loss=68582.463226 time: 31.939837\n",
      "[Epoch 127] train=1.000000 val_top1=0.765700 val_top5=0.952400 loss=68387.984222 time: 31.864088\n",
      "[Epoch 128] train=1.000000 val_top1=0.766600 val_top5=0.952700 loss=71341.769028 time: 31.986205\n",
      "[Epoch 129] train=1.000000 val_top1=0.758400 val_top5=0.948800 loss=66739.933685 time: 32.032851\n",
      "[Epoch 130] train=1.000000 val_top1=0.767400 val_top5=0.953200 loss=67278.904877 time: 32.051073\n",
      "[Epoch 131] train=1.000000 val_top1=0.760400 val_top5=0.954600 loss=62542.937729 time: 31.892500\n",
      "[Epoch 132] train=1.000000 val_top1=0.769300 val_top5=0.954000 loss=69130.658829 time: 31.836753\n",
      "[Epoch 133] train=1.000000 val_top1=0.762200 val_top5=0.948600 loss=64711.577316 time: 32.017518\n",
      "[Epoch 134] train=1.000000 val_top1=0.766000 val_top5=0.953200 loss=66636.139114 time: 33.256352\n",
      "[Epoch 135] train=1.000000 val_top1=0.772200 val_top5=0.956700 loss=67637.064651 time: 31.969571\n",
      "[Epoch 136] train=1.000000 val_top1=0.768000 val_top5=0.952200 loss=66950.266876 time: 31.899018\n",
      "[Epoch 137] train=1.000000 val_top1=0.771000 val_top5=0.956700 loss=63728.752792 time: 31.936786\n",
      "[Epoch 138] train=1.000000 val_top1=0.771300 val_top5=0.952200 loss=69983.942932 time: 32.007074\n",
      "[Epoch 139] train=1.000000 val_top1=0.771900 val_top5=0.952800 loss=63650.080429 time: 32.060130\n",
      "[Epoch 140] train=1.000000 val_top1=0.772600 val_top5=0.955100 loss=67608.733185 time: 31.995137\n",
      "[Epoch 141] train=1.000000 val_top1=0.773400 val_top5=0.955700 loss=66702.405151 time: 32.083027\n",
      "[Epoch 142] train=1.000000 val_top1=0.769200 val_top5=0.956700 loss=65733.514511 time: 31.917896\n",
      "[Epoch 143] train=1.000000 val_top1=0.778000 val_top5=0.954100 loss=67683.224518 time: 32.092449\n",
      "[Epoch 144] train=1.000000 val_top1=0.777900 val_top5=0.956200 loss=65544.244125 time: 33.638150\n",
      "[Epoch 145] train=1.000000 val_top1=0.775500 val_top5=0.954100 loss=63166.840393 time: 32.010236\n",
      "[Epoch 146] train=1.000000 val_top1=0.777500 val_top5=0.953600 loss=68948.274689 time: 31.980037\n",
      "[Epoch 147] train=1.000000 val_top1=0.771200 val_top5=0.949500 loss=63204.538406 time: 31.978963\n",
      "[Epoch 148] train=1.000000 val_top1=0.775700 val_top5=0.955000 loss=65847.951004 time: 32.018033\n",
      "[Epoch 149] train=1.000000 val_top1=0.777100 val_top5=0.951700 loss=64503.431839 time: 32.019539\n",
      "[Epoch 150] train=1.000000 val_top1=0.780900 val_top5=0.953600 loss=65312.970383 time: 31.864742\n",
      "[Epoch 151] train=1.000000 val_top1=0.780200 val_top5=0.954900 loss=61483.736633 time: 32.058444\n",
      "[Epoch 152] train=1.000000 val_top1=0.777800 val_top5=0.953000 loss=70516.690140 time: 31.919154\n",
      "[Epoch 153] train=1.000000 val_top1=0.782500 val_top5=0.954500 loss=65083.948227 time: 31.870755\n",
      "[Epoch 154] train=1.000000 val_top1=0.781100 val_top5=0.954000 loss=64744.230865 time: 31.951184\n",
      "[Epoch 155] train=1.000000 val_top1=0.778500 val_top5=0.956700 loss=65821.993393 time: 31.738151\n",
      "[Epoch 156] train=1.000000 val_top1=0.785600 val_top5=0.955900 loss=63801.887848 time: 32.053875\n",
      "[Epoch 157] train=1.000000 val_top1=0.788800 val_top5=0.953600 loss=61354.881744 time: 32.206851\n",
      "[Epoch 158] train=1.000000 val_top1=0.789600 val_top5=0.956400 loss=60634.098984 time: 33.449905\n",
      "[Epoch 159] train=1.000000 val_top1=0.787200 val_top5=0.956000 loss=63455.502335 time: 31.903830\n",
      "[Epoch 160] train=1.000000 val_top1=0.778100 val_top5=0.953800 loss=63291.912918 time: 31.922063\n",
      "[Epoch 161] train=1.000000 val_top1=0.787500 val_top5=0.957900 loss=61300.317444 time: 31.803466\n",
      "[Epoch 162] train=1.000000 val_top1=0.788100 val_top5=0.958900 loss=64207.419586 time: 31.956566\n",
      "[Epoch 163] train=1.000000 val_top1=0.786500 val_top5=0.956700 loss=62196.854187 time: 32.022439\n",
      "[Epoch 164] train=1.000000 val_top1=0.790700 val_top5=0.958300 loss=64622.008530 time: 31.888042\n",
      "[Epoch 165] train=1.000000 val_top1=0.790000 val_top5=0.956600 loss=64584.511566 time: 31.869302\n",
      "[Epoch 166] train=1.000000 val_top1=0.790200 val_top5=0.956700 loss=61433.910507 time: 31.852065\n",
      "[Epoch 167] train=1.000000 val_top1=0.790400 val_top5=0.957000 loss=60726.429581 time: 31.868769\n",
      "[Epoch 168] train=1.000000 val_top1=0.789300 val_top5=0.957700 loss=62996.689621 time: 31.920262\n",
      "[Epoch 169] train=1.000000 val_top1=0.790000 val_top5=0.955300 loss=59458.497009 time: 33.530555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 170] train=1.000000 val_top1=0.788600 val_top5=0.957600 loss=61174.518494 time: 31.828571\n",
      "[Epoch 171] train=1.000000 val_top1=0.793500 val_top5=0.958700 loss=64335.899887 time: 31.923747\n",
      "[Epoch 172] train=1.000000 val_top1=0.789900 val_top5=0.956300 loss=60109.536148 time: 31.977775\n",
      "[Epoch 173] train=1.000000 val_top1=0.795800 val_top5=0.956500 loss=61957.955521 time: 31.833708\n",
      "[Epoch 174] train=1.000000 val_top1=0.793600 val_top5=0.957100 loss=60627.448120 time: 32.043170\n",
      "[Epoch 175] train=1.000000 val_top1=0.793500 val_top5=0.958200 loss=63458.117493 time: 31.870210\n",
      "[Epoch 176] train=1.000000 val_top1=0.793800 val_top5=0.957500 loss=57985.049423 time: 31.860579\n",
      "[Epoch 177] train=1.000000 val_top1=0.795600 val_top5=0.957100 loss=64862.617599 time: 32.140785\n",
      "[Epoch 178] train=1.000000 val_top1=0.794800 val_top5=0.957100 loss=60743.287125 time: 31.908537\n",
      "[Epoch 179] train=1.000000 val_top1=0.793900 val_top5=0.957000 loss=62882.291229 time: 32.019080\n",
      "[Epoch 180] train=1.000000 val_top1=0.794100 val_top5=0.958000 loss=59822.448807 time: 32.060693\n",
      "[Epoch 181] train=1.000000 val_top1=0.790500 val_top5=0.957500 loss=59566.836670 time: 32.005629\n",
      "[Epoch 182] train=1.000000 val_top1=0.793100 val_top5=0.957300 loss=62010.552277 time: 31.857154\n",
      "[Epoch 183] train=1.000000 val_top1=0.793000 val_top5=0.957400 loss=61268.215439 time: 32.067368\n",
      "[Epoch 184] train=1.000000 val_top1=0.793200 val_top5=0.957200 loss=63797.758118 time: 33.443581\n",
      "[Epoch 185] train=1.000000 val_top1=0.793300 val_top5=0.956200 loss=62416.569748 time: 31.855781\n",
      "[Epoch 186] train=1.000000 val_top1=0.793800 val_top5=0.957000 loss=58320.067352 time: 31.813024\n",
      "[Epoch 187] train=1.000000 val_top1=0.795400 val_top5=0.957000 loss=59474.656631 time: 31.965001\n",
      "[Epoch 188] train=1.000000 val_top1=0.793200 val_top5=0.957400 loss=57867.446198 time: 31.797927\n",
      "[Epoch 189] train=1.000000 val_top1=0.794800 val_top5=0.957400 loss=61471.925598 time: 32.105397\n",
      "[Epoch 190] train=1.000000 val_top1=0.795800 val_top5=0.957400 loss=61006.364990 time: 32.001052\n",
      "[Epoch 191] train=1.000000 val_top1=0.794500 val_top5=0.956000 loss=60893.649826 time: 32.035371\n",
      "[Epoch 192] train=1.000000 val_top1=0.794400 val_top5=0.956700 loss=61945.321930 time: 32.058684\n",
      "[Epoch 193] train=1.000000 val_top1=0.795000 val_top5=0.956800 loss=59956.123230 time: 31.918157\n",
      "[Epoch 194] train=1.000000 val_top1=0.794500 val_top5=0.956400 loss=58252.988739 time: 33.536391\n",
      "[Epoch 195] train=1.000000 val_top1=0.794800 val_top5=0.956400 loss=64122.710693 time: 31.938866\n",
      "[Epoch 196] train=1.000000 val_top1=0.794500 val_top5=0.955900 loss=61537.091293 time: 31.888459\n",
      "[Epoch 197] train=1.000000 val_top1=0.794700 val_top5=0.956100 loss=60344.776764 time: 32.005344\n",
      "[Epoch 198] train=1.000000 val_top1=0.794300 val_top5=0.956600 loss=59033.117340 time: 32.013594\n",
      "[Epoch 199] train=1.000000 val_top1=0.795000 val_top5=0.956400 loss=62769.281204 time: 32.035661\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3wUVfeHn9nspvcGISGEntBLaAIK0ouAioqC7VWwIHbF9lpfy2v/KXZeOyKICqJIlSLSS6ihQwoBUkjvuzu/P042hfQQCBvu8yGf2Zm5c/fuJHzn7LnnnqPpuo5CoVAo7B9DQw9AoVAoFPWDEnSFQqFoJChBVygUikaCEnSFQqFoJChBVygUikaCEnSFQqFoJFQr6JqmfalpWqKmaXsrOa9pmvaBpmlHNE3brWlaj/ofpkKhUCiqoyYW+tfAyCrOjwLaFv1MAz45/2EpFAqForZUK+i6rq8DzlbRZDzwrS5sArw1TQuqrwEqFAqFomYY66GPYCCu1H580bFT5zbUNG0aYsXj5ubWMzw8vNZvlpCeS16BtW4jVSgUiksAZ0cDzbxc6nTt9u3bk3VdD6joXH0IulbBsQrzCei6/jnwOUBkZKS+bdu2enh7hUKhuHzQNC2msnP1EeUSDzQvtR8CJNRDvwqFQqGoBfUh6L8BtxVFu/QF0nVdL+duUSgUCsWFpVqXi6Zpc4FBgL+mafHAC4AJQNf1T4ElwGjgCJAD3HmhBqtQKBSKyqlW0HVdv7ma8zowvd5GpFAoGpzCwkLi4+PJy8tr6KFctjg7OxMSEoLJZKrxNfUxKapQKBoZ8fHxeHh4EBYWhqZVFPeguJDouk5KSgrx8fG0bNmyxteppf8KhaIceXl5+Pn5KTFvIDRNw8/Pr9bfkJSFrlAUoeu6ErBSXMx7oes6ueZc8i35GDQDbiY3Ci2FWLHiYnTBoBnQdR2rbsWiWwBwdHDEqlvJM+dRYCnA0cERF6MLmqah6zpm3UyBpQBd1zEajDhoDhi0EhvWqlspsBagoWHQDBg0Axoa8k/DQXNA0zSsuhVd19GLorFtVd50pN/SfdYndbn/StAVlyy6rvPH8T9Iz09ncsTkKtvmmnNxMdZtoYau67yx5Q22ntnKNyO/wcPRo0792EjISuDg2YMEugXS0a9j8XscSj1Ea+/WGA3V/7fLKczhxQ0v4mpyZWqXqaTlp9HUtSl+Ln7ouk52YTZmq5lCayGeTp44OTiRXZjN5lObOZJ2BEeDIxF+EfQJ6kO+JZ+fDv7E/EPzifCNYGiLoXy480NaebXiiV5PYNSMbD69mY0JG0nISuBMzhlmtpiJR6YHBs1ArjkXZ6MzrkZXjAYjbiY3dF0nOTcZV5MrbiY3cgpzyLPkYbFacDW5ilhaCjBoBrIKs8gpzMFoMOJidMHZ6ExGQQZmixkHgwNuJjdyzbnkFOZUej80tGJBteFkdMJsMRcLPFAsrlb9/BcfapoIvcVqqbKdycGExWpBR0dDq5EQN3Vtirez93mP8Vy0hqopqhYWNU5sf081tS5OZ5/mqb+fYnzr8UxoMwFN07BYLayLX8fX+75mR+IOAJZdv4wT6Sf47dhvvHTFSzg5OBX38fnuz/kk6hOe7P0kk9pPIsecw97kvYR4hBDsHszsPbPZn7Kffs36MSJsBFkFWXwf/T0Z+Rl4O3ljxcp3+78DYGyrsbw+8PXivjcmbGT2ntmk5acxrMUwbom4hbe3vs2p7FN4OHqwP2U/ABG+Ip47E3ey5PgSuQdo3NHxDjr4dWDhkYX8k/APN4ffzPRu03lm/TPEZ8ZjNBhp4tqEboHd6N+sPwGuAZzJPsPb294mKikKg2bAbDUD4G5yZ0qHKfx5/E9iMkrWlvg4+XBd2+tYdHQRybnJZe7v+Nbj2ZW0ixMZJ+jo15GDqQcxW80EuweTkptCnqXkK72/iz8tPFvQ1K0p493H4xPqg46Oi9GFXHNusUiaHExoaBRYCmr0OzYajHg4emDRLWQXZmOxWnAyOuHi4EKhtZAccw4GzUCgSyDuju6YrWYSkhJYtGAR0+6dRq45t/jbk4aG0WDEolvIKsgq7tvJwYk8cx7Xj7+ej7/8GB8fHxw0BxwdHItF2aybsepW3nj5DfoN6MfgIYNxNDiiI5a/VbeWscILrYVYdWuxFa4VraG0/W1raBRaCymwFBRb8zo68q9qXfVy8sLN5FbtvYuOjiYiIqLMMU3Ttuu6HllReyXolyixGbG8ve1tNDRGtRrFyLCRLD66mOzCbG5qf1Otvo7FZMTg7eSNl5NXuXPp+enM3jObk1knGdZiGKNajirXRtd1opKiiPCNwNnoDEChtZC4jDhOZZ9i6YmlbD+znayCLDILM/EwefBM32cYGVY+p9u5gj9r5yw+2/0ZAFc0u4L7ut7H29veZlfSLgJdArkp/CY+3PkhD/V4iD+O/cGRtCPc2fFOHo18FBDBvWfFPfg4+3A27yzuJneyCrMACHQN5KneT/HomkdxM7mRXZiNi9Gl+Ku7r4svZ3PPUmAtYETYCFp5teKTXZ8wptUYhoQOIdecy0sbXiLANYAmrk3YkbgDN5Mb+eZ82vu2J7Mgk/a+7TFoBvYk7SEhOwFHgyO3d7ydgSEDWXRkET8f/hkQMY7wi2Dr6a2E+4ZzJO0Ig5sPptBSyMnskxxOPVzmPhk1I69f+ToRvhGsiFlBM7dmzDs4jx2JO2jj3YZrWl+Dk4MTRs3IsphlbD29lQjfCB6NfJQu/l0w62Y+3fUp3+3/jmD3YJ7v+zxXBF/BsbRjbDuzjfFtxpOYncjymOW4m9zp4NeBTv6din8v0dHRhIeHo6Nj0AxYdStmq5l8Sz5ncs5gsVoIdg8uPuZqchV3Bxq55lwcDA44OThh1a3FYmf7/RdaCzEZTMXHLFZLsTVs48SJE4wdO5a9e8smebVYLDg4OFT+x97IUIJ+CXDw7EGmrZjGW1e+Re+g3gAcSzvGiYwTXB16dZm2K2NW8uHOD3nzyjdp79u++Pi9K+9l++ntuJncKLAWMHfMXK7/7XryLfnc3fluHuz+YPF/iNWxq1kWs4wWni24qf1NOGgOPP3300yJmEIbnzaM+nkUDgYHJrabyGM9H8PBUPIf4vXNr/PjwR9xcnAi1COUBeMWsCFhAzvO7KCZezOuaHYFX+39ih8O/ECwezDjWo8jNjOWv+P/JqMgAwBXoyv9g/vj6+yLu8mdLae3sCd5D49HPs7tHW8vfq/0/HTuX3k/h9MO08GvA69c8Qp3Lb+LMM8wrg69mve2v0eOOQcXowvP9nmW0a1GYzKYmLJkCkfTjpJVmEWYZxgxGTFcGXIlMRkxnMg4QWuv1swZM4eFRxYSmxGLr7MvTd2a8sqmV8i35BPsHswv437heMZxfjzwI7quM73bdILcg8gz53Hg7AE6+nVE0zTe2PIGfxz7o/ihEO4bzuzhs/F09OTb/d/yy+FfeK7vc/Rq2qvM71HXdWIzY3E1uhLgWpJm43j6cSxWC0HuQWhoXP/b9cRnxfPyFS9zbdtri9sl5iQSlRjF2byz+Lv40yWgC4GugWXew2K1sC9lHx39Opb5Heq6zsmskzR1a1rOnXPw7EGaezTH1eRa3Z9tGSoSktLvZxP6C8WkSZNYtGgR7du3x2Qy4e7uTlBQEFFRUezfv58JEyYQFxdHXl4eDz30ENOmTQMgLCyMbdu2kZWVxahRoxgwYAAbNmwgODiYRYsW4eLiwh133MHYsWOZOHEiYWFh3H777SxevJjCwkJ++uknwsPDSUpK4pZbbiElJYVevXqxdOlStm/fjr+/f5lxbtmyhYcffpjc3FxcXFz46quvaN++PRaLhZkzZ7Js2TI0TWPq1KnMmDGDrVu38tBDD5GdnY2TkxOrVq3Cw6NyF58S9AYk15yLyWBi8pLJ7E/Zz9DQobw3+D3Wxa/j8bWPk2vO5e7Od9PMvRnp+enc1ekubvnjFvam7MXD0YOJbSfiYnLB28mb1za/xuORjxPZNJJJv0+iiWsTknKTGBI6hBUxK4jwjeCRno8Q2SSSkb+MJLMgkzxzHuG+4TRzb8aq2FWEeoQyuPlgvov+juEthrP0xFLu73Y/93W9D4Dk3GRG/jyS0S1H08StCZ/v/pwNN2/ghsU3EJcZV+azjW89nn0p+ziSdgR/F3/6BfWjX7N+BLgG0MW/SxnBKLQWMmPVDPam7GX59cuLXSBf7P6C6LPRjG8znmXHl+Ho4EhKXgpvXfkWI1uOJCErgTnRc7i2zbW08WlT3N8P0T/w+pbX8XX2ZeH4hTzw1wPkFObQ3KM5Hf06MqHNBJq4NSn3+/j92O/8Z9N/eH/w+/QN6lvj32O+JZ8jqUdIL0inW0C3WothVRxPP87B1IMVfnu5lCgtJC8t3sf+hIx67b9DM09euKZjpedLW+hr1qxhzJgx7N27tziE7+zZs/j6+pKbm0uvXr1Yu3Ytfn5+ZQS9TZs2bNu2jW7dunHjjTcybtw4pkyZUk7QH3vsMWbMmMHHH3/Mjh07mD17Ng888ADBwcE8/fTTLF26lFGjRpGUlFRO0DMyMnB1dcVoNLJy5Uo++eQTfv75Zz755BNWrlzJvHnzMBqNnD17Fnd3d8LDw5k3bx69evUqc21l1FbQ1aRoLdF1nfjMeJp7lqSvMVvNfLDzA77a+xU+Tj6k5qcS4RvBmvg1rIlbw8OrH6a9b3vaeLdh9p7ZxdcVWgrZm7KX2zrcxsZTG/k++nvMVjM6OqEeodwSfgsmBxMDgwfy98m/GdNqDK8NeI3fjv7G57s/Z8ZfM5gcMZnEnEQ+HvIxOjoz/ppB9NloBgQPYP3J9Xyz/xuGtxjOm1e+idFg5NNdn7L+5HpOZp4k0DWQQmshd3W+i7jMOKy6leUnlhOXGccTkU8wIHgAK2NX4uHowaT2kwDIs+RVO/loMpi4t+u93Prnrby97W2WnlhKZkEmRoORt696myGhQ7i6+dVMXzUdT0dPBocOBqCZezOe6PVEuf6Ghw3nve3vcXP4zfg4+zBn9Jwa/a7GthrLiLARmAw1X5gB4OTgREf/ysXmfGjp1ZKWXjWPK1YIvXv3LhOP/cEHH/Drr78CEBcXx+HDh/Hz8ytzTcuWLenWrRsAPXv25MSJExX2fd111xW3+eWXXwBYv359cf8jR47Ex8enwmvT09O5/fbbOXz4MJqmUVhYCMDKlSu59957i8Xa19eXPXv2EBQURK9e8u3O09Oz1vehOpSg14C9yXvZk7yHG9rdwJd7v+TDnR8yd8xcOvl3Qtd1Hl/7OKtiVzGq5ShyC3MJ9gjmurbXcf1v1/Pw6odp4tqE/w3/H24mN65odgUhHiG8tPElPt71MSaDiamdpxYLWVpeGmvj19LBrwMmBxGiGd1nEJ8Vzz1d7sGgGZjQZgIDggdw/W/X8+XeL2nj3YYBwQPQNI3XB7zOrqRdPNnrSW778zZ2J+/mzk53omkaz/V9jpiMGAothfRo0oO/4//mmlbX0MKzBd5OMuP+xZ4vABgQPIBW3q2Y5j2tzL2oaSRJt8Bu9AjswU+HfiLILYgPBn9AW5+2xX78gSED+e+V/8VkMJWZ4KwIfxd//rz+T3ycKv5PVRW1FXNFeaqypC8Wbm4lE4hr1qxh5cqVbNy4EVdXVwYNGlRhvLaTU8nflYODA7m5uRX2bWvn4OCA2SwT0JV5Lj766CO++EL+jyxZsoR///vfDB48mF9//ZUTJ04waNCg4uvPnee6GGGxStArYOnxpUQlRXE6+zRxmXEcSj0EwNq4tWw+vRmAnw79RCf/Tvx06CdWxa7ikZ6P8K9O/yrTTwe/DkSnRPPqgFdxd3QHYEyrMQA81esp7lp+F4ObDy4TvuTt7M34NuPL9BPhF8FvE34rc8zfxZ/XBrzGjL9mMK3LtOI/lNGtRjO61WgAXun/CtvObKOTfycA3Exu/DDmh+I+8i35GDX5E/By8qKVVyuOpR8j0CWwXqzIR3o+woc7P+SFfi8Q6hla7nxFE7CV4e/iX30jRaPBw8ODzMzMCs+lp6fj4+ODq6srBw4cYNOmTfX+/gMGDGD+/PnMnDmT5cuXk5qaCsD06dOZPr0k00l6ejrBwcEAfP3118XHhw8fzqeffsqgQYOKXS7h4eEkJCSwdetWevXqRWZmJi4uLlW6XGqLEvRzOJN9hifWPYGL0YUgtyCaujVlXOtx5JnzmBU1C19nX3oE9uDP438ypuUY3t72Nv2C+nFHxzvK9fV83+eJz4onsml5d1fvoN68eeWbdAnoUuex9g/uz/pJ6yv18bbybkUr71aVXn+uZdw1oCvH0o/Rt1nferEkugV2438j/nfe/SguP/z8/Ojfvz+dOnXCxcWFJk1K5khGjhzJp59+SpcuXWjfvj19+9Z8fqSmvPDCC9x8883MmzePq666iqCgoAonL5988kluv/123n33Xa6+uiTg4e677+bQoUN06dIFk8nE1KlTeeCBB5g3bx4zZswonkRduXIl7u7u9TZuNSl6DnOi5/DGljdYNGERrbzKiuGfx/8k1DMUq9XKLUtuASDILYjvRn1X4aScvbHg0AJe2vgSrw14jWtaX9PQw1E0IFVFuVwO5Ofn4+DggNFoZOPGjdx3331ERUVd9HGoSdHzZGXMStp4tykn5lDiItB1nR6BPcg15zJryKxy4WX2yrAWwzicerhcaKVCcbkRGxvLjTfeiNVqxdHRsdhvfqmjBL0UybnJbD+znXu63lNlO03TmD1iNkbN2Khyf3g5efF0n6cbehgKRYPTtm1bdu7c2dDDqDWXVbZFq25l/sH5ZBVkUWgt5IMdH7AraVfx+dVxq9HRGRo6tNq+Sq90UygUikuBy0rQNyVs4pVNr/Bd9Hesi1/HF3u+4I6ld7DwyEIA9iXvw9vJm3Y+7Rp4pAqFQlF7LitB//vk3wD8evhXFh5eiJ+zH138u/Da5tewWC3EZsbSwrOFsrwVCoVdcln50NfFr8PD5MGp7FOcyj7FHR3vIMwzjB2JO0jITiA2I5beTXs39DAVCoWiTjR6Cz0pJ4lH1zzKkmNLiM2M5Z6u9xSvVhzXehytvVsDsD9lP2dyzlS4AEahUFza1Gcsd3W8//775ORUnru9On766Sc6duyIwWCgvkO3G72F/u3+b1kRs4IVMSsAGBI6BKtuZX/Kftr6tCU9Px0Q6x2ghWeLBhurQqG49Hn//feZMmUKrq51S9rWqVMnfvnlF+65p+pourrQqC30rIIsFhxaQL+gfgS7BxPhG0GIRwh3drqTt656C5BQvQCXgGJBD/VQFrpC0dDMnDmTjz/+uHj/xRdf5KWXXmLIkCH06NGDzp07s2jRomr7Wbx4MX369KF79+4MHTqUM2fOAJCVlcWdd95J586d6dKlCz//LHnrly5dSo8ePejatStDhgwp198HH3xAQkICgwcPZvBgSSo3d+5cOnfuTKdOnZg5c2ZxW3d3dx577DF69OjBkCFDSEpKAiAiIoL27duX67s+aLQWek5hDrP3zCarMIuHejxES6+WlVZYaeXVqjhHS+ksigqFAvjzKTi9p377bNoZRr1R6elJkybx8MMPc//99wMwf/58li5dyiOPPIKnpyfJycn07duXcePGVRnEMGDAADZt2iRrR2bP5s033+Sdd97hlVdewcvLiz175HOlpqaSlJTE1KlTWbduHS1btuTs2bPl+nvwwQd59913Wb16Nf7+/iQkJDBz5ky2b9+Oj48Pw4cPZ+HChUyYMIHs7Gx69OjBO++8w8svv8xLL73ErFmzzvPGVU2jFPQjqUe4Zckt5JpzGRA8oDgValU5Tzaf3oyPkw+ejvWf0lKhUNSO7t27k5iYSEJCAklJSfj4+BAUFMQjjzzCunXrMBgMnDx5kjNnztC0adNK+4mPj+emm27i1KlTFBQUFKfgXblyJT/++GNxOx8fHxYvXsyVV15Z3MbX17facW7dupVBgwYRECBFTSZPnsy6deuYMGECBoOBm266CYApU6YUp+m9kDRKQT+YepBccy5vDHyD4S2GV9u+tZdMjKoJUYWiAqqwpC8kEydOZMGCBZw+fZpJkyYxZ84ckpKS2L59OyaTibCwsHJpc5999ln++OMPAKKiopgxYwaPPvoo48aNY82aNbz44otA7dLbjhgxgjNnzhAZGcns2bPLXVNTLkY4dKP0odsK5Q4MGVicU7wqbBkJlf9cobh0mDRpEj/++CMLFixg4sSJpKenExgYiMlkYvXq1cTExJS75tVXXyUqKqo4kVbp9LbffPNNcbvhw4eXcX+kpqbSr18/1q5dy/HjxwGKXS7Lli0jKiqqWMxLp/bt06cPa9euJTk5GYvFwty5c7nqqqsAsFqtLFiwAIAffviBAQMG1Ov9qQj7E/Szx2Dvz1U2SclNwdHgiIep8lp9pWnt3RoNTVWSUSguITp27EhmZibBwcEEBQUxefJktm3bRmRkJHPmzCE8PLzaPl588UVuuOEGBg4cWKZ83HPPPUdqaiqdOnWia9eurF69moCAAD7//HOuu+46unbtWuwuOZdp06YxatQoBg8eTFBQEK+//jqDBw+ma9eu9OjRg/HjpZ6Bm5sb+/bto2fPnvz11188//zzAPz666+EhISwceNGxowZw4gRI+rhbgn2lz73n/+DFc/DU3HgXLG/+9n1z7Lt9DaWTVxW4243n9pMB78OeDjW7CGgUDRmLvf0ufWBu7s7WVlZ59VH40+f6ylfn8g4WamgJ+cm4+fiV+G5yugT1Od8R6ZQKBQNiv25XLxCZJt+stImdRF0hUKhqE/O1zqvC/Yr6BnxlTZJyU1RNSgVCsVlh/0JuntT0AyQXrGgW6wWUvNT8XNWFrpCobi8sD9BdzCCR1ClLpfU/FSsulVZ6AqF4rLD/gQdZGK0EpdLSm4KgBJ0hUJx2WGfgu4VXKmFbltUpCZFFYrLB3tKn/viiy8SHBxMt27d6NatG0uWLKm3sdVI0DVNG6lp2kFN045omvZUBedDNU1brWnaTk3TdmuaNrreRlgRnsEStlhBDL1N0P2dlYWuUCjqn/MVdIBHHnmkeEXr6NH1J5fVCrqmaQ7AR8AooANws6ZpHc5p9hwwX9f17sAk4GMuJF4hYM6DnPLZ0FLyxOWiLHSFwn5pzOlzLyQ1WVjUGzii6/oxAE3TfgTGA/tLtdEB2yofLyChPgdZjuLFRfHgVla4k3OTcTG6VJpZUaFQ1I7/bvkvB84eqNc+w33Dmdl7ZqXnG3v63FmzZvHtt98SGRnJO++8g4+Pz/nczmJq4nIJBuJK7ccXHSvNi8AUTdPigSXAjIo60jRtmqZp2zRN23ZeTyuvorevwI+enJusJkQVCjundPrcXbt2FafPfeaZZ+jSpQtDhw4tTp9b7HrNzwJzPiQdhMRoSI8n/sAORgwZROcO4bz1xqvs27EZkg6yctkSpt96LaTFQn4mPl4ebFq7giuv6E3LAHfISsTX0QyZp0Vnkg9D4n4Jl7ZaZP/UbrYuX8Cgfj0JMGRiTDvO5HFXs27ZIkg+Iulzh/SA5CNMGTOQ9Wv/gpSj3DdxKEf3bicqKoqgoCAee+yxertvNbHQK3r8neu8vhn4Wtf1dzRN6wd8p2laJ13XrWUu0vXPgc9BcrnUZcAAeBUVoaggFv1s7lkVg65Q1CNVWdK1xpwPualgKYDMU+DsAybnknPmfFln4mBi4phhLPhqFqeTzjJp7NXM+fRtkuKOsn35fExOLoR17U9e4lEwZoJuhZTDPPvGR/yxaj1oGlHL5zLjsZk8Ou1Wxo0YzJote3jxrQ9BM6BbLWgFWTKWnBRAQ89KRDPnlougG3HLdM4kpxLZrTOz33xG3ktzABcfdKOzPFAsBTJuzUEuKi19uhWsZjR0sBbSJCgIjCYwGJg6dSpjx46tt9tbEws9HihdxieE8i6Vu4D5ALqubwScgQtnJrv6g4NjhaGLibmJBLgGXLC3VigUNcR6jqilxYqVm3kKctPE+k2Kltc5Z+Xc2aOQItbwpDFX8eOipSxYvIyJ14wkPTObwMAATAZY/dcqYuJOQl4GFOaApoFnMK++8DRRu6KI2hsNzbqRnmshuPMACOrCNwtXgtEZ/NsyfPQ4Zs1bAU06g2cIqYUm+g2bwNotezie4wZNOnPWMRiadmXZmg1E7Y1m9vfzIagLHt6+ZDoGgHdz+gy5hrWbd5Js8Mfi25q5i1dx1cgJENBO0ueu2QUB7fhh2WYGDBoKAeGcKnAtzkP166+/0qlTp3q75TWx0LcCbTVNawmcRCY9bzmnTSwwBPha07QIRNAv3AyAwSATo2mxZQ7rus7p7NP0b9b/gr21QnHZoOtgKSyymKuQioJsyEoEdDA6gaMHZCdCfqYYXk4eYDFDfjq4BYB7oBy3FMDZ45BWlNfc5AaezcSlYc6l48AIMnNfJDg0jKBO/ZnctD3XXHMNkaOn0K1bN0mfGxAOTVsAmvTrHlhmaLb0ucHBwfTt27c41/lzzz3H9OnT6dSlCw4ODrzwwgtcd911kj73hpuwWq0EBgayYsWKsp9VMxSnzw0KCmL16tXF6XN1XWf06NEVps/18vJi3rx5ADz55JNERUWhaRphYWF89tln9fHbkuHVJH1uURji+4AD8KWu669qmvYysE3X9d+Kol6+ANwRd8yTuq4vr6rPOqfPtfHtBMhLh2mriw+l56cz4McBPBH5BLd1vK3ufSsUlyO6Li4IJw+iDx4iIsAo1i+A0QUMRjDnimi6+ELWmSK/da64GhxM4jJBl4eAq7+Idn6GWOieweUEF3MBJB+U9w4IB6PjRf/YF4pLNn2urutLkMnO0seeL/V6P3BxzWKfMIj+rcyhU9mnAAhyD7qoQ1EoLjnMBSLOHk1kPyMBFk2HJh2h2xTY96u4N/LSxeWRnQgZp8CSD16hMGAWeHtKmg00aWctFJdFRgJknAZ0cHSXNm4BYHAAq1lE3uQi1jqIxW3Jh4oiz4yO4N+u5LXivLC/fOg2fFrIZEZ+pnylA05nnwagqWvlRWMVCrtA1+H0bklGZxPl0hTmwc7vIPkQBHWF7lNgyxdwKgpG/hfmToL4bXDLPBHXn+4QP/XR1bBBJtBh+OgAACAASURBVAbxDgVnb/HnBkdCRDOxoKPmyjoPj/bgUfR/yTYGXRcfeGGuWNy2CU0bBiO4eJ9zzAEMVYQR24S/kdEQ6XPtV9C9W8g2NQaayqSCstAVlwxn9omxEdq39tee3AF/PAYJO8AzBG6eC0dWQmAEtB8lwvzjZIjdIBazOQ/itsCObwEdDq8Qd4hHEHx/vVjWHs3gruXiE4/dABHjxCiqiD73QfQ+dLfA8iFumiZ+bsUFpy7V5OwzlwuU/DGmlRSKPZV9CpPBhK+zbwMNSnFZELsJ4rdXfj7pIHw5Er4ZB6dl4QrmfFjxgsRHA8RtFcE/l8Mr4OsxIshDnhcf9mcDYdVLsPA+ier48RY4uQ2u/x88fRJaD4Ed34g7ZfTbMkHZexrc8ze0GQqDnoYHtkJQFwjpCVfMqFzMARyMOLt5knL2bJ1ERXH+6LpOSkoKzs7O1Tcuhf1a6D5FBZ1TTxQfOp19miauTTBo9vucUjQQGQkScRFWyVSQTdh0HebeDLlnoevNMPgZ2L8I1r0lro+AcDi4VNwIJlf46U6Ytkas53/el7a97oblz0KLAXDbQplMBNj5Pfz2oAjz5AXi5mg3CjZ/AsE9YfFDYnHHb4Fxs6DzRLnuhq9h/XvQ8w4R6ohrwL2JWNO3/Fin2xESEkJ8fPxFWa6uqBhnZ2dCQkJqdY39CrqLj4RHpcYQlxmHp6Mnp7NPK3dLYyMvQ+ZKfFuKmFotVYfQVURGgrglfFtVfN5ihjk3QuI+Ed+grudcfwq+HSd+6najRMxbDIC9P8PueRLBETZQ4qoTdolLYuKXYl1/NwF+uElirAMixOe9/Fl5HbMevrlGPl92svTb+mq48dvieSGadIBxH8rr6MXiegnpDd0ml4zP2ROGvlCy73H+c0gmk4mWLVuedz+Ki4v9CrqmiTWSFsP0VdNp4dGCU9mn6NWkV0OPTFGfrPi3WLWPH4Etn8uE3sO7S6xakIiO+C3Qor/8XZRG12HODeLT7nS9+KBNLrKflw5u/hKVcWaPWNS/Pwp3rZB+/vk/QId9C0WIo34A16JVyGPfBUc3+OcDEfD+D5V/b4BrP4dfp4no3/qr+MWPrIJrPxOrevNn0OIKaHmlTFL2vb/sZyvN1f+Wh8uYd2QthkJxDvYr6CATo2ePcUbXickQX3pTNxXh0miwWuHAEgm/OxUlYaqZCeKXDu5R0m79e7DmNWg7QsTOoykc/FMe+FYLnNkrgnloKexdUHKdyQ0Ks+V1u5HQ8Vr49R7YOluuXWmzejXp+/Ay2PsLOHuBX1sR1dFvVv0ZutwgFnTGSWjWTX4i/yXnhvxbfmpKs25w/4aat1dcdti3oPuEUXhsNTnuJVkGlMulEXEqSuKjQcQ4fqu8jt1UIugWM2z/WtYlHFsD73cSV1xBpljTYQMlEuTG78SiPrNPIj2adARHV/GbH/xT/NFuAbBnASx/TvzXvq3hjt9lJaTVIoJ+dJVMNNbGQm43oh5vikJROfb9vc2nBenWvDKHVAx6A2IugDVvwLfjxY1REyxm2P4NJB8pOVaYJ1b54eWAJqF7mz+XRSuaAWI3lrQ9vFys9uGvwvRNMOxl6DAOrvlAhHj/QggfK7HRDiaxcpv3EjEH8c33u1/irzUNJnwibdNipS/PZuDfFgLayzhAfNgKxSWIfVvobgGkGSS7Wd+gvmw6tYkwz7CGHZO9ErcFnDwhMLxm7fOzJLTOr7XsF+bB16PhZFE434YPYfDT1fdz6E9Y/KC87nW3uEx+uRuO/CWuipBICOoGW7+Q/B/tRoqFvv1rWPmSHPMIkuMORvFl2zDnwZ9PQs/ba3wbcA+QxTjH10H4mJLjmgZtisIDmytBV1ya2LeF7hZAetFX3zs73snC8Qtp7tm8mosU5YjfDl+PlQnImrLsafiwJ/z5lIj732+LmF83GzpMgA0fyARedRxeLg+S7lPEd73pE4nmcHSVFYntRkDYAGkb0luiQLITYcmT4lJx8oCBj1Uc+dJ7Gjy4U/zntaFZ94onOXvcJtEtStAVlyh2bqH7k+4ggu7t7E1r79YNPKAGQtfFHVFZdETcVvEJe4eWP5eVBPMmS66NtKI6JkdWip+5/aiK+7MUiuh6NpMY6b0LxEXSZZJMAob0hAN/wMZZMOJVWVRjMMoS8J3fS9RKQbZEehxeCa0GyYKYo6th6VMi8NO3QMwGEfDCXLHE2wyB0H4yBoMDTFkgvvPK0LTKQxXrQkgk3PlH/fWnUNQz9m2hu/oXW+heTl4NPJgGZPlz8OlAEdpzycuAr0bCh5Hi37Zayp7f+a1YwmEDJRIDYNXLsOBfEr9dETH/iICPehPuWimLaTyCYMRrct63FbQeDAeXyMPm88Gw5HER9j+fkonJk9th/q3i/247TEIJr35Oru9zD7j6QsRYsdTd/OD+TdDvAfFltxgAI1+vWswVissQ+7bQXX1JL/Khezt5V9PYzji1S1YGBoRDv+nQtDOsKEpwOezlsm2P/iWFAnbNFbcAiJBqmkSGWM0Q0h3WvC7L0q/9rCSzXfRiSczUdhic+FseAKknZFHMqpfh2k/l2F//EcvdzV/E2+Qq1rOjq0SC2N7PRtvh4k7ZNVcW7KTFQMurJPpk4pci6GvfkLZthsq2yySJRGk7vPz98Cv17UtZyQpFhdi3hW5wIM3JFSMarkY7LQqdmwbH1sprSyGc3iuv/35HxDf6d1j0gJzb+qUsIS9dCaYgG5KKCviue0siTbJT4O124t6I3SSRIbf+Kg+Cfb9IXhCQSI6EnbJU3FZ4OzG6aMFNoIjxypdkNeO2/4mont4rkSNthpZEikB5f7NNlJcWTYwWZMlrRw9odRX0f1Des2nnkmRPBgN0GC/WukKhqDX2LehAuqMLXjhUWfn7omO1SgrSgpzq2277UpaVnz0OW/8Hn/aXCJHo3yXqY+AjEo994A+xbnNTpSCAjVO7ZBVir6ki0Fu/EL92dqKkU43bJKLp5CETfZ2uhx3fydiif5c+Sgu6LSRw2Euy0Gb9e/LAmPQDTP4Jbv1FXB2Rd1b9uXxayLeLvDTpxzNY3CvthhdVtXETy/7Gb2t9exUKRcXYt8sFSDc54qVX4DtuSI6ugoX3ilXae6q4I9b+Vyzu6/9XdlGKLbnY4eXi/gDxiWsGuTYvQ1wfK0vl6ojdKKlUoSRM8KqZUjR71csygejoIQ8CgxEi7yq5NvIuyUGya678BHYUy9s2oRpTtBIxsAN0uwUGHxa/uy2cMbQvPLSrZveh7TB5GHS9WSZk//k/eXjYqM8JS4VC0QgE3cEB78L8hh5GWfb+ItsTf4uVvWg6RM2RY51vAN0iFve1n0J6UWTJrh9FgLtNEXFvdZWIoK6LdZt6QjLupcdDzEax6NPj5LxXc4mfHvcBfNxXkj3dMl+KHFjNZXNyt7gC/NvLJKVuheu+kOO2yjSxm2Tfll7Vv23d70PvaeDgJOldm3aRRUTtKomcUSgU5439C7oGQYUFDT2MEsz5ItYAJ9bLT9Qc6P+wlP1a9bIIcUGWRIXYQgUTdsi2179g1H9Lqrhomvijt38lwph8SKJHCmwrMbUSq9c9ECbNlUo37UbIpOWRlWUFXdOg772ShGrcLOhyoxx3MEnK1azT4OQl2SzPF+/QklwlnkEw8rXz71OhUFSK3fvQ07DgVZhfdqKwIbBaRTyj5kh18w4TxFJe9ZK4P66aKROBSdElYpx8WCzuoG6y794UgrqDk3vZmPIO42TbbqTEYRdkiR+77QhAL5uoKrSPuGpAwgAHP1e+wkzPO+HJY9Dj1rLHbe18KohXVygUlzx2L+gZ1kK8rRaZLGxIdnwjxQd+f0TqNNpiquO3QsfxEhHSbYpEcYz8r5yL2yQV0ztPFOu4w/iKkz61vhoe2S8LdtoOk2x/Y9+D8R/JNRHjKh5Ts+5w1RPlj2uaxHmfi1fRxKitvJ9CobAr7Nrlkm/JJ1c342WxQk6yLEBpCHJT4a9XoHkf8ZH7hIFfG/Ftp8fJpCBIQd0bvxVf8vLnZGUkSPWl+zZIBfXKsImtX2uYGVMSJlifUSK2SBe1YEehsEvsWtDT89MB8LJaxRedsBO63FRxoYELybq3pXDvrW9L3UYb7UbA8b8h9Iqy7R2MkuXPFiLo3VwW7NSUC/X5PJWFrlDYM3Yt6Gn5aQB4WSziqz69G5p0gqadLt4gdF1yaEeMLSvmACPfkAVBFblR/NrIBCeIJX8pUOxDV4KuUNgjdu1Dt1no3lariDlIXpILSXayLIM3F0XWpByRyJDWV5dv62Aqu5qyNLal7Ca3+okoqQ9aDYLON5aNilEoFHaDXQt6Rn4GUORysXGhBX3vL7LEPuYf2T++TrZhtUzR6lcU3+0VcvFdRJXh5g/XfyGTrgqFwu6wa0FPzZfIFi+jm4QGglRer09O74HMMyX7iftlG7dFtif+lkU5frVM3evXRrbel4i7RaFQ2D12Legns05i1IwENOslsdeufvVjoeu6bHPOwuyh8OkAOFm08KdY0DdLuxPrJfVsba1s/1IWukKhUNQDdj0pGpcZRzP3ZhivK6rkfmjZ+VvoVit81Bu6T5biwuY8iSv/5hrJYZIYLe3it0le7+wkaDmw9u/jFiC1LtuNPL/xKhQKRRF2LeixGbFlS855ND1/Cz09FlIOw+rXZOVmcE+ppvPFYNj2FeRnQEgvWTD0+yOSq6TNsNq/j6bBpDnnN1aFQqEohd26XHRdJy4zjubupQU9qKy/uy4kFuUWtxSKuPe4XZbmuzeVtLQAPe+QbfwW6DNN8pQoFApFA2O3gp6Wn0ZWYRahnqXyjng0lUr055ZZqw1JRS6VEa+BfzvJH24wyJL7nBQ5Fz6mqECxFwx4tO7vpVAoFPWI3bpcYjNjAWjucY7LRbdIrLhHk7p1nHgAPJpBv/vlx0a7kbDzO1lN6eIjgu/kWXFOFIVCoWgA7FbQ4zIl7WyoR2kLvcj1kXmq7oKedKCkmENpWg2SwhGBHWS/66S69a9QKBQXiBq5XDRNG6lp2kFN045omvZUJW1u1DRtv6Zp+zRN+6F+h1meuIw4NDSCPYJLDno0lW1dI12sVlmOH1CBoDu5y+Ro/4fq1rdCoVBcYKq10DVNcwA+AoYB8cBWTdN+03V9f6k2bYGngf66rqdqmhZ4oQZsIzYzliZuTXBycCo5WCzodYx0SYuRavcVCTpAz9vr1q9CoVBcBGpiofcGjui6fkzX9QLgR2D8OW2mAh/pup4KoOt6Yv0OszxxmXFl3S0gOcWh7hZ6UlGEi61ep0KhUNgRNRH0YCCu1H580bHStAPaaZr2j6ZpmzRNq3C1jKZp0zRN26Zp2rakpKS6jdg2iMx4QjzOWWXpYJIFO1nnKegB7c9rbAqFQtEQ1ETQK1rTrp+zbwTaAoOAm4HZmqZ5l7tI1z/XdT1S1/XIgICA2o61dD+k56fj41RBlkL3pnWPRc84JYmpVHIqhUJhh9RE0OOB0hmkQoCECtos0nW9UNf148BBROAvCPmWfMy6GfeKKvy4eNe9HF1OMrjWotCEQqFQXELURNC3Am01TWupaZojMAn47Zw2C4HBAJqm+SMumGP1OdDSZBVKkWV3U0WC7gN5adV3kpcBn/SH3fNLjmUny4IhhUKhsEOqFXRd183AA8AyIBqYr+v6Pk3TXtY0zVadeBmQomnafmA18ISu6ykXatCZBZkAdbPQc4vE/sR6OLMXFt4Hx9bKsZyU2pWCUygUikuIGi0s0nV9CbDknGPPl3qtA48W/VxwsguzgSos9NxUSW17bkrbs8dgVi+44RuI2SCJtXxbwsL74dF9YqE3634RPoFCoVDUP3aZy6XYQq9M0C0FUJhb/tzxdWA1w45v4cQ6aN4bOt8AGfFQkKMsdIVCYdfYpaDbLHQPW5Wi0jgXBddU5HaJ3SzbIyvh9F4pTGGrdJ90AKyFalJUoVDYLXYp6DYL3c3kVv6kreByRYIet0lqeeoWQJfCFLZK97Yi08pCVygUdopdCrotyqVCC90m6OdGumQlig+9x60Q2BGMLlK8wmahnyoSdGWhKxQKO8Uusy3aBN3V5Fr+pEsFLheLWWqAAjTvC816SN4Wo1NJcYpiC12FLSoUCvvEPgW9IAsXowsmg6n8yWKXS5GFfmY/fDYQnDwkqqVZNxFyiuqAOrqJ3/3MPtlXFrpCobBT7FLQswuzK45wgfKTotG/SQUjz2Bo0rFIzM/BMxgSiwRd+dAVCoWdYpeCnlmQWfGEKIglrjmUCPrhFeIrn7qq8g49m4mgm9zA5FL/A1YoFIqLgN1OilY4IQqymMi2/D87GU5uh7bDq+7QFumi/OcKhcKOsVtBr9RCh5Ll/0dWAboUeK4KW6SL8p8rFAo7xj4FvaAKCx1Klv8fXib50YO6Vd1hsYWuBF2hUNgv9ino1VroPpCVDNG/Q8Q4MFTzMW2Crix0hUJhx9inoBdkVR7lAhLpkhEHlnzoc2/1HdpcLsqHrlAo7Bi7E3SL1UKOOadql4uTp+Q7bz0EAtpV36lXMDg4gldo9W0VCoXiEsXuwhazzZKYq0qXS0EWoEPkv2rWqZMH3LsevFuc/wAVCoWigbA7Qc8qqCKPy7m4N6l5x6owtEKhsHPszuViy+NSpYVu49wCFwqFQtGIsT9Bt1nopiosdN0q25rUFrVRkA1W63mMTKFQKBoW+xN0W4HoiuqJ2nD2lG3U3Jp1WpgL73aA3fPOc3QKhULRcNifoBdZ6FWGLRZkg6M77F8IGQnVd5p+Uqz5pAP1NEqFQqG4+NifoNfEQs9NldhyqwW2zq6+04yTss1JrocRKhQKRcNgv4JelYWemwYeTaDNUNjzE+h61Z3arPjslHoapUKhUFx87C5scVTYKNr5tMPFWEWa29xUCUNsNQiOrIDkw1UvMFIWukKhaATYnaAHuQcR5B5UdaO8NMnnYsuyeHh5NYJus9CVoCsUCvvF7lwu1aLrYqG7+IB3KAREiKBXhU3Qc5TLRaFQ2C+NT9ALc8BSUFIsuu0wiNkAP90Jq1+v+BqbyyU/A8z5F2ecCoVCUc80PkG3FYe2FYsOHwPWQtj3C2ycBRZz+WsyEsBWcFpZ6QqFwk5phIJeVEvUViw6tC9M3woTPpGkXYn7ZJL0xHo5X5gnk6GB4bKv/OgKhcJOsbtJ0WqxCbrNQgeZEDU5y+vYzWKtJ+6HJ49D5ik53rQrnN6jIl0UCoXd0vgEPe8cl4sNr+bg0Qz2LoC4zXLs9G4oyJHXQV0gCsg5e9GGqlAoFPVJ43W52CZFbWgahPYpEXMQt4stwqVpF9kql4tCobBTGqGgV2KhAzTvI9vQfuDXBo7/XRLh0qQjaAblclEoFHZL43O55KaCwSjJuc4lbIBsu06ChJ2w52fIzwQXX8nQ6OKrLHSFQmG3NE5Bd/auuLhF084wbY1MgDq6w/avIXYDjP9Yzrv5KwtdoVDYLY1P0G3L/iujWXfZtrwKTG7Q/0HoPlmOufqrBF0KhcJuqZEPXdO0kZqmHdQ07YimaU9V0W6ipmm6pmmR9TfEWpKbWn5CtCLcA2DmcRhU6uO4+SkLXaFQ2C3VCrqmaQ7AR8AooANws6ZpHSpo5wE8CGw+99xFxeZyqQlGp7L7rv7Kh65QKOyWmljovYEjuq4f03W9APgRGF9Bu1eAN4G8ehxf7clKBPcmdbvWoynkni2JTVcoFAo7oiaCHgzEldqPLzpWjKZp3YHmuq7/XlVHmqZN0zRtm6Zp25KSkmo92GqxWiDrjAhzXfAvSrGbfKj+xqRQKBQXiZoIegXhIhSXANI0zQC8BzxWXUe6rn+u63qkruuRAQEBNR9lTclOAt1ad0EPjJCtqi2qUCjskJoIejzQvNR+CFC68rIH0AlYo2naCaAv8FuDTIxmni4aUTUFMCrDt5VkXUyMrvj8sbWQpKx3hUJxaVITQd8KtNU0raWmaY7AJOA320ld19N1XffXdT1M1/UwYBMwTtf1bRdkxFVxvoLuYAL/thVb6OYC+PEWWPVS3cenUCgUF5BqBV3XdTPwALAMiAbm67q+T9O0lzVNG3ehB1grbJkT6+pyAQgIr1jQY/6R9Ltn9hXtb4CEqLq/j0KhUNQzNVpYpOv6EmDJOceer6TtoPMfVh3JPA1o4B5Y9z4CwmHfr3DgD4j6ASZ+BUZHOLRMzqeegIJs+HkqoMOM7WCqomC1QqFQXCQaV3KuzFOyfN/BVPc+AsMBHRbcBQd+h0N/yvHDy8DkKueO/gUZ8ZLYa8vn9TFyhUKhOG8amaCfPj93C0hRaQBzrixQ2v41JB+Bs8eg5x1ybvvXsvVpCX+/U5LhUaFQKBqQxiXoWafrPiFqw7eVCHnPO6HvfXB0Nfx8F2gO0OceMLrAkVWyHT8L8tLhyMq6vdeqlyFu6/mNV6FQKIpoHIL+51Ow7cv6sdAdjDBjB4x5B7pPkayNSQdg4pfgEwYB7QEdgntA877g5Akn/q79++ScFet+97zzG69CoVAU0TiyLe7+ERwcZdn/+VroIEm6ALxC4MZvwTsUgrrKsSYd4VQUNO8t4t/iCimUUVtSjso2Pa7qdgqFQlFD7F/QLeaSsnNw/hb6uURcU3bftpo0pLdswwbCoaWQfhK8gqkxKUdkmx5//mNUKBQKGoPLJfecos7u9Szo5xI+FiLGQcuBsm/b1tbtknJYtmmXkIVekAPr3pJFVAqFwu6wf0G3pbv1biFbjzpmWqwpvi3hpu/AyUP2m3SWSdTj62rXj81Cz0+XidVLgSMr4K//1G1OQKFQNDj2L+i2ghRDnocBj0p5uYuJwQDtR8PenyEttuy5tW/Bd9dVfF3KUYmcgUvH7ZJeVDA7LaZhx6FQKOqE/Qu6zUIPjIChL8hE5cXm6mcBDZY9Iwm8bMm99syHo6vg1O6y7a1WEfSQovxll4qgZxQJeqoSdIXCHrF/Qc8pqgHqdgHS8dYUrxAY8AhEL4Zvx8H310PGqZK86rt+lK3VAn+9KpOo5lxoNViOp8XCx/3Eoq+OjFMweyic3iM+77/+A1n1lFs+Q1noCoU9Y/9RLjYL3cW3YcfR/yHxq2edhn/+D9a/K8d9W4ulPuxlOLAY1r0JDkWl71pcIel6D/wOifvBnAdXPi6x76U5uQO+GQdT/4LYjRC/FX6bAc16wLb/gaufLII6l7RYubbjhPLndL38+2QUZUVWFrpCYZc0Ags9GVx8GsbVUhqTM/S7H658AozOsHU2OHrA0Bel8EbU97KQyCMIrGa5xr+dhDoeWyP7Z4+JVa/r8mNj789QkAkH/xBB1xwgYaeIOYho20g9IStZAVa8AD/dLouYSnPgD3invbiHSqN86AqFXWP/gp6dLMWdLxWcPKDtMKmc1KIftB8FLfrD4ofETXL1czD8FbGuPZqCV1HtEN9Wst09D764WtIC2Di8XLZHiyz09qNkIrZpF2g9BBKKBL0gG767FuZMhFO74GBRgsy4c+p2r39fSvX9cGNJdI7VIsnNjM7ixsrPOv97kZsGu+fLwyk/U3LgWK3n369CoagQ+xf0nBTJsHgp0fFa2YYNkMyPU36BTtdLiGPnG6HfdJi2Wlwe3qHStvsUWY369zsi0Js+Fsv67HGx2l39JAd76glx1dw0B6aulodGyhERz+XPSXvNAebeLC4cgNhNJWM7vRfit8DAx+SB8tercjzrDOgWCOkl+zYrPT+r7snHNn8Kv0yVh9DmT+WhdnRV3fpSKBTVYv+Cnp0sYncpET5WJkm73iz7JmfJBXPv35JbvTQ2C73dSGg/Rl73uE3EePvXJdb54GdKXDWh/SRc0sEolj5ILpttX8rDosdtMsHp21oEurSgb/9KfPj9HoBuk8V6zzxd4j9vcYVsU47C6tfg3Q4wK1IeBCAPjDk3yjeA6rCNffc82P2TvI5eXHI+L0N+FApFvWD/k6I5yRDap6FHURajk/jOz+XcSUgQy9zNHwI7SLSMf1ux8FNPwMaPpHiGX1sR36XPgMEorhYbzbrLdvWrMjE86Cmx7Hd+D90nS1qEzZ9BYZ5khdzxrXxbcPWVtAarX5VJWZvbyibof70i3wzCx4qP/uvR0P1W2PsLZCbImCd8XPk9yEqS6xwcIWouWPJlTuHgErC+J26Yr0aDizfc8XsdbrJCoTgX+7bQrVYRr0vJh15bvJtD76ki9s5e0Ok6eT3gUSjMlepLI14TYW8/UvzzpSeAXX3F/241Q/8HxYfv0wIe2gVXPCQZIS0FsPhBmH+rPAxGvCbXBoTLwyJ6cYmF3rQLmNxEzFsPgUlz4F9L5cGx5XN5nxb9RZgthZV/riMrAB2umilibjDBsBdlgjhuM+z4Gs7skdJ+pSdtE6OVn12hqCP2Leh5aeL3vdR86PVB68HwbIKEKrYbLsdu+AZu+Lp829ArJA6/19SSY55BIvyhfWV/9zyx/G9bJA8BkAdHxDWSLTJ+i+R4d/GRB4LmACOK/Os+LeS6mTHwyD7oe79Y/jH/SFz874/KitjC3JL3P7xc8ur0f0gie9qPhC43ibtn2TMSP+/VXCaPj62Wa46shI/7VlwFqjCvJPrn90fgcB1z0CsUjRj7FnRbDLo9W+i1QdMqdtuMfB3uXQ9O7uXPufnDtZ/DHX+IH//cNj1uFet/368SQqlpUshjxKslmSVtOLrKHEDrq6Uc34rn4cOesOMbmexc+19pl50Mh1cUfZswwd0rYdws+fbQ/0F5GBiMcNP38gA5vFKibJb/W67/530R8OTDYq3nZ0qY5Y5vJLZ+25fw6z3lwzEVissc+/ah2/K4uF1ik6IXG2dP+amMrjdVfs63FVz7GcybDJ5F6X9tpfYqw9EV2gyF6N/ELTPmHfj7bfjnA5nY3TNfrPUrZkh7r5CSa69+Tn5stB4i7pn178niqsi7JL7+yxGSd37021JYJC9N4ueNRQW5c5Jh/m3iM4Hf0gAAEKVJREFUpup6M0SMrXisFnP9rVFIPgKbPoJRb55f3VqF4gJh34J+uVnoF4qIsXDdF+Bei0yVI9+QCd22w8WqH/4fOLoGvholbpSedxRVd6qGtsNg7wKZhG01SB4OZ/ZC3BaZRD3we8kkcMwGmVNw8oJed8lqXAdHSDoI4WPKf3uJ/l3KB961AoK6nPvOlZMeL2sG2o8qe9wWSdTlphJXlkJxCWHngp4o24bM49JY6HJj7dp7BZct6OHiA/esgz+fgJiNEm1TEyKuEfEMGyjirmlw43cSF79nPmz6VMIqHRyhIAv2LJC2Q56XEM3Dy2HhfXBivZQFPL5OBD60r6RHsIV/jn23ZuMpyJFcPEkH4F/Ly0ZQ2WrHxm1Wgq64JLFvQU+Llf/otbEsFRcONz/x01eUJ6YyHN1KJl9teDSRn7x02PChiGuvqZJOwZwnYqppMj/Q8VpY+hSsfFH+HmwPeZBVry36yzeAEa/JeoD8LInUcfGWNoW5Mrnbor/MJSx9St7P2RuWPyuibsmX6Jzkg3JN3JbzvlUKxYXAvgU9NUYiJQz2Pbfb6KipmFdHaF9xr+SnQ4dxEolzapcsrLJhcoGut8DmTyAgAq79VGL5d8+X2H5HV/h2PCx5XHzyZ/aJS8grVKJ9zh6X/nvcLg+HHd9IZI5fW/jtAfhvmERS2b7BNOsuydFq89A6s18eNK0G1c99USgqwb4FPS1GJswUjRMHE7S5Gg4skRWvbYbJCtbgHmXbXfUkBLSTyVFT0aTplY/L1mqVh/7O78QXf+WTsvArcb98AwjsANZCEfJDy2SSeNAz8t6xG6WPY2vFd+4VKgu8ljwu3wbcAyFqDng0g/DRFX+GmI2SW8ecDzO2SZqGmA0w+FlJq7ztf7KYy6u5pIFoN7LyB0XqCUm4Nvw/sn5BoTgH+xb01BMlKyUVjZNhr0jki8lFMln2vKNEtG24+kLkvyq+3mCQkoHZyRKZU5FY5meJ8GbES94dk7Mct62EPbMP/jdC/P3Ni4qDr34Njq+VhGYAPe8Ut0zWGYm7H/6KuHPmTBSXYEYC/PYgnNwucwGn90qop3tTWclrKarj2utuiaIxOJQf5+rXYP9CcRlNmlOr26i4PLBfQc/LkHhmWy1RRePEu3mJNWpyrptlWt1D38kdbv4BEqKgzZDy55t0hEf3Sew9mqyk3f0jBEfChE8kvfH2r8RSD2gn6ZDn3AiaQR4+d/wOW76QqBwXX3kobf9a0iffvVJ8/flZEn+/4QOZBL5+tlyblSjuo+AesOcnseIP/C6x+22HyvgshTKGdiNlbsBqVW7IyxT7FXRbNkAfJeiKeiCoq/xUhrNXyeux7wG6ZM40GMQ3fvW/JdrKYJBIm++uFUt6yi/g2Uz88kkHoc80CLsSQnpDyytL+jU6iVXvGSwTs1+PkQRvK1+ClMNFbZzhjiXw3QTJYnnLfClj+Mdj4jJqfTVc8SAsuBOuegp63g5rXoegbjI/UF9zG4pLFk0vXUjhIhIZGalv27at7h1EL4Z5U2DaGuV2UVx6HFwq3yC73Vz7a/cvkvQGOSng6A7X/J8ULWnWTVbxphwtKnOYAH5tIHGfROnE/ANo4v+3mkXIbbnyQ3qL6wlkYrnlleVdV6XJy6h6sZqiwdA0bbuu65EVnrNbQd8wS8LKnjxekptEoWgsFOaJa8W/XcWLorKSpJxh0kH5ZjH0JcmRc3I7XPc5/HCTJFi75v/EH7/kSfk2kJ8hPnwXX7Hunb3luFuA5P9xC5TqWtG/i9un88Ty7x01VxZ/XflESfin4qLROAV9yRMSJfBU7P+3d7cxUtVXHMe/x11YQAEVtCoPdVXaFGMilFCbFtFUK9IKWlsDKalGG2JbkxrbphoSS31hq0b7EE0Vqqk1Wqy2ptuo0abaGl+AoIKAgAJSdwEBQXblmV1OX5w7MDvOLLPLzp2d6++TkJm9c5k9+c/dM/f+H87VpaRIoV1bY9JAbhB305uwYFaMB4ybFbV7dqyLmT57d0ZphZx+g6Jcw85muHp+FGpb82z08w8YGt07EIO9Y6+ExklR8uG442I657bVMWsnVzfIPe6mVazWEMRrEGsS5KiymdAfvybqct/4au8FJZJlXc2db98fM3baNsfUTTN4aHL8jQE0DInun483RV38L14X5RqaF8PB3XDaeTGFeNMyaH0/EvqFP4PW5vjy2Pl+VAstnN65rw3mXxyxzf6PunnKkM2Efv/EWECi6VsilbF7eyzG6jcw7oxV3xAVLvOL4R3qiBk2r/42+u2HnR2DxK/NTwZzLebZ7/84at1PvSfm8tf3j8JpT18X6wwgFo9NvTeuFratjls45g9GC9ALCd3MpgC/A+qAP7r7rwtevwX4PtAObAOud/cubx1/TAndHe48I+b+TrmzZ+8hIpVzcF+UShh2TnSl7N0Z/frNC+OWkcPGRMLfsz0WSnUc6HxjdIj+/PNnHrlNYf2AqJzZ2hL/b9DwuAJo3xelHQ7ugw2vxP1yB55UXpzuUY+/tSXuzlUD43HHlNDNrA54B7gUaAEWAzPd/e28fS4GFrn7HjP7AXCRu3dRs/UYE/qeHXB3I1z2K/jyD3v2HiKSLveYrbP8qUjEg0+D874T8+fdo/jZR+/FlcCQEVFfv2VJfAGYQfuBSPyDT4tB3N3bYhHXrg9gx/ojv2fYOdA4Oam54zEoXD8wBoGPHx7Je2dzXDXsb4sBXoixgoYTYv2A1cVjfUMM/J7yhfi/616KL6gRE6KHYF9rzDrq2B9XMgNOjPc91BHrJg51xDqBjiT2un4x9nDuVRFPD3SV0MuZhz4RWOvu65M3WwBMBw4ndHd/OW//hcCsHkVartaWeMyv9icifZtZLIbKLYgqfC13Z66cMZdGQiy2ajbfgd2w6MFI/Cc1xjz8pU9EV0+/gdEVtK81uoE69kfSHToqunMGDIVv3BcLt1Y/G1cDfijq9/ihGFvYsz3WFuz5MLqBOtrjnr0Hk8HcISPi6uHg3uguahgcXwjte+NGLnUNkcjr+sfVxK4tcfvHHib0rpST0EcAzXk/twBd3ZX5BuD5Yi+Y2WxgNsDo0aPLDLGIto3xOGRk1/uJSG07WjKHOGOe9JMjP9+yOhJy4Tz7g/viLLnUwGtX61nc44shd2MT90jM/QZ1fyDXPb4sKqCchF5sWLxoP42ZzQImAJOLve7u84B5EF0uZcb4STpDF5FS6vsX395vwJE6Pd1l1vkuVWbR9dPT97Iyvqh6oJyE3gLkF9AYCWwq3MnMLgHmAJPdfX/vhFdC28a4i/zxp1b014iI1JJyKvgsBsaYWaOZ9QdmAE35O5jZOOAhYJq7by3yHr2rdWOsalMBIhGRw46aEd29HbgJeAFYBfzV3Vea2R1mNi3Z7R7gBOApM1tqZk0l3q53tG1U/7mISIGyqi26+3PAcwXbbs97XmTYuoJam2GU7ukoIpKv9vosDh2K5ckaEBUR6aT2EvrurXHLsCFK6CIi+Wovobcmc9CHqg9dRCRf7SX0tmQOus7QRUQ6qb2ErjN0EZGiai+hj5oIF91WfjU1EZFPidq7SfTICRUpaiMiUutq7wxdRESKUkIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyYiyErqZTTGzNWa21sxuLfJ6g5k9mby+yMzO7O1ARUSka0dN6GZWBzwAXA6MBWaa2diC3W4APnL3c4DfAHf1dqAiItK1+jL2mQisdff1AGa2AJgOvJ23z3RgbvL8aeB+MzN3916MFYBf/nMlb29q6+23FRFJzdgzhvCLK87t9fctJ6GPAJrzfm4BvlRqH3dvN7NWYBjwYf5OZjYbmJ38uMvM1vQkaGB44Xv3IX01NsXVPYqr+/pqbH0yrrk9j+uzpV4oJ6FbkW2FZ97l7IO7zwPmlfE7uw7IbIm7TzjW96mEvhqb4uoexdV9fTW2T1Nc5QyKtgCj8n4eCWwqtY+Z1QNDgR29EaCIiJSnnIS+GBhjZo1m1h+YATQV7NMEXJs8/zbwUiX6z0VEpLSjdrkkfeI3AS8AdcAj7r7SzO4Alrh7E/Aw8JiZrSXOzGdUMmh6odumgvpqbIqrexRX9/XV2D41cZlOpEVEskErRUVEMkIJXUQkI2ouoR+tDEGKcYwys5fNbJWZrTSzHyfb55rZRjNbmvybWoXYNpjZ8uT3L0m2nWxm/zKzd5PHk1KO6fN5bbLUzNrM7OZqtZeZPWJmW81sRd62om1k4ffJMfeWmY1POa57zGx18rufMbMTk+1nmtnevLZ7MOW4Sn52ZnZb0l5rzOyySsXVRWxP5sW1wcyWJttTabMu8kNljzF3r5l/xKDsOuAsoD+wDBhbpVhOB8YnzwcD7xClEeYCP61yO20Ahhdsuxu4NXl+K3BXlT/HD4gFElVpL+BCYDyw4mhtBEwFnifWW1wALEo5rq8D9cnzu/LiOjN/vyq0V9HPLvk7WAY0AI3J32xdmrEVvH4vcHuabdZFfqjoMVZrZ+iHyxC4+wEgV4Ygde6+2d3fSJ5/DKwiVsz2VdOBR5PnjwJXVjGWrwHr3P1/1QrA3V/hk2slSrXRdODPHhYCJ5rZ6WnF5e4vunt78uNCYi1Iqkq0VynTgQXuvt/d3wPWEn+7qcdmZgZcA/ylUr+/REyl8kNFj7FaS+jFyhBUPYlaVJccByxKNt2UXDY9knbXRsKBF83sdYtyCwCfcffNEAcbcGoV4sqZQec/sGq3V06pNupLx931xJlcTqOZvWlm/zWzSVWIp9hn15faaxKwxd3fzduWapsV5IeKHmO1ltDLKjGQJjM7AfgbcLO7twF/AM4Gzgc2E5d7afuKu48nKmT+yMwurEIMRVksTpsGPJVs6gvtdTR94rgzszlAO/B4smkzMNrdxwG3AE+Y2ZAUQyr12fWJ9krMpPPJQ6ptViQ/lNy1yLZut1mtJfRyyhCkxsz6ER/W4+7+dwB33+LuHe5+CJhPBS81S3H3TcnjVuCZJIYtuUu45HFr2nElLgfecPctSYxVb688pdqo6sedmV0LfBP4riedrkmXxvbk+etEX/Xn0oqpi8+u6u0Fh8uQfAt4MrctzTYrlh+o8DFWawm9nDIEqUj65h4GVrn7fXnb8/u9rgJWFP7fCsd1vJkNzj0nBtRW0Lk8w7XAP9KMK0+nM6Zqt1eBUm3UBHwvmYlwAdCau2xOg5lNAX4OTHP3PXnbT7G4XwFmdhYwBlifYlylPrsmYIbFjW8ak7heSyuuPJcAq929JbchrTYrlR+o9DFW6dHeCoweTyVGjNcBc6oYx1eJS6K3gKXJv6nAY8DyZHsTcHrKcZ1FzDBYBqzMtRFRzvjfwLvJ48lVaLNBwHZgaN62qrQX8aWyGThInB3dUKqNiMvhB5JjbjkwIeW41hL9q7nj7MFk36uTz3gZ8AZwRcpxlfzsgDlJe60BLk/7s0y2/wm4sWDfVNqsi/xQ0WNMS/9FRDKi1rpcRESkBCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJiP8D6i/1p96U7QQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "train_history2 = TrainingHistory(['training-acc', 'val-acc-top1', 'val-acc-top5'])\n",
    "\n",
    "print(\"Training loop started for {} epochs:\".format(epochs))\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data  = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        if mixup or stitch_mixup:\n",
    "            if mixup and not(stitch_mixup):\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                if epoch >= epochs - mixup_off_epochs:\n",
    "                    lam = 1\n",
    "                data = [lam*X + (1-lam)*X[::-1] for X in data]\n",
    "            \n",
    "                if label_smoothing:\n",
    "                    eta = 0.1\n",
    "                else:\n",
    "                    eta = 0.0\n",
    "                label = mixup_transform(label, num_classes, lam, eta)\n",
    "            \n",
    "            elif stitch_mixup and not(mixup):\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                if lam < 0.1:\n",
    "                    lam = 1\n",
    "                if epoch >= epochs - mixup_off_epochs:\n",
    "                    lam = 1\n",
    "                data = [stitch_mixup_transform(X, lam) for X in data]\n",
    "                \n",
    "                if label_smoothing:\n",
    "                    eta = 0.1\n",
    "                else:\n",
    "                    eta = 0.0\n",
    "                label = stitch_mixup_label_transform(label, num_classes, lam, eta)\n",
    "            \n",
    "            else:\n",
    "                print(\"No Mixup.\")\n",
    "        \n",
    "        elif label_smoothing:\n",
    "            hard_label = label\n",
    "            label = smooth(label, num_classes)\n",
    "        \n",
    "        if distillation:\n",
    "            teacher_prob = [nd.softmax(teacher(X.astype(dtype, copy=False)) / T) for X in data]\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            outputs = [net(X.astype(dtype, copy=False)) for X in data]\n",
    "            if distillation:\n",
    "                loss = [loss_fn(yhat.astype(dtype, copy=False),\n",
    "                                y.astype(dtype, copy=False),\n",
    "                                p.astype(dtype, copy=False)) for yhat, y, p in zip(outputs, \n",
    "                                                                                   label, \n",
    "                                                                                   teacher_prob)]\n",
    "            else:\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            \n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        \n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            \n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        # Update metrics\n",
    "        if mixup or stitch_mixup:\n",
    "            output_softmax = [nd.SoftmaxActivation(out.astype(dtype, copy=False)) \\\n",
    "                              for out in outputs]\n",
    "            train_metric.update(label, output_softmax)\n",
    "        else:\n",
    "            if label_smoothing:\n",
    "                train_metric.update(hard_label, outputs)\n",
    "            else:\n",
    "                train_metric.update(label, outputs)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    \n",
    "    # Evaluate on Validation data\n",
    "    #name, val_acc = test(ctx, val_data)\n",
    "    val_acc_top1, val_acc_top5 = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc_top1])\n",
    "    train_history2.update([acc, val_acc_top1, val_acc_top5])\n",
    "    \n",
    "    print('[Epoch %d] train=%f val_top1=%f val_top5=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc_top1, val_acc_top5, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_history.plot(['training-error', 'validation-error'], \n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_errors_{t}.png\".format(o=optimizer,\n",
    "                                                                                           ep=epochs,\n",
    "                                                                                           t=timestamp))\n",
    "train_history2.plot(['training-acc', 'val-acc-top1', 'val-acc-top5'],\n",
    "                   save_path=\"./cifar100_resnet56_v1_{o}_{ep}epochs_accuracies_{t}.png\".format(o=optimizer,\n",
    "                                                                                               ep=epochs,\n",
    "                                                                                               t=timestamp))\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
